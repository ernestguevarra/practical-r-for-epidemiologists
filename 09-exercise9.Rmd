# Computer intensive methods {#exercise9}

## Estimation

Estimation involves the calculation of a measure with some sense of precision based upon sampling variation.

Only a few estimators (e.g. the sample mean from a normal population) have exact formulae that may be used to estimate sampling variation. Typically, estimates of variability are based upon approximations informed by expected or postulated properties of the sampled population. The development of variance formulae for some measures may require in-depth statistical and mathematical knowledge or may even be impossible to derive.

*Bootstrap* methods are computer-intensive methods that can provide estimates and measures of precision (e.g. confidence intervals) without resort to theoretical models, higher mathematics, or assumptions about the sampled population. They rely on repeated sampling, sometimes called *resampling*, of the observed data.

As a simple example of how such methods work, we will start by using bootstrap methods to estimate the mean from a normal population. We will work with a very simple dataset which we will enter directly:

```{r, eval = TRUE}
x <- c(7.3, 10.4, 14.0, 12.2, 8.4)
```

We can summarise this data quite easily:

```{r, eval = TRUE}
mean(x)
```

The `sample()` function can be used to select a bootstrap `replicate`: 

```{r, eval = TRUE}
sample(x, length(x), replace = TRUE)
```

Enter this command several times to see some more bootstrap replicates. Remember that previous commands can be recalled and edited using the up and down arrow keys – they do not need to be typed out in full each time. The `length()` parameter is not required for taking bootstrap replicates and can be omitted.

It is possible to apply a summary measure to a replicate:

```{r, eval = TRUE}
mean(sample(x, replace = TRUE))
```

Enter this command several times. A bootstrap estimate of the mean of `x` can be made by repeating this process many times and taking the `median` of the means for each replicate.

One way of doing this is to create a matrix where each column contains a bootstrap replicate and then use the `apply()` and `mean()` functions to get at the estimate.

First create the matrix of replicates. Here we take ten replicates:

```{r, eval = TRUE}
x1 <- matrix(sample(x, length(x) * 10, replace = TRUE),
             nrow = length(x), ncol = 10)
x1
```

Then calculate and store the means of each replicate. We can do this using the `apply()` function to apply the `mean()` function to the columns of matrix `x1`:

```{r, eval = TRUE}
x2 <- apply(x1, 2, mean)
x2
```

The bootstrap estimate of the mean is:

```{r, eval = TRUE}
median(x2)
```

The bootstrap estimate may differ somewhat from the mean of `x`: 

```{r, eval = TRUE}
mean(x)
```

The situation is improved by increasing the number of replicates. Here we take 5000 replicates:

```{r, eval = TRUE}
x1 <- matrix(sample(x, length(x) * 5000, replace = TRUE),
             nrow = length(x), ncol = 5000)
x2 <- apply(x1, 2, mean)
median(x2)
```

This is a pretty useless example as estimating the mean / standard deviation, or standard error of the mean of a sample from a normal population can be done using standard formulae.

The utility of bootstrap methods is that they can be applied to summary measures that are not as well understood as the arithmetic mean. The bootstrap method also has the advantage of retaining simplicity even with complicated measures.

To illustrate this, we will work through an example of using the bootstrap to estimate the harmonic mean. 

Again, we will work with a simple dataset which we will enter directly:

```{r, eval = TRUE}
d <- c(43.64, 50.67, 33.56, 27.75, 43.35, 29.56, 38.83, 35.95, 20.01)
```

The data represents distance (in kilometres) from a point source of environmental pollution for nine female patients with oral / pharyngeal cancer.

The harmonic mean is considered to be a sensitive measure of spatial clustering. The first step is to construct a function to calculate the harmonic mean:

```{r, eval = TRUE}
h.mean <- function(x) {length(x) / sum(1 / x)}
```

Calling this function with the sample data:

```{r, eval = TRUE}
h.mean(d)
```

Should return an estimated harmonic mean distance of 33.47 kilometres. This is simple. The problem is that calculating the variance of this estimate is complicated using standard methods. This problem is relatively simple to solve using bootstrap methods:

```{r, eval = TRUE}
replicates <- 5000
n <- length(d)
x1 <- matrix(sample(d, n * replicates, replace = TRUE),
             nrow = n, ncol = replicates)
x2 <- apply(x1, 2, h.mean)
median(x2)
```

A 95% confidence interval can be extracted from `x2` using the `quantile()` function: 

```{r, eval = TRUE}
quantile(x2, c(0.025, 0.975))
```

A 99% confidence interval can also be extracted from `x2` using the `quantile()` function: 

```{r, eval = TRUE}
quantile(x2, c(0.005, 0.995))
```

As a final example of the bootstrap method we will use the method to obtain an estimate of an odds ratio from a two-by-two table. We will work with the `salex` dataset which we used in exercise 2 and exercise 3:

```{r, eval = TRUE}
salex <- read.table("salex.dat",  header = TRUE, na.strings = "9")
table(salex$EGGS, salex$ILL)
```

We should set up our estimator function to calculate an odds ratio from a two-by-two table:

```{r, eval = TRUE}
or <- function(x) {(x[1,1] / x[1,2]) / (x[2,1] / x[2,2])}
```

We should test this:

```{r, eval = TRUE}   
or(table(salex$EGGS, salex$ILL))
```

The problem is to take a bootstrap replicate from two vectors in a data.frame. This can be achieved by using `sample()` to create a vector of row indices and then use this sample of indices to select replicates from the data.frame:
   
```{r, eval = TRUE}
boot <- NULL
for(i in 1:1000) {
  sampled.rows <- sample(1:nrow(salex), replace = TRUE)
  x <- salex[sampled.rows, "EGGS"]
  y <- salex[sampled.rows, "ILL"]
  boot[i] <- or(table(x, y))
}
```

The vector `boot` now contains the odds ratios calculated from 1000 replicates. Estimates of the odds ratio and its 95% confidence interval may be obtained using the `median()` and `quantile()` functions

```{r, eval = FALSE}   
median(boot)
quantile(boot, c(0.025, 0.975))
```

```{r, echo = FALSE, eval = TRUE}   
median(boot)
quantile(boot, c(0.025, 0.975))
```

This approach may fail when some tables have cells that contain zero. Infinite values arise due to division by zero when calculating the odds ratio for some replicates. We can avoid this problem by selecting only those values of `boot` that are not (`!=`) infinite (`Inf`):

```{r, eval = FALSE}
boot <- boot[boot != Inf]
median(boot)
quantile(boot, c(0.025, 0.975))
```

```{r, echo = FALSE, eval = TRUE}
boot <- boot[boot != Inf]
median(boot)
quantile(boot, c(0.025, 0.975))
```

Another way to avoid this problem is to use an `adjusted odds ratio` calculated by adding 0.5 to each cell of the two-by-two table:

```{r, eval = FALSE}
boot <- NULL
for(i in 1:1000) {
  sampled.rows <- sample(1:nrow(salex), replace = TRUE)
  x <- salex[sampled.rows, "EGGS"]
  y <- salex[sampled.rows, "ILL"]
  boot[i] <- or(table(x, y) + 0.5)
  }
median(boot)
quantile(boot, c(0.025, 0.975))
```

```{r, echo = FALSE, eval = TRUE}
boot <- NULL
for(i in 1:1000) {
  sampled.rows <- sample(1:nrow(salex), replace = TRUE)
  x <- salex[sampled.rows, "EGGS"]
  y <- salex[sampled.rows, "ILL"]
  boot[i] <- or(table(x, y) + 0.5)
  }
median(boot)
quantile(boot, c(0.025, 0.975))
```

This procedure is preferred when working with sparse tables.


## Hypothesis testing

Computer-intensive methods also offer a general approach to statistical hypothesis testing. To illustrate this we will use *computer based simulation* to investigate spatial clustering around a point.

Before continuing, we will retrieve a dataset:

```{r, eval = TRUE}
waste <- read.table("waste.dat", header = TRUE)
```

The file `waste.dat` contains the location of twenty-three recent cases of childhood cancer in 5 by 5 km square surrounding an industrial waste disposal site. The columns in the dataset are:

+----------+------------------------------+
| **x**    | The x location of cases      |
+----------+------------------------------+
| **y**    | The y location of cases      |
+----------+------------------------------+

The `x` and `y` variables have been transformed to lie between 0 and 1 with the industrial waste disposal site centrally located (i.e. at `x` = 0.5, `y` = 0.5).

Plot the data and the location of the industrial waste disposal site on the same chart:

```{r, eval = TRUE}
plot(waste$x, waste$y, xlim = c(0, 1), ylim = c(0, 1))
points(0.5, 0.5, pch = 3)
```

We can calculate the distance of each point from the industrial waste disposal site using Pythagoras' Theorem:

```{r, eval = TRUE}
distance.obs <- sqrt((waste$x - 0.5) ^ 2 + (waste$y - 0.5) ^ 2)
```

The observed mean distance or each case from the industrial waste disposal site is:

```{r, eval = TRUE}   
mean(distance.obs)
```

To test whether this distance is unlikely to have arisen by chance (i.e. evidence of spatial clustering) we need to simulate the distribution of distances when no spatial pattern exists:

```{r, eval = TRUE}   
r <- 10000
x.sim <- matrix(runif(r * 23), 23, r)
y.sim <- matrix(runif(r * 23), 23, r)
distance.run <- sqrt((x.sim - 0.5)^2 + (y.sim - 0.5)^2)
distance.sim <- apply(distance.run, 2, mean)
hist(distance.sim, breaks = 20)
abline(v = mean(distance.obs), lty = 3)
```

The probability (i.e. the *p-value*) of observing a mean distance smaller than the *observed mean* distance under the null hypothesis can be estimated as the number of estimates of the mean distance under the null hypothesis falling below the observed mean divided by the total number of estimates of the mean distance under the null hypothesis:

```{r, eval = TRUE}
m <- mean(distance.obs)
z <- ifelse(distance.sim < m, 1, 0)
sum(z) / r
```

You might like to repeat this exercise using the harmonic mean distance and the median distance.

We can check if this method is capable of detecting a simple cluster using simulated data:

```{r, eval = TRUE}
x <- rnorm(23, mean = 0.5, sd = 0.2)
y <- rnorm(23, mean = 0.5, sd = 0.2)
plot(x, y, xlim = c(0, 1), ylim = c(0, 1))
points(0.5, 0.5, pch = 3)
```

We need to recalculate the distance of each simulated case from the industrial waste disposal site:

```{r, eval = TRUE}
distance.obs <- sqrt((x - 0.5) ^ 2 + (y - 0.5) ^ 2)
```

The observed mean distance of each case from the industrial waste disposal site is:
   
```{r, eval = TRUE}   
mean(distance.obs)
```

We can use the the simulated null hypothesis data to test for spatial clustering:

```{r, eval = TRUE}
m <- mean(distance.obs)
z <- ifelse(distance.sim < m, 1, 0)
sum(z) / r
```

We should also check if the procedure can detect a plume of cases, such as might be created by a prevailing wind at a waste incineration site, in a similar way:

```{r, eval = TRUE}
x <- rnorm(23, 0.25, 0.1) + 0.5
y <- rnorm(23, 0.25, 0.1) + 0.5
plot(x, y, xlim = c(0, 1), ylim = c(0, 1))
points(0.5, 0.5, pch = 3)
distance.obs <- sqrt((x - 0.5)^2 + (y - 0.5)^2)
m <- mean(distance.obs)
z <- ifelse(distance.sim < m, 1, 0)
sum(z) / r
```

The method is not capable of detecting plumes.

You might like to try adapting the simulation code presented here to provide a method capable of detecting plumes of cases.

## Simulating processes

In the previous example we simulated the expected distribution of data under the *null hypothesis*. Computer based simulations are not limited to simulating data. They can also be used to simulate processes.

In this example we will simulate the behaviour of the *lot quality assurance sampling* (LQAS) survey method when sampling constraints lead to a loss of sampling independence. In this example the sampling process is simulated and applied to real-world data.

LQAS is a small-sample classification technique that is widely used in manufacturing industry to judge the quality of a batch of manufactured items. In this context, LQAS is used to identify batches that are likely to contain an unacceptably large number of defective items. In the public health context, LQAS may be used to identify communities with unacceptably low levels of service (e.g. vaccine) coverage or worrying levels of disease prevalence.

The LQAS method produces data that is easy to analyse. Data analysis is performed as data is collected and consists solely of counting the number of *defects* (e.g. children with a specific disease) in the sample and checking whether a predetermined threshold value has been exceeded. This combination of data collection and data analysis is called a *sampling plan*. LQAS sampling plans are developed by specifying:


- **A TRIAGE SYSTEM**: A classification system that defines *high*, *moderate*, and *low* categories of the prevalence of the phenomenon of interest.

- **ACCEPTABLE PROBABILITIES OF ERROR**: There are two probabilities of error. These are termed provider *probability of error* (PPE) and *consumer probability of error* (CPE):

    - **Provider Probability of Error (PPE)**: The risk that the survey will indicate that prevalence is *high* when it is, in fact, *low*. PPE is analogous to *type I* ($\alpha$) error in statistical hypothesis testing.
    
    
    - **Consumer Probability of Error (CPE)**: The risk that the survey will indicate that prevalence is *low* when it is, in fact, *high*. CPE is analogous to *type II* ($\beta$) error in statistical hypothesis testing.
    
Once the upper and lower levels of the triage system and acceptable levels of error have been decided, a set of probability tables are constructed that are used to select a maximum sample size (`n`) and the number of defects or cases (`d`) that are allowed in the sample of `n` subjects before deciding that a population is a high prevalence population. The combination of maximum sample size (`n`) and number of defects (`d`) form the stopping rules of the sampling plan. Sampling stops when either the maximum sample size (`n`) is met or the allowable number of defects (`d`) is exceeded:

* If `d` is exceeded then the population is classified as high prevalence.

* If `n` is met without d being exceeded then the population is classified as low prevalence.

The values of `n` and `d` used in a sampling plan depend upon the threshold values used in the triage system and the acceptable levels of error. The values of `n` and `d` used in a sampling plan are calculated using binomial probabilities. For example, the probabilities of finding 14 or fewer cases ($d = 14$) in a sample of 50 individuals ($n = 50$) from populations with prevalences of either 20% or 40% are:

```{r, eval = FALSE}
pbinom(q = 14, size = 50, prob = 0.2)
pbinom(q = 14, size = 50, prob = 0.4)
```

```{r, echo = FALSE, eval = TRUE}
pbinom(q = 14, size = 50, prob = 0.2)
pbinom(q = 14, size = 50, prob = 0.4)
```

The sampling plan with $n = 50$ and $d = 14$ is, therefore, a reasonable candidate for a sampling plan intended to distinguish between populations with prevalences of less than or equal to 20% and populations with prevalences greater than or equal to 40%.

There is no middle ground with LQAS sampling plans. Population are always classified as either high or low prevalence. Populations with prevalences between the upper and lower standards of the triage system are classified as high or low prevalence populations. The probability of a moderate prevalence population being classified as high or low prevalence is proportional to the proximity of the prevalence in that population to the triage standards. Moderate prevalence populations close to the upper standard will tend to be classified as high prevalence populations. Moderate prevalence populations close to the lower standard will tend to be classified as low prevalence populations. This behaviour is summarised by the operating characteristic (OC) curve for the sampling plan. For example:

```{r, eval = TRUE}
plot(seq(0, 0.6, 0.01),
     pbinom(14, 50, seq(0, 0.6, 0.01), lower.tail = FALSE),
     main = "OC Curve for n = 50, d = 14",
     xlab = "Proportion diseased",
     ylab = "Probability",
     type = "l", lty = 2)
```

The data we will use for the simulation is stored in forty-eight separate files. These files contain the returns from whole community screens for active trachoma (TF/TI) in children undertaken as part of trachoma control activities in five African countries. Each file has the file suffix .sim. The name of the file reflects the country in which the data were collected. The data files are:

+--------------------+-----------+--------------------+
| **File name**      | **Files** | **Origin**         |
+:===================+==========:+:===================+
| **egyptXX.sim**    | 10        | Egypt              |
+--------------------+-----------+--------------------+
| **gambiaXX.sim**   | 10        | Gambia             |
+--------------------+-----------+--------------------+
| **ghanaXX.sim**    | 3         | Ghana              |
+--------------------+-----------+--------------------+
| **tanzaniaXX.sim** | 14        | Tanzania           |
+--------------------+-----------+--------------------+
| **togoXX.sim**     | 11        | Togo               |
+--------------------+-----------+--------------------+

All of these data files have the same structure. The variables in the data files are:

+---------------+--------------------------------------------------+
| **hh**        | Household identifier                             |
+---------------+--------------------------------------------------+
| **sex**       | Sex of child (1=male, 2=female)                  |
+---------------+--------------------------------------------------+
| **age**       | Age of child in years                            |
+---------------+--------------------------------------------------+
| **tfti**      | Child has active (TF/TI) trachoma (0=no, 1=yes)  |
+---------------+--------------------------------------------------+

Each row in these files represents a single child. For example:

```{r, eval = TRUE}
x <- read.table("gambia09.sim", header = TRUE)
x[1:10, ]
```

Any rapid survey method that is appropriate for general use in developing countries is restricted to sampling `households` rather than `individuals`. Sampling households in order to sample individuals violates a principal requirement for a sample to be representative of the population from which it is drawn (i.e. that individuals are selected `independently` of each other). This lack of statistical independence amongst sampled individuals may invalidate standard approaches to selecting sampling plans leading to increased probabilities of error. This is likely to be a particular problem if cases tend to be clustered within households. Trachoma is an infectious disease that confers no lasting immunity in the host. Cases are, therefore, very likely to be clustered within households. One solution to this problem would be to sample (i.e. at random) a single child from each of the sampled households. This is not appropriate for active trachoma as the examination procedure often causes distress to younger children. This may influence survey staff to select older children, who tend to have a lower risk of infection, for examination. Sampling is, therefore, constrained to sampling all children in selected households.

The purpose of the simulations presented here is to determine whether the LQAS method is robust to:


1. The loss of sampling independence introduced by sampling households at random and examining all children in selected households for active trachoma.

And:

2. The slight increase in the maximum sample size (`n`) introduced by examining all children in selected households for active trachoma.

Each row in the datasets we will be using represents an individual child. Since we will simulate sampling households rather than individual children we need to be able to convert the datasets from one row per child to one row per household. We will write a function to do this.

Create a new function called `ind2hh()`: 

```{r, eval = FALSE}
ind2hh <- function() {}
```

This creates an empty function called `ind2hh()`. Use the `fix()` function to edit the `ind2hh()` function: 

```{r, eval = FALSE}
fix(ind2hh)
```

Edit the function to read:

```{r, eval = FALSE}
function(data) {
  n.kids <- n.cases <- NULL
  id <- unique(data$hh)
  for(household in id) {
    temp <- subset(data, data$hh == household)
    n.kids <- c(n.kids, nrow(temp))
    n.cases <- c(n.cases, sum(temp$tfti))
  }
  result <- as.data.frame(cbind(id, n.kids, n.cases))
  return(result)
}
```

```{r, echo = FALSE, eval = TRUE}
ind2hh <- function(data) {
  n.kids <- n.cases <- NULL
  id <- unique(data$hh)
  for(household in id) {
    temp <- subset(data, data$hh == household)
    n.kids <- c(n.kids, nrow(temp))
    n.cases <- c(n.cases, sum(temp$tfti))
  }
  result <- as.data.frame(cbind(id, n.kids, n.cases))
  return(result)
}
```

Once you have made the changes shown above, check your work, save the file, and quit the editor.

Now we have created the `ind2hh()` function we should test it for correct operation. We will create a simple test data.frame (`test.df`) for this purpose:

```{r, eval = TRUE}
test.df <- as.data.frame(cbind(c(1, 1, 2, 2, 2),  c(1, 1, 1, 0 ,0)))
names(test.df) <- c("hh", "tfti")
test.df
```

The expected operation of the `ind2hh()` function given `test.df` as input is:

+-----+----------+
| hh  | tfti     |
+-----+----------+
| 1   | 1        |
+-----+----------+
| 1   | 1        |
+-----+----------+
| 2   | 1        |
+-----+----------+
| 2   | 0        |
+-----+----------+
| 2   | 0        |
+-----+----------+

becomes

+-----+----------+----------+
| id  | n.kids   | n.case   |
+-----+----------+----------+
| 1   | 2        | 2        |
+-----+----------+----------+
| 2   | 3        | 1        |
+-----+----------+----------+

Confirm this behaviour:

```{r, eval = FALSE}
test.df
ind2hh(test.df)
```

```{r, echo = FALSE, eval = TRUE}
test.df
ind2hh(test.df)
```

We can apply this function to the datasets as required. For example:

```{r, eval = FALSE}
x <- read.table("gambia09.sim", header = TRUE)
x
x.hh <- ind2hh(x)
x.hh
```

```{r, echo = FALSE, eval = TRUE}
x <- read.table("gambia09.sim", header = TRUE)
x
x.hh <- ind2hh(x)
x.hh
```

We will now write a function that will simulate a single LQAS survey. Create a new function called `lqas.run()`:

```{r, eval = FALSE}
lqas.run <- function() {}
```

This creates an empty function called `lqas.run()`.

Use the `fix()` function to edit the `lqas.run()` function:

```{r, eval = FALSE}
fix(lqas.run)
```


Edit the function to read:

```{r, echo = FALSE, eval = TRUE}
lqas.run <- function(x, n, d) {
  kids <- cases <- 0
  y <- x[sample(1:nrow(x),replace = TRUE), ]
  for(i in 1:nrow(y)) {
    kids <- kids + y[i, ]$n.kids
    cases <- cases + y[i, ]$n.cases
    if(cases > d) {
      outcome = 1
       break
    }
    if(kids >= n & cases <= d) {
      outcome = 0
      break
    }
  }
  result <- list(kids = kids, cases = cases, outcome = outcome)
  return(result)
}
```

Once you have made the changes shown above, check your work, save the file, and quit the editor.

We should try this function on a low, a moderate, and a high prevalence dataset. To select suitable test datasets we need to know the prevalence in each dataset:

```{r, eval = TRUE}
for(i in dir(pattern = "\\.sim$")) {
  data <- read.table(i, header = TRUE)
  cat(i, ":", mean(data$tfti), "\n")
}
```

The coding scheme used for the `tfti` variable (0=no, 1=yes) allows us to use the `mean()` function to calculate prevalence in these datasets.

The pattern `"\\.sim$"` is a regular expression for files ending in .sim.

If you want to use the `dir()` function to list files stored outside of the current working directory you will need to specify an appropriate value for the `path` parameter. On some systems you may also need to set the value of the `full.names` parameter to `TRUE`. For example:

```{r, eval = FALSE}
for(i in dir(path = "~/prfe", pattern = "\\.sim$", full.names = TRUE)) {
  data <- read.table(i, header = TRUE)
  cat(i, ":", mean(data$tfti), "\n")
}
```

cycles through all files ending in `.sim` (specified by giving the value `"\\.sim$"` to the `pattern` parameter) that are stored the `prfe` directory under the users home directory (specified by giving the value `"~/prfe"` to the `path` parameter) on UNIX systems. You **cannot** usefully specify a URL for the `path` parameter of the `dir()` function.

We will use `tanzania04.sim` as an example of a low prevalence dataset:

```{r, eval = TRUE}
x <- read.table("tanzania04.sim", header = TRUE)
x.hh <- ind2hh(x)
lqas.run(x = x.hh, n = 50, d = 14)
```

Repeat the last function call several times. Remember that previous commands can be recalled and edited using the up and down arrow keys – they do not need to be typed out in full each time.

The function should, for most calls, return:

```
$outcome
[1] 0
```

We will use `tanzania08.sim` as an example of a high prevalence dataset:

```{r, eval = TRUE}
x <- read.table("tanzania08.sim", header = TRUE)
x.hh <- ind2hh(x)
lqas.run(x = x.hh, n = 50, d = 14)
```

Repeat the last function call several times. The function should, for most calls, return:

```
$outcome
[1] 1
```

