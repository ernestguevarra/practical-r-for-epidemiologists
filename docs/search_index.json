[["index.html", "Practical R for Epidemiologists Welcome", " Practical R for Epidemiologists Mark Myatt 2022-02-15 Welcome This is the website for Practical R for Epidemiologists. Visit the GitHub repository for this site or buy it as a Kindle ebook on Amazon. "],["introduction.html", "Introduction", " Introduction These notes are intended as a practical introduction to using the R environment for data analysis and graphics to work with epidemiological data. Topics covered include univariate statistics, simple statistical inference, charting data, two-by-two tables, stratified analysis, chi-square test for trend, logistic regression, survival analysis, computer-intensive methods, and extending R using user-provided functions. You should be able to follow the material if you are reasonably familiar with the mechanics of statistical estimation (e.g. calculation of odds ratios and confidence intervals) and require a system that can perform simple or complex analyses to your exact specifications. These notes are split into ten sections:   Introduction: You are reading this section now!   Introducing R: Some information about the R system, the way the R system works, how to get a copy of R, and how to start R.   Exercise 1: Read a dataset, producing descriptive statistics, charts, and perform simple statistical inference. The aim of the exercise is for you to become familiar with R and some basic R functions and objects.   Exercise 2: In this exercise we explore how to manipulate R objects and how to write functions that can manipulate and extract data and information from R objects and produce useful analyses.   Exercise 3: In this exercise we explore how R handles generalised linear models using the example of logistic regression as well as seeing how R can perform stratified (i.e. Mantel-Haenszel) analysis as well as analysing data arising from matched case-control studies.   Exercise 4: In this exercise we use R to analyse a small dataset using the methods introduced in the previous exercises.   Exercise 5: In this exercise we explore how R can be extended using add-in packages. Specifically, we will use an add-in package to perform a survival analysis.   Exercise 6: In this exercise we explore how to make your own R functions behave like R objects so that they return a data-structure that can be manipulated or interrogated by other R functions.   Exercise 7: In this exercise we explore how you can use R to produce custom graphical functions.   Exercise 8: In this exercise we explore some more graphical functions and create custom graphical functions that produce two variable plots, pyramid charts, Pareto charts, charts with error bars, and simple mesh-maps.   Exercise 9: In this exercise we explore ways of implementing computer-intensive methods, such as the bootstrap and computer based simulation, using standard R functions.   If you are interested in a system that is flexible, can be tailored to produce exactly the analysis you want, provides modern analytical facilities, and have a basic understanding of the mechanics of hypothesis testing and estimation then you should consider following this material. "],["introducing-r.html", "Introducing R Retrieving data", " Introducing R R is a system for data manipulation, calculation, and graphics. It provides: Facilities for data handling and storage A large collection of tools for data analysis Graphical facilities for data analysis and display A simple but powerful programming language R is often described as an environment for working with data. This is in contrast to a package which is a collection of very specific tools. R is not strictly a statistics system but a system that provides many classical and modern statistical procedures as part of a broader data-analysis tool. This is an important difference between R and other statistical systems. In R a statistical analysis is usually performed as a series of steps with intermediate results being stored in objects. Systems such as SPSS and SAS provide copious output from (e.g.) a regression analysis whereas R will give minimal output and store the results of a fit for subsequent interrogation or use with other R functions. This means that R can be tailored to produce exactly the analysis and results that you want rather than produce an analysis designed to fit all situations. R is a language based product. This means that you interact with R by typing commands such as:   table(SEX, LIFE)   rather than by using menus, dialog boxes, selection lists, and buttons. This may seem to be a drawback but it means that the system is considerably more flexible than one that relies on menus, buttons, and boxes. It also means that every stage of your data management and analysis can be recorded and edited and re-run at a later date. It also provides an audit trail for quality control purposes. R is available under UNIX (including Linux), the Macintosh operating system OS X, and Microsoft Windows. The method used for starting R will vary from system to system. On UNIX systems you may need to issue the R command in a terminal session or click on an icon or menu option if your system has a windowing system. On Macintosh systems R will be available as an application but can also be run in a terminal session. On Microsoft Windows systems there will usually be an icon on the Start menu or the desktop. R is an open source system and is available under the GNU general public license (GPL) which means that it is available for free but that there are some restrictions on how you are allowed to distribute the system and how you may charge for bespoke data analysis solutions written using the R system. Details of the general public license are available from http://www.gnu.org/copyleft/gpl.html. R is available for download from http://www.r-project.org/. This is also the best place to get extension packages and documentation. You may also subscribe to the R mailing lists from this site. R is supported through mailing lists. The level of support is at least as good as for commercial packages. It is typical to have queries answered in a matter of a few hours. Even though R is a free package it is more powerful than most commercial packages. Many of the modern procedures found in commercial packages were first developed and tested using R or S-Plus (the commercial equivalent of R). When you start R it will issue a prompt when it expects user input. The default prompt is:   &gt;   This is where you type commands that call functions that instruct R to (e.g.) read a data file, recode data, produce a table, or fit a regression. For example:   &gt; table(SEX, LIFE)   If a command you type is not complete then the prompt will change to:   + on subsequent lines until the command is complete:   &gt; table( + SEX, LIFE +)   The &gt; and + prompts are not shown in the example commands in the rest of this material. The example commands in this material are often broken into shorter lines and indented for ease of understanding. The code still works as lines are split in places where R knows that a line is not complete. For example:   table(SEX, LIFE)   could be entered on a single line as:   table(SEX, LIFE)   In this example R knows that the command is not complete until the brackets are closed. The following example could also be written on one line:   salex.lreg.coeffs &lt;- coef(summary(salex.lreg))   In this case R knows that the &lt;- operator at the end of the first line needs further input. R maintains a history of previous commands. These can be recalled and edited using the up and down arrow keys. Output that has scrolled off the top of the output / command window can be recalled using the window or terminal scroll bars. Output can be saved using the sink() function with a file name: sink(\"results.out\") to start recording output. Use the sink() function without a file name to stop recording output: sink() You can also use clipboard functions such as copy and paste to (e.g.) copy and then paste selected chunks of output into an editor or word processor running alongside R. All the sample data files used in the exercises in this manual are space delimited text files using the general format:   ID AGE IQ 1 39 94 2 41 89 3 42 83 4 30 99 5 35 94 6 44 90 7 31 94 8 39 87   R has facilities for working with files in different formats including (through the use of extension packages) ODBC (open database connectivity) and SQL data sources, EpiInfo, EpiData, Minitab, SPSS, SAS, S-Plus, and Stata format files. Retrieving data All of the exercises in this manual assume that the necessary data files are located in the current working directory. All of the data files that you require to follow this material are in a ZIP archive that can be downloaded from https://github.com/ernestguevarra/practical-r-for-epidemiologists/raw/master/data/prfe.zip. A command such as:   read.table(&quot;data/fem.dat&quot;, header = TRUE)   retrieves the data stored in the file named fem.dat which is stored in the current working directory. To retrieve data that is stored in files outside a different directory you need to specify the full path to the file. For example:   read.table(&quot;~/prfe/fem.dat&quot;, header = TRUE)   will retrieve the data stored in the file named fem.dat stored in the prfe directory under the user’s home directory on UNIX, Linux, and OS X systems. R follows many UNIX operating and naming conventions including the use of the backslash (\\) character to specify special characters in strings (e.g. using \\n to specify a new line in printed output). Windows uses the backslash (\\) character to separate directory and file names in paths. This means that Windows users need to escape any backslashes in file paths using an additional backslash character. For example:   read.table(&quot;c:\\\\prfe\\\\fem.dat&quot;, header = TRUE)   will retrieve the data that is stored in the file named fem.dat which is stored in the prfe directory off the root directory of the C: drive. The Windows version of R also allows you to specify UNIX-style path names (i.e. using the forward slash (/) character as a separator in file paths). For example:   read.table(&quot;c:/prfe/fem.dat&quot;, header = TRUE)   Path names may include shortcut characters such as:   . The current working directory .. Up one level in the directory tree ~ The user’s home directory (on UNIX-based systems)   R also allows you to retrieve files from any location that may be represented by a standard uniform resource locator (URL) string. For example:   read.table(&quot;file://~/prfe/fem.dat&quot;, header = TRUE)   will retrieve the data stored in the file named fem.dat stored in the prfe directory under the users home directory on UNIX-based systems. All of the data files used in this section are stored in the /prfe directory Brixton Health’s website. This means, for example, that you can use the read.table() function specifying “https://github.com/ernestguevarra/practical-r-for-epidemiologists/blob/master/data/fem.dat” as the URL to retrieve the data that is stored in the file named fem.dat which is stored in the data/ directory of this guide’s GitHub repository. "],["exercise1.html", "Exercise 1 Getting acquainted with R 1.1 Summary", " Exercise 1 Getting acquainted with R In this exercise we will use R to read a dataset and produce some descriptive statistics, produce some charts, and perform some simple statistical inference. The aim of the exercise is for you to become familiar with R and some basic R functions and objects. The first thing we will do, after starting R, is issue a command to retrieve an example dataset:   fem &lt;- read.table(&quot;fem.dat&quot;, header = TRUE)   This command illustrates some key things about the way R works. We are instructing R to assign (using the &lt;- operator) the output of the read.table() function to an object called fem. The fem object will contain the data held in the file fem.dat as an R data.frame object:   class(fem) ## [1] &quot;data.frame&quot; You can inspect the contents of the fem data.frame (or any other R object) just by typing its name:   fem ## ID AGE IQ ANX DEP SLP SEX LIFE WT ## 1 1 39 94 2 2 2 1 1 2.23 ## 2 2 41 89 2 2 2 1 1 1.00 ## 3 3 42 83 3 3 2 1 1 1.82 ## 4 4 30 99 2 2 2 1 1 -1.18 ## 5 5 35 94 2 1 1 1 2 -0.14 ## 6 6 44 90 NA 1 2 2 2 0.41   Note that the fem object is built from other objects. These are the named vectors (columns) in the dataset:   names(fem) ## [1] &quot;ID&quot; &quot;AGE&quot; &quot;IQ&quot; &quot;ANX&quot; &quot;DEP&quot; &quot;SLP&quot; &quot;SEX&quot; &quot;LIFE&quot; &quot;WT&quot;   The [1] displayed before the column names refers to the numbered position of the first name in the output. These positions are known as indexes and can be used to refer to individual items. For example:   names(fem)[1] ## [1] &quot;ID&quot;   names(fem)[8] ## [1] &quot;LIFE&quot;   names(fem)[2:4] ## [1] &quot;AGE&quot; &quot;IQ&quot; &quot;ANX&quot; The data consist of 118 records:   nrow(fem) ## [1] 118   each with nine variables:   ncol(fem) ## [1] 9   for female psychiatric patients.   The columns in the dataset are:   ID Patient ID AGE Age in years IQ IQ score ANX Anxiety (1=none, 2=mild, 3=moderate, 4=severe) DEP Depression (1=none, 2=mild, 3=moderate or severe) SLP Sleeping normally (1=yes, 2=no) SEX Lost interest in sex (1=yes, 2=no) LIFE Considered suicide (1=yes, 2=no) WT Weight change (kg) in previous 6 months The first ten records of the fem data.frame are:   ## ID AGE IQ ANX DEP SLP SEX LIFE WT ## 1 1 39 94 2 2 2 1 1 2.23 ## 2 2 41 89 2 2 2 1 1 1.00 ## 3 3 42 83 3 3 2 1 1 1.82 ## 4 4 30 99 2 2 2 1 1 -1.18 ## 5 5 35 94 2 1 1 1 2 -0.14 ## 6 6 44 90 NA 1 2 2 2 0.41 ## 7 7 31 94 2 2 NA 1 1 -0.68 ## 8 8 39 87 3 2 2 1 2 1.59 ## 9 9 35 -99 3 2 2 1 1 -0.55 ## 10 10 33 92 2 2 2 1 1 0.36   You may check this by asking R to display all columns of the first ten records in the fem data.frame:   fem[1:10, ] ## ID AGE IQ ANX DEP SLP SEX LIFE WT ## 1 1 39 94 2 2 2 1 1 2.23 ## 2 2 41 89 2 2 2 1 1 1.00 ## 3 3 42 83 3 3 2 1 1 1.82 ## 4 4 30 99 2 2 2 1 1 -1.18 ## 5 5 35 94 2 1 1 1 2 -0.14 ## 6 6 44 90 NA 1 2 2 2 0.41 ## 7 7 31 94 2 2 NA 1 1 -0.68 ## 8 8 39 87 3 2 2 1 2 1.59 ## 9 9 35 -99 3 2 2 1 1 -0.55 ## 10 10 33 92 2 2 2 1 1 0.36 The space after the comma is optional. You can think of it as a placeholder for where you would specify the indexes for columns you wanted to display. For example:   fem[1:10,2:4]   displays the first ten rows and the second, third and fourth columns of the fem data.frame:   ## AGE IQ ANX ## 1 39 94 2 ## 2 41 89 2 ## 3 42 83 3 ## 4 30 99 2 ## 5 35 94 2 ## 6 44 90 NA ## 7 31 94 2 ## 8 39 87 3 ## 9 35 -99 3 ## 10 33 92 2   NA is a special value meaning not available or missing. You can access the contents of a single column by name:   fem$IQ ## [1] 94 89 83 99 94 90 94 87 -99 92 92 94 91 86 90 -99 91 82 ## [19] 86 88 97 96 95 87 103 -99 91 87 91 89 92 84 94 92 96 96 ## [37] 86 92 102 82 92 90 92 88 98 93 90 91 -99 92 92 91 91 86 ## [55] 95 91 96 100 99 89 89 98 98 103 91 91 94 91 85 92 96 90 ## [73] 87 95 95 87 95 88 94 -99 -99 87 92 86 93 92 106 93 95 95 ## [91] 92 98 92 88 85 92 84 92 91 86 92 89 -99 96 97 92 92 98 ## [109] 91 91 89 94 90 96 87 86 89 -99   fem$IQ[1:10] ## [1] 94 89 83 99 94 90 94 87 -99 92 The $ sign is used to separate the name of the data.frame and the name of the column of interest. Note that R is case-sensitive so that IQ and iq are not the same. You can also access rows, columns, and individual cells by specifying row and column positions. For example, the IQ column is the third column in the fem data.frame:   fem[ ,3] ## [1] 94 89 83 99 94 90 94 87 -99 92 92 94 91 86 90 -99 91 82 ## [19] 86 88 97 96 95 87 103 -99 91 87 91 89 92 84 94 92 96 96 ## [37] 86 92 102 82 92 90 92 88 98 93 90 91 -99 92 92 91 91 86 ## [55] 95 91 96 100 99 89 89 98 98 103 91 91 94 91 85 92 96 90 ## [73] 87 95 95 87 95 88 94 -99 -99 87 92 86 93 92 106 93 95 95 ## [91] 92 98 92 88 85 92 84 92 91 86 92 89 -99 96 97 92 92 98 ## [109] 91 91 89 94 90 96 87 86 89 -99   fem[9, ] ## ID AGE IQ ANX DEP SLP SEX LIFE WT ## 9 9 35 -99 3 2 2 1 1 -0.55   fem[9,3] ## [1] -99   There are missing values in the IQ column which are all coded as -99. Before proceeding we must set these to the special NA value:   fem$IQ[fem$IQ == -99] &lt;- NA   The term inside the square brackets is also an index. This type of index is used to refer to subsets of data held in an object that meet a particular condition. In this case we are instructing R to set the contents of the IQ variable to NA if the contents of the IQ variable is -99. Check that this has worked:   fem$IQ ## [1] 94 89 83 99 94 90 94 87 NA 92 92 94 91 86 90 NA 91 82 ## [19] 86 88 97 96 95 87 103 NA 91 87 91 89 92 84 94 92 96 96 ## [37] 86 92 102 82 92 90 92 88 98 93 90 91 NA 92 92 91 91 86 ## [55] 95 91 96 100 99 89 89 98 98 103 91 91 94 91 85 92 96 90 ## [73] 87 95 95 87 95 88 94 NA NA 87 92 86 93 92 106 93 95 95 ## [91] 92 98 92 88 85 92 84 92 91 86 92 89 NA 96 97 92 92 98 ## [109] 91 91 89 94 90 96 87 86 89 NA   We can now compare the groups who have and have not considered suicide. For example:   by(fem$IQ, fem$LIFE, summary)   Look at the help for the by() function:   help(by)   Note that you may use ?by as a shortcut for help(by). The by() function applies another function (in this case the summary() function) to a column in a data.frame (in this case fem$IQ) split by the value of another variable (in this case fem$LIFE). It can be tedious to always have to specify a data.frame each time we want to use a particular variable. We can fix this problem by ‘attaching’ the data.frame:   attach(fem)   We can now refer to the columns in the fem data.frame without having to specify the name of the data.frame. This time we will produce summary statistics for WT by LIFE:   by(WT, LIFE, summary) ## LIFE: 1 ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -2.2300 -0.2700 1.0000 0.7867 1.7300 3.7700 4 ## ------------------------------------------------------------ ## LIFE: 2 ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -1.6800 -0.4500 0.6400 0.6404 1.5000 2.9500 7   We can view the same data as a box and whisker plot:   boxplot(WT ~ LIFE) We can add axis labels and a title to the graph: boxplot(WT ~ LIFE, xlab = &quot;Life&quot;, ylab = &quot;Weight&quot;, main = &quot;Weight BY Life&quot;)   A more descriptive title might be “Weight Change BY Considered Suicide.” The groups do not seem to differ much in their medians and the distributions appear to be reasonably symmetrical about their medians with a similar spread of values. We can look at the distribution as histograms: hist(WT[LIFE == 1])   hist(WT[LIFE == 2]) and check the assumption of normality using quantile-quantile plots: qqnorm(WT[LIFE == 1]) qqline(WT[LIFE == 1])   qqnorm(WT[LIFE == 2]) qqline(WT[LIFE == 2]) or by using a formal test: shapiro.test(WT[LIFE == 1]) ## ## Shapiro-Wilk normality test ## ## data: WT[LIFE == 1] ## W = 0.98038, p-value = 0.4336   shapiro.test(WT[LIFE == 2]) ## ## Shapiro-Wilk normality test ## ## data: WT[LIFE == 2] ## W = 0.97155, p-value = 0.3292   Remember that we can use the by() function to apply a function to a data.frame, including statistical functions such as shapiro.test():   by(WT, LIFE, shapiro.test) ## LIFE: 1 ## ## Shapiro-Wilk normality test ## ## data: dd[x, ] ## W = 0.98038, p-value = 0.4336 ## ## ------------------------------------------------------------ ## LIFE: 2 ## ## Shapiro-Wilk normality test ## ## data: dd[x, ] ## W = 0.97155, p-value = 0.3292 We can also test whether the variances differ significantly using Bartlett’s test for the homogeneity of variances:   bartlett.test(WT, LIFE) ## ## Bartlett test of homogeneity of variances ## ## data: WT and LIFE ## Bartlett&#39;s K-squared = 0.32408, df = 1, p-value = 0.5692   There is no significant difference between the two variances. Many functions in R have a formula interface that may be used to specify multiple variables and the relations between multiple variables. We could have used the formula interface with the bartlett.test() function:   bartlett.test(WT ~ LIFE) ## ## Bartlett test of homogeneity of variances ## ## data: WT by LIFE ## Bartlett&#39;s K-squared = 0.32408, df = 1, p-value = 0.5692   Having checked the normality and homogeneity of variance assumptions we can proceed to carry out a t-test:   t.test(WT ~ LIFE, var.equal = TRUE) ## ## Two Sample t-test ## ## data: WT by LIFE ## t = 0.59869, df = 104, p-value = 0.5507 ## alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0 ## 95 percent confidence interval: ## -0.3382365 0.6307902 ## sample estimates: ## mean in group 1 mean in group 2 ## 0.7867213 0.6404444   There is no evidence that the two groups differ in weight change in the previous six months. We could still have performed a t-test if the variances were not homogenous by setting the var.equal parameter of the t.test() function to FALSE:   t.test(WT ~ LIFE, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: WT by LIFE ## t = 0.60608, df = 98.866, p-value = 0.5459 ## alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0 ## 95 percent confidence interval: ## -0.3326225 0.6251763 ## sample estimates: ## mean in group 1 mean in group 2 ## 0.7867213 0.6404444   or performed a non-parametric test:   wilcox.test(WT ~ LIFE) ## ## Wilcoxon rank sum test with continuity correction ## ## data: WT by LIFE ## W = 1488, p-value = 0.4622 ## alternative hypothesis: true location shift is not equal to 0 An alternative, and more general, non-parametric test is:   kruskal.test(WT ~ LIFE) ## ## Kruskal-Wallis rank sum test ## ## data: WT by LIFE ## Kruskal-Wallis chi-squared = 0.54521, df = 1, p-value = 0.4603   We can use the table() function to examine the differences in depression between the two groups:   table(DEP, LIFE) ## LIFE ## DEP 1 2 ## 1 0 26 ## 2 42 24 ## 3 16 1   The two distributions look very different from each other. We can test this using a chi-square test on the table:   chisq.test(table(DEP, LIFE)) ## ## Pearson&#39;s Chi-squared test ## ## data: table(DEP, LIFE) ## X-squared = 43.876, df = 2, p-value = 2.968e-10 Note that we passed the output of the table() function directly to the chisq.test() function. We could have saved the table as an object first and then passed the object to the chisq.test() function:   tab &lt;- table(DEP, LIFE) chisq.test(tab) ## ## Pearson&#39;s Chi-squared test ## ## data: tab ## X-squared = 43.876, df = 2, p-value = 2.968e-10   The tab object contains the output of the table() function:   class(tab) ## [1] &quot;table&quot; tab ## LIFE ## DEP 1 2 ## 1 0 26 ## 2 42 24 ## 3 16 1   We can pass this table object to another function. For example:   fisher.test(tab) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: tab ## p-value = 1.316e-12 ## alternative hypothesis: two.sided   When we are finished with the tab object we can delete it using the rm() function:   rm(tab)   You can see a list of available objects using the ls() function:   ls() ## [1] &quot;fem&quot;   This should just show the fem object. We can examine the association between loss of interest in sex and considering suicide in the same way:   tab &lt;- table(SEX, LIFE) tab ## LIFE ## SEX 1 2 ## 1 58 38 ## 2 5 12 fisher.test(tab) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: tab ## p-value = 0.03175 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.080298 14.214482 ## sample estimates: ## odds ratio ## 3.620646 Note that with a two-by-two table the fisher.test() function produces an estimate of, and confidence intervals for, the odds ratio. Again, we will delete the tab object:   rm(tab)   We could have performed the Fisher exact test without creating the tab object by passing the output of the table() function directly to the fisher.test() function:   fisher.test(table(SEX, LIFE)) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(SEX, LIFE) ## p-value = 0.03175 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.080298 14.214482 ## sample estimates: ## odds ratio ## 3.620646   Choose whichever method you find easiest but remember that it is easy to save the results of any function for later use. We can explore the correlation between two variables using the cor() function:   cor(IQ, WT, use = &quot;pairwise.complete.obs&quot;) ## [1] -0.2917158 or by using a scatter plot:   plot(IQ, WT)   and by a formal test:   cor.test(IQ, WT) ## ## Pearson&#39;s product-moment correlation ## ## data: IQ and WT ## t = -3.0192, df = 98, p-value = 0.003231 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.4616804 -0.1010899 ## sample estimates: ## cor ## -0.2917158 With some functions you can pass an entire data.frame rather than a list of variables:   cor(fem, use = &quot;pairwise.complete.obs&quot;) pairs(fem) ## ID AGE IQ ANX DEP ## ID 1.00000000 0.03069077 0.0370598672 -0.02941825 -0.0554147209 ## AGE 0.03069077 1.00000000 -0.4345435680 0.06734300 -0.0387049246 ## IQ 0.03705987 -0.43454357 1.0000000000 -0.02323787 -0.0001307404 ## ANX -0.02941825 0.06734300 -0.0232378691 1.00000000 0.5437946347 ## DEP -0.05541472 -0.03870492 -0.0001307404 0.54379463 1.0000000000 ## SLP -0.07268743 0.02606547 0.0812993104 0.22317875 0.5248724551 ## SEX 0.08999634 0.10609216 -0.0536558660 -0.21062493 -0.3058422258 ## LIFE -0.05604349 -0.10300193 -0.0915396469 -0.34211268 -0.6139017253 ## WT 0.02640131 0.41574411 -0.2917157832 0.11817532 0.0233742465 ## SLP SEX LIFE WT ## ID -0.072687434 0.08999634 -0.05604349 0.026401310 ## AGE 0.026065468 0.10609216 -0.10300193 0.415744109 ## IQ 0.081299310 -0.05365587 -0.09153965 -0.291715783 ## ANX 0.223178752 -0.21062493 -0.34211268 0.118175321 ## DEP 0.524872455 -0.30584223 -0.61390173 0.023374247 ## SLP 1.000000000 -0.29053971 -0.35186578 -0.009259774 ## SEX -0.290539709 1.00000000 0.22316967 -0.027826514 ## LIFE -0.351865775 0.22316967 1.00000000 -0.058605326 ## WT -0.009259774 -0.02782651 -0.05860533 1.000000000 The output can be a little confusing particularly if it includes categorical or record identifying variables. To avoid this we can create a new object that contains only the columns we are interested in using the column binding cbind() function:   newfem &lt;- cbind(AGE, IQ, WT) cor(newfem, use = &quot;pairwise.complete.obs&quot;) pairs(newfem) ## AGE IQ WT ## AGE 1.0000000 -0.4345436 0.4157441 ## IQ -0.4345436 1.0000000 -0.2917158 ## WT 0.4157441 -0.2917158 1.0000000   When we have finished with the newfem object we can delete it:   rm(newfem)   There was no real need to create the newfem object as we could have fed the output of the cbind() function directly to the cor() or pairs() function:   cor(cbind(AGE, IQ, WT), use = &quot;pairwise.complete.obs&quot;) pairs(cbind(AGE, IQ, WT)) ## AGE IQ WT ## AGE 1.0000000 -0.4345436 0.4157441 ## IQ -0.4345436 1.0000000 -0.2917158 ## WT 0.4157441 -0.2917158 1.0000000   It is, however, easier to work with the newfem object rather than having to retype the cbind() function. This is particularly true if you wanted to continue with an analysis of just the three variables. The relationship between AGE and WT can be plotted using the plot() function:   plot(AGE, WT) And tested using the cor() and cor.test() functions:   cor(AGE, WT, use = &quot;pairwise.complete.obs&quot;) ## [1] 0.4157441 cor.test(AGE, WT) ## ## Pearson&#39;s product-moment correlation ## ## data: AGE and WT ## t = 4.6841, df = 105, p-value = 8.457e-06 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.2452434 0.5612979 ## sample estimates: ## cor ## 0.4157441 Or by using the linear modelling lm() function:   summary(lm(WT ~ AGE)) ## ## Call: ## lm(formula = WT ~ AGE) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.10678 -0.85922 -0.05453 0.71434 2.70874 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.25405 0.85547 -3.804 0.00024 *** ## AGE 0.10592 0.02261 4.684 8.46e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.128 on 105 degrees of freedom ## (11 observations deleted due to missingness) ## Multiple R-squared: 0.1728, Adjusted R-squared: 0.165 ## F-statistic: 21.94 on 1 and 105 DF, p-value: 8.457e-06   We use the summary() function here to extract summary information from the output of the lm() function. It is often more useful to use lm() to create an object:   fem.lm &lt;- lm(WT ~ AGE) And use the output in other functions:   summary(fem.lm) ## ## Call: ## lm(formula = WT ~ AGE) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.10678 -0.85922 -0.05453 0.71434 2.70874 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.25405 0.85547 -3.804 0.00024 *** ## AGE 0.10592 0.02261 4.684 8.46e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.128 on 105 degrees of freedom ## (11 observations deleted due to missingness) ## Multiple R-squared: 0.1728, Adjusted R-squared: 0.165 ## F-statistic: 21.94 on 1 and 105 DF, p-value: 8.457e-06 plot(AGE, WT) abline(fem.lm)   In this case we are passing the intercept and slope information held in the fem.lm object to the abline() function which draws a regression line. The abline() function adds to an existing plot. This means that you need to keep the scatter plot of AGE and WT open before issuing the abline() function call. A useful function to apply to the fem.lm object is plot() which produces diagnostic plots of the linear model:   plot(fem.lm) Objects created by the lm() function (or any of the modelling functions) can use up a lot of memory so we should remove them when we no longer need them:   rm(fem.lm)   It might be interesting to see whether a similar relationship exists between AGE and WT for those who have and have not considered suicide. This can be done using the coplot() function:   coplot(WT ~ AGE | as.factor(LIFE)) ## ## Missing rows: 21, 22, 31, 43, 44, 45, 69, 81, 101, 104, 114, 115 The two plots looks similar. We could also use coplot() to investigate the relationship between AGE and WT for categories of both LIFE and SEX:   coplot(WT ~ AGE | as.factor(LIFE) * as.factor(SEX)) ## ## Missing rows: 12, 17, 21, 22, 31, 43, 44, 45, 66, 69, 81, 101, 104, 105, 114, 115   although the numbers are too small for this to be useful here. We used the as.factor() function with the coplot() function to ensure that R was aware that the LIFE and SEX columns hold categorical data. We can check the way variables are stored using the data.class() function:   data.class(fem$SEX) ## [1] &quot;numeric&quot; We can ‘apply’ this function to all columns in a data.frame using the sapply() function:   sapply(fem, data.class) ## ID AGE IQ ANX DEP SLP SEX LIFE ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## WT ## &quot;numeric&quot;   The sapply() function is part of a group of functions that apply a specified function to data objects: Function(s) Applies a function to … apply() rows and columns of matrices, arrays, and tables lapply() components of lists and data.frames sapply() components of lists and data.frames mapply() components of lists and data.frames tapply() subsets of data Related functions are aggregate() which compute summary statistics for subsets of data, by() which applies a function to a data.frame split by factors, and sweep() which applies a function to an array. The parameters of most R functions have default values. These are usually the most used and most useful parameter values for each function. The cor.test() function, for example, calculates Pearson’s product moment correlation coefficient by default. This is an appropriate measure for data from a bivariate normal distribution. The DEP and ANX variables contain ordered data. An appropriate measure of correlation between DEP and ANX is Kendall’s tau. This can be obtained using:   cor.test(DEP, ANX, method = &quot;kendall&quot;) ## ## Kendall&#39;s rank correlation tau ## ## data: DEP and ANX ## z = 5.5606, p-value = 2.689e-08 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.4950723   Before we finish we should save the fem data.frame so that next time we want to use it we will not have to bother with recoding the missing values to the special NA value. This is done with the write.table() function:   write.table(fem, file = &quot;newfem.dat&quot;, row.names = FALSE)   Everything in R is either a function or an object. Even the command to quit R is a function:   q()   When you call the q() function you will be asked if you want to save the workspace image. If you save the workspace image then all of the objects and functions currently available to you will be saved. These will then be automatically restored the next time you start R in the current working directory. For this exercise there is no need to save the workspace image so click the No or Don’t Save button (GUI) or enter n when prompted to save the workspace image (terminal). 1.1 Summary R is a functional system. Everything is done by calling functions. R provides a large set of functions for descriptive statistics, charting, and statistical inference. Functions can be chained together so that the output of one function is the input of another function. R is an object oriented system. We can use functions to create objects that can then be manipulated or passed to other functions for subsequent analysis. "],["exercise2.html", "Exercise 2 Manipulating objects and creating new functions 2.1 Summary", " Exercise 2 Manipulating objects and creating new functions In this exercise we will explore how to manipulate R objects and how to write functions that can manipulate and extract data and information from R objects and produce useful analyses. Before we go any further we should start R and retrieve a dataset:   salex &lt;- read.table(&quot;salex.dat&quot;, header = TRUE, na.strings = &quot;9&quot;)   Missing values are coded as 9 throughout this dataset so we can use the na.strings parameter of the read.table() function to replace all 9’s with the special NA code when we retrieve the dataset. Check that this works by examining the salex data.frame:   salex names(salex) ## ILL HAM BEEF EGGS MUSHROOM PEPPER PORKPIE PASTA RICE LETTUCE TOMATO COLESLAW ## 1 1 1 1 1 1 1 2 2 2 2 2 2 ## 2 1 1 1 1 2 2 1 2 2 2 1 2 ## 3 1 1 1 1 1 1 1 1 1 1 2 2 ## 4 1 1 1 1 2 2 2 2 2 1 1 2 ## 5 1 1 1 1 1 1 1 1 1 1 1 1 ## 6 1 1 1 1 2 2 2 2 2 2 1 1 ## CRISPS PEACHCAKE CHOCOLATE FRUIT TRIFLE ALMONDS ## 1 2 2 2 2 2 2 ## 2 2 2 2 2 2 2 ## 3 1 2 1 2 2 2 ## 4 2 2 1 2 2 2 ## 5 2 2 1 2 1 2 ## 6 1 2 1 2 2 2 ## [1] &quot;ILL&quot; &quot;HAM&quot; &quot;BEEF&quot; &quot;EGGS&quot; &quot;MUSHROOM&quot; &quot;PEPPER&quot; ## [7] &quot;PORKPIE&quot; &quot;PASTA&quot; &quot;RICE&quot; &quot;LETTUCE&quot; &quot;TOMATO&quot; &quot;COLESLAW&quot; ## [13] &quot;CRISPS&quot; &quot;PEACHCAKE&quot; &quot;CHOCOLATE&quot; &quot;FRUIT&quot; &quot;TRIFLE&quot; &quot;ALMONDS&quot;   This data comes from a food-borne outbreak. On Saturday 17th October 1992, eighty-two people attended a buffet meal at a sports club. Within fourteen to twenty-four hours, fifty-one of the participants developed diarrhoea, with nausea, vomiting, abdominal pain and fever. The columns in the dataset are as follows: ILL Ill or not-ill HAM Baked ham BEEF Roast beef EGGS Eggs MUSHROOM Mushroom flan PEPPER Pepper flan PORKPIE Pork pie PASTA Pasta salad RICE Rice salad LETTUCE Lettuce TOMATO Tomato salad COLESLAW Coleslaw CRISPS Crisps PEACHCAKE Peach cake CHOCOLATE Chocolate cake FRUIT Tropical fruit salad TRIFLE Trifle ALMONDS Almonds Data is available for seventy-seven of the eighty-two people who attended the sports club buffet. All of the variables are coded 1=yes, 2=no. We can use the attach() function to make it easier to access our data:   attach(salex)   The two-by-two table is a basic epidemiological tool. In analysing data from a food-borne outbreak collected as a retrospective cohort study, for example, we would tabulate each exposure (suspect foodstuffs) against the outcome (illness) and calculate risk ratios and confidence intervals. R has no explicit function to calculate risk ratios from two-by-two tables but we can easily write one ourselves. The first step in writing such a function would be to create the two-by-two table. This can be done with the table() function. We will use a table of HAM by ILL as an illustration:   table(HAM, ILL)   This command produces the following output:   ## ILL ## HAM 1 2 ## 1 46 17 ## 2 5 9   We can manipulate the output directly but it is easier if we instruct R to save the output of the table() function in an object:   tab &lt;- table(HAM, ILL) The tab object contains the output of the table() function:   tab ## ILL ## HAM 1 2 ## 1 46 17 ## 2 5 9   As it is stored in an object we can examine its contents on an item by item basis. The tab object is an object of class table:   class(tab) ## [1] &quot;table&quot;   We can extract data from a table object by using indices or row and column co-ordinates:   tab[1,1] tab[1,2] tab[2,1] ## [1] 46 ## [1] 17 ## [1] 5   The numbers in the square brackets refer to the position (as row and column co-ordinates) of the data item in the table not the values of the variables. We can extract data using the values of the row and column variables by enclosing the index values in double quotes (“). For example:   tab[&quot;1&quot;,&quot;1&quot;] ## [1] 46 The two methods of extracting data may be combined. For example:   tab[1,&quot;1&quot;] ## [1] 46   We can calculate a risk ratio using the extracted data:   (tab[1,1]/(tab[1,1]+tab[1,2]))/(tab[2,1]/(tab[2,1]+tab[2,2]))   Which returns a risk ratio of   ## [1] 2.044444   This is a tedious calculation to have to type in every time you need to calculate a risk ratio from a two-by-two table. It would be better to have a function that calculates and displays the risk ratio automatically. Fortunately, R allows us to do just that. The function() function allows us to create new functions in R:   tab2by2 &lt;- function(exposure, outcome) {}   This creates an empty function called tab2by2 that expects two parameters called exposure and outcome. We could type the whole function in at the R command prompt but it is easier to use a text editor:   fix(tab2by2)   This will start an editor with the empty tab2by2() function already loaded. We can now edit this function to make it do something useful: function(exposure, outcome) { tab &lt;- table(exposure, outcome) a &lt;- tab[1,1] b &lt;- tab[1,2] c &lt;- tab[2,1] d &lt;- tab[2,2] rr &lt;- (a / (a + b)) / (c / (c + d)) print(tab) print(rr) }   Once you have made the changes shown above, check your work, save the file, and quit the editor. Before proceeding we should examine the tab2by2() function to make sure we understand what the function will do: The first line defines tab2by2 as a function that expects to be given two parameters which are called exposure and outcome. The body of the function (i.e. the work of the function) is enclosed within curly brackets ({}). The first line of the body of the function creates a table object (tab) using the variables specified when the tab2by2() function is called (these are the parameters exposure and outcome). The next line creates four new objects (called a, b, c, and d) which contain the values of the four cells in the two-by-two table. The following line calculates the risk ratio using the objects a, b, c, and d and stores the result of the calculation in an object called rr. The final two lines print the contents of the tab and rr objects. Let’s try the tab2by2() function with our test data:   tab2by2(HAM, ILL) ## outcome ## exposure 1 2 ## 1 46 17 ## 2 5 9 ## [1] 2.044444   The tab2by2() function displays a table of HAM by ILL followed by the risk ratio calculated from the data in the table. Try producing another table:   tab2by2(PASTA, ILL) ## outcome ## exposure 1 2 ## 1 25 3 ## 2 26 23 ## [1] 1.682692   Have a look at the R objects available to you:   ls() ## [1] &quot;fem&quot; &quot;salex&quot; &quot;tab&quot; &quot;tab2by2&quot;   Note that there are no a, b, c, d, or rr objects. Examine the tab object:   tab ## ILL ## HAM 1 2 ## 1 46 17 ## 2 5 9   This is the table of HAM by ILL that you created earlier not the table of PASTA by ILL that was created by the tab2by2() function. The tab, a, b, c, d, and rr objects in the tab2by2() function are local to that function and do not change anything outside of that function. This means that the tab object inside the function is independent of any object of the same name outside of the function. When a function completes its work, all of the objects that are local to that function are automatically removed. This is useful as it means that you can use object names inside functions that will not interfere with objects of the same name that are stored elsewhere. It also means that you do not clutter up the R workspace with temporary objects. Just to prove that tab in the tab2by2() function exists only in the tab2by2() function we can delete the tab object from the R workspace:   rm(tab) Now try another call to the tab2by2() function:   tab2by2(FRUIT, ILL) ## outcome ## exposure 1 2 ## 1 1 4 ## 2 49 22 ## [1] 0.2897959 Now list the R objects available to you:   ls() ## [1] &quot;fem&quot; &quot;salex&quot; &quot;tab2by2&quot;   Note that there are no tab, a, b, c, d, or rr objects. The tab2by2() function is very limited. It only displays a table and calculates and displays a simple ratio. A more useful function would also calculate and display a confidence interval for the risk ratio. This is what we will do now. Use the fix() function to edit the tab2by2() function:   fix(tab2by2)   We can now edit this function to calculate and display a 95% confidence interval for the risk ratio.   function(exposure, outcome) { tab &lt;- table(exposure, outcome) a &lt;- tab[1,1] b &lt;- tab[1,2] c &lt;- tab[2,1] d &lt;- tab[2,2] rr &lt;- (a / (a + b)) / (c / (c + d)) se.log.rr &lt;- sqrt((b / a) / (a + b) + (d / c) / (c + d)) lci.rr &lt;- exp(log(rr) - 1.96 * se.log.rr) uci.rr &lt;- exp(log(rr) + 1.96 * se.log.rr) print(tab) print(rr) print(lci.rr) print(uci.rr) } Once you have made the changes shown above, check your work, save the file, and quit the editor. We should test our revised function:   tab2by2(EGGS, ILL)   which produces the following output:   ## outcome ## exposure 1 2 ## 1 40 6 ## 2 10 20 ## [1] 2.608696 ## [1] 1.553564 ## [1] 4.38044   The function works but the output could be improved. Use the fix() function to edit the tab2by2() function:   function(exposure, outcome) { tab &lt;- table(exposure, outcome) a &lt;- tab[1,1] b &lt;- tab[1,2] c &lt;- tab[2,1] d &lt;- tab[2,2] rr &lt;- (a / (a + b)) / (c / (c + d)) se.log.rr &lt;- sqrt((b / a) / (a + b) + (d / c) / (c + d)) lci.rr &lt;- exp(log(rr) - 1.96 * se.log.rr) uci.rr &lt;- exp(log(rr) + 1.96 * se.log.rr) print(tab) cat(&quot;\\nRR :&quot;, rr, &quot;\\n95% CI :&quot;, lci.rr, uci.rr, &quot;\\n&quot;) }   Once you have made the changes shown above, save the file and quit the editor. Now we can test our function again:   tab2by2(EGGS, ILL)   Which produces the following output:   ## outcome ## exposure 1 2 ## 1 40 6 ## 2 10 20 ## ## RR : 2.608696 ## 95% CI : 1.553564 4.38044   The tab2by2() function displays output but does not behave like a standard R function in the sense that you cannot save the results of the tab2by2() function into an object:   test2by2 &lt;- tab2by2(EGGS, ILL) ## outcome ## exposure 1 2 ## 1 40 6 ## 2 10 20 ## ## RR : 2.608696 ## 95% CI : 1.553564 4.38044   displays output but does not save anything in the test2by2 object:   test2by2 ## NULL The returned value (NULL) means that test2by2 is an empty object. We will not worry about this at the moment as the tab2by2() function is good-enough for our current purposes. In Exercise 6 we will explore how to make our own functions behave like standard R functions. We will now add the calculation of the odds ratio and its 95% confidence interval to the tab2by2() function using the fix() function. There are two ways of doing this. We could either calculate the odds ratio from the table and use (e.g.) the method of Woolf to calculate the confidence interval:   or &lt;- (a / b) / (c / d) se.log.or &lt;- sqrt(1 / a + 1 / b + 1 / c + 1 / d) lci.or &lt;- exp(log(or) - 1.96 * se.log.or) uci.or &lt;- exp(log(or) + 1.96 * se.log.or) cat(&quot;\\nOR :&quot;, or, &quot;\\n95% CI :&quot;, lci.or, uci.or, &quot;\\n&quot;)   or use the output of the fisher.test() function:   ft &lt;- fisher.test(tab) cat(&quot;\\nOR :&quot;, ft$estimate, &quot;\\n95% CI :&quot;, ft$conf.int, &quot;\\n&quot;)   Note that we can refer to components of a function’s output using the same syntax as when we refer to columns in a data.frame (e.g. ft$estimate to examine the estimate of the odds ratio from the fisher.test() function stored in the object ft). The names of elements in the output of a standard function such as fisher.test() can be found in the documentation or the help system. For example:   help(fisher.test) Output elements are listed under the Value heading. Revise the tab2by2() function to include the calculation of the odds ratio and the 95% confidence interval. The revised function will look something like this:   function(exposure, outcome) { tab &lt;- table(exposure, outcome) a &lt;- tab[1,1] b &lt;- tab[1,2] c &lt;- tab[2,1] d &lt;- tab[2,2] rr &lt;- (a / (a + b)) / (c / (c + d)) se.log.rr &lt;- sqrt((b / a) / (a + b) + (d / c) / (c + d)) lci.rr &lt;- exp(log(rr) - 1.96 * se.log.rr) uci.rr &lt;- exp(log(rr) + 1.96 * se.log.rr) or &lt;- (a / b) / (c / d) se.log.or &lt;- sqrt(1 / a + 1 / b + 1 / c + 1 / d) lci.or &lt;- exp(log(or) - 1.96 * se.log.or) uci.or &lt;- exp(log(or) + 1.96 * se.log.or) ft &lt;- fisher.test(tab) cat(&quot;\\n&quot;) print(tab) cat(&quot;\\nRelative Risk :&quot;, rr, &quot;\\n95% CI :&quot;, lci.rr, uci.rr, &quot;\\n&quot;) cat(&quot;\\nSample Odds Ratio :&quot;, or, &quot;\\n95% CI :&quot;, lci.or, uci.or, &quot;\\n&quot;) cat(&quot;\\nMLE Odds Ratio :&quot;, ft$estimate, &quot;\\n95% CI :&quot;, ft$conf.int, &quot;\\n\\n&quot;) }   Once you have made the changes shown above, check your work, save the file, and quit the editor. Test the tab2by2() function when you have added the calculation of the odds ratio and its 95% confidence interval. Now that we have a function that will calculate risk ratios and odds ratios with confidence intervals from a two- by-two table we can use it to analyse the salex data:   tab2by2(HAM, ILL) tab2by2(BEEF, ILL) tab2by2(EGGS, ILL) tab2by2(MUSHROOM, ILL) tab2by2(PEPPER, ILL) tab2by2(PORKPIE, ILL) tab2by2(PASTA, ILL) tab2by2(RICE, ILL) tab2by2(LETTUCE, ILL) tab2by2(TOMATO, ILL) tab2by2(COLESLAW, ILL) tab2by2(CRISPS, ILL) tab2by2(PEACHCAKE, ILL) tab2by2(CHOCOLATE, ILL) tab2by2(FRUIT, ILL) tab2by2(TRIFLE, ILL) tab2by2(ALMONDS, ILL) ## ## outcome ## exposure 1 2 ## 1 46 17 ## 2 5 9 ## ## Relative Risk : 2.044444 ## 95% CI : 0.9964841 4.194501 ## ## Sample Odds Ratio : 4.870588 ## 95% CI : 1.428423 16.60756 ## ## MLE Odds Ratio : 4.75649 ## 95% CI : 1.22777 20.82921 ## ## outcome ## exposure 1 2 ## 1 45 22 ## 2 6 4 ## ## Relative Risk : 1.119403 ## 95% CI : 0.6568821 1.907592 ## ## Sample Odds Ratio : 1.363636 ## 95% CI : 0.3485746 5.334594 ## ## MLE Odds Ratio : 1.357903 ## 95% CI : 0.2547114 6.428414 ## ## outcome ## exposure 1 2 ## 1 40 6 ## 2 10 20 ## ## Relative Risk : 2.608696 ## 95% CI : 1.553564 4.38044 ## ## Sample Odds Ratio : 13.33333 ## 95% CI : 4.240168 41.92706 ## ## MLE Odds Ratio : 12.74512 ## 95% CI : 3.762787 50.05419 ## ## outcome ## exposure 1 2 ## 1 24 6 ## 2 25 19 ## ## Relative Risk : 1.408 ## 95% CI : 1.028944 1.926697 ## ## Sample Odds Ratio : 3.04 ## 95% CI : 1.037274 8.909506 ## ## MLE Odds Ratio : 2.995207 ## 95% CI : 0.9421008 10.7953 ## ## outcome ## exposure 1 2 ## 1 24 3 ## 2 23 22 ## ## Relative Risk : 1.73913 ## 95% CI : 1.26876 2.383882 ## ## Sample Odds Ratio : 7.652174 ## 95% CI : 2.013718 29.07844 ## ## MLE Odds Ratio : 7.448216 ## 95% CI : 1.861728 44.12015 ## ## outcome ## exposure 1 2 ## 1 21 9 ## 2 29 17 ## ## Relative Risk : 1.110345 ## 95% CI : 0.8044752 1.532509 ## ## Sample Odds Ratio : 1.367816 ## 95% CI : 0.5113158 3.659032 ## ## MLE Odds Ratio : 1.362228 ## 95% CI : 0.4636016 4.190667 ## ## outcome ## exposure 1 2 ## 1 25 3 ## 2 26 23 ## ## Relative Risk : 1.682692 ## 95% CI : 1.255392 2.255433 ## ## Sample Odds Ratio : 7.371795 ## 95% CI : 1.964371 27.66451 ## ## MLE Odds Ratio : 7.195422 ## 95% CI : 1.829867 42.07488 ## ## outcome ## exposure 1 2 ## 1 28 4 ## 2 23 22 ## ## Relative Risk : 1.711957 ## 95% CI : 1.250197 2.344268 ## ## Sample Odds Ratio : 6.695652 ## 95% CI : 2.017327 22.22335 ## ## MLE Odds Ratio : 6.532868 ## 95% CI : 1.852297 29.84928 ## ## outcome ## exposure 1 2 ## 1 28 1 ## 2 23 25 ## ## Relative Risk : 2.014993 ## 95% CI : 1.488481 2.727744 ## ## Sample Odds Ratio : 30.43478 ## 95% CI : 3.826938 242.041 ## ## MLE Odds Ratio : 29.32825 ## 95% CI : 4.161299 1284.306 ## ## outcome ## exposure 1 2 ## 1 29 9 ## 2 22 17 ## ## Relative Risk : 1.352871 ## 95% CI : 0.974698 1.877771 ## ## Sample Odds Ratio : 2.489899 ## 95% CI : 0.9347213 6.632562 ## ## MLE Odds Ratio : 2.459981 ## 95% CI : 0.8467562 7.558026 ## ## outcome ## exposure 1 2 ## 1 29 3 ## 2 21 23 ## ## Relative Risk : 1.89881 ## 95% CI : 1.366876 2.63775 ## ## Sample Odds Ratio : 10.5873 ## 95% CI : 2.806364 39.9417 ## ## MLE Odds Ratio : 10.26269 ## 95% CI : 2.600771 60.35431 ## ## outcome ## exposure 1 2 ## 1 21 10 ## 2 30 16 ## ## Relative Risk : 1.03871 ## 95% CI : 0.7529065 1.433004 ## ## Sample Odds Ratio : 1.12 ## 95% CI : 0.4258139 2.945888 ## ## MLE Odds Ratio : 1.118358 ## 95% CI : 0.3858206 3.340535 ## ## outcome ## exposure 1 2 ## 1 2 2 ## 2 49 24 ## ## Relative Risk : 0.744898 ## 95% CI : 0.27594 2.010846 ## ## Sample Odds Ratio : 0.4897959 ## 95% CI : 0.06497947 3.691936 ## ## MLE Odds Ratio : 0.4947099 ## 95% CI : 0.03393887 7.209143 ## ## outcome ## exposure 1 2 ## 1 12 2 ## 2 38 24 ## ## Relative Risk : 1.398496 ## 95% CI : 1.045064 1.871456 ## ## Sample Odds Ratio : 3.789474 ## 95% CI : 0.7791326 18.43089 ## ## MLE Odds Ratio : 3.733535 ## 95% CI : 0.7318646 37.28268 ## ## outcome ## exposure 1 2 ## 1 1 4 ## 2 49 22 ## ## Relative Risk : 0.2897959 ## 95% CI : 0.04985828 1.684408 ## ## Sample Odds Ratio : 0.1122449 ## 95% CI : 0.01185022 1.06318 ## ## MLE Odds Ratio : 0.1157141 ## 95% CI : 0.002240848 1.256134 ## ## outcome ## exposure 1 2 ## 1 19 5 ## 2 32 21 ## ## Relative Risk : 1.311198 ## 95% CI : 0.9718621 1.769016 ## ## Sample Odds Ratio : 2.49375 ## 95% CI : 0.8067804 7.708156 ## ## MLE Odds Ratio : 2.465794 ## 95% CI : 0.7363311 9.778463 ## ## outcome ## exposure 1 2 ## 1 3 3 ## 2 38 19 ## ## Relative Risk : 0.75 ## 95% CI : 0.3300089 1.7045 ## ## Sample Odds Ratio : 0.5 ## 95% CI : 0.09203498 2.716358 ## ## MLE Odds Ratio : 0.505905 ## 95% CI : 0.06170211 4.141891   Make a note of any positive associations (i.e. with a risk ratio &gt; 1 with a 95% confidence intervals that does not include one). We will use these for the next exercise when we will use logistic regression to analyse this data. Save the tab2by2() function:   save(tab2by2, file = &quot;tab2by2.r&quot;)   We can now quit R:   q()   For this exercise there is no need to save the workspace image so click the No or Don’t Save button (GUI) or enter n when prompted to save the workspace image (terminal). 2.1 Summary R objects contain information that can be examined and manipulated. R can be extended by writing new functions. New functions can perform simple or complex data analysis. New functions can be composed of parts of existing function. New functions can be saved and used in subsequent R sessions. Objects defined within functions are local to that function and only exist while that function is being used. This means that you can re-use meaningful names within functions without them interfering with each other. "],["exercise3.html", "Exercise 3 Logistic regression and stratified analysis 3.1 Matched data 3.2 Summary", " Exercise 3 Logistic regression and stratified analysis In this exercise we will explore how R handles generalised linear models using the example of logistic regression. We will continue using the salex dataset. Start R and retrieve the salex dataset:   salex &lt;- read.table(&quot;salex.dat&quot;, header = TRUE, na.strings = &quot;9&quot;)   When we analysed this data using two-by-two tables and examining the risk ratio and 95% confidence interval associated with each exposure we found many significant positive associations:   Variable RR 95% CI EGGS 2.61 1.55, 4.38 MUSHROOM 1.41 1.03, 1.93 PEPPER 1.74 1.27, 2.38 PASTA 1.68 1.26, 2.26 RICE 1.72 1.25, 2.34 LETTUCE 2.01 1.49, 2.73 COLESLAW 1.89 1.37, 2.64 CHOCOLATE 1.39 1.05, 1.87   Some of these associations may be due to confounding in the data. We can use logistic regression to help us identify independent associations. Logistic regression requires the dependent variable to be either 0 or 1. In order to perform a logistic regression we must first recode the ILL variable so that 0=no and 1=yes:   table(salex$ILL) salex$ILL[salex$ILL == 2] &lt;- 0 table(salex$ILL) ## ## 1 2 ## 51 26 ## ## 0 1 ## 26 51   We could work with our data as it is but if we wanted to calculate odds ratios and confidence intervals we would calculate their reciprocals (i.e. odds ratios for non-exposure rather than for exposure). This is because of the way the data has been coded (1=yes, 2=no). In order to calculate meaningful odds ratios the exposure variables should also be coded 0=no, 1=yes. The actual codes used are not important as long as the value used for ‘exposed’ is one greater than the value used for ‘not exposed.’ We could issue a series of commands similar to the one we have just used to recode the ILL variable. This is both tedious and unnecessary as the structure of the dataset (i.e. all variables are coded identically) allows us to recode all variables with a single command:   salex &lt;- read.table(&quot;salex.dat&quot;, header = TRUE, na.strings = &quot;9&quot;) salex[1:5, ] ## ILL HAM BEEF EGGS MUSHROOM PEPPER PORKPIE PASTA RICE LETTUCE TOMATO COLESLAW ## 1 1 1 1 1 1 1 2 2 2 2 2 2 ## 2 1 1 1 1 2 2 1 2 2 2 1 2 ## 3 1 1 1 1 1 1 1 1 1 1 2 2 ## 4 1 1 1 1 2 2 2 2 2 1 1 2 ## 5 1 1 1 1 1 1 1 1 1 1 1 1 ## CRISPS PEACHCAKE CHOCOLATE FRUIT TRIFLE ALMONDS ## 1 2 2 2 2 2 2 ## 2 2 2 2 2 2 2 ## 3 1 2 1 2 2 2 ## 4 2 2 1 2 2 2 ## 5 2 2 1 2 1 2 salex &lt;- 2 - salex salex[1:5, ] ## ILL HAM BEEF EGGS MUSHROOM PEPPER PORKPIE PASTA RICE LETTUCE TOMATO COLESLAW ## 1 1 1 1 1 1 1 0 0 0 0 0 0 ## 2 1 1 1 1 0 0 1 0 0 0 1 0 ## 3 1 1 1 1 1 1 1 1 1 1 0 0 ## 4 1 1 1 1 0 0 0 0 0 1 1 0 ## 5 1 1 1 1 1 1 1 1 1 1 1 1 ## CRISPS PEACHCAKE CHOCOLATE FRUIT TRIFLE ALMONDS ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 ## 3 1 0 1 0 0 0 ## 4 0 0 1 0 0 0 ## 5 0 0 1 0 1 0   WARNING : The attach() function works with a copy of the data.frame rather than the original data.frame. Commands that manipulate variables in a data.frame may not work as expected if the data.frame has been attached using the attach() function. It is better to manipulate data before attaching a data.frame. The detach() function may be used to remove an attachment prior to any data manipulation. Many R users avoid using the attach() function altogether. We can now use the generalised linear model glm() function to specify the logistic regression model:   salex.lreg &lt;- glm(formula = ILL ~ EGGS + MUSHROOM + PEPPER + PASTA + RICE + LETTUCE + COLESLAW + CHOCOLATE, family = binomial(logit), data = salex)   The method used by the glm() function is defined by the family parameter. Here we specify binomial errors and a logit (logistic) linking function. We have saved the output of the glm() function in the salex.lreg object. We can examine some basic information about the specified model using the summary() function:   summary(salex.lreg) ## ## Call: ## glm(formula = ILL ~ EGGS + MUSHROOM + PEPPER + PASTA + RICE + ## LETTUCE + COLESLAW + CHOCOLATE, family = binomial(logit), ## data = salex) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.92036 -0.49869 0.06877 0.40906 2.07182 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.021864 0.676606 -2.988 0.00281 ** ## EGGS 3.579366 1.267870 2.823 0.00476 ** ## MUSHROOM -3.584345 1.728999 -2.073 0.03817 * ## PEPPER 2.348074 1.428177 1.644 0.10015 ## PASTA 1.774818 1.162762 1.526 0.12692 ## RICE 0.114180 1.193840 0.096 0.92381 ## LETTUCE 3.401828 1.234060 2.757 0.00584 ** ## COLESLAW 0.763857 1.024373 0.746 0.45586 ## CHOCOLATE 0.009782 1.314683 0.007 0.99406 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 91.246 on 69 degrees of freedom ## Residual deviance: 41.260 on 61 degrees of freedom ## (7 observations deleted due to missingness) ## AIC: 59.26 ## ## Number of Fisher Scoring iterations: 7   We will use backwards elimination to remove non-significant variables from the logistic regression model. Remember that previous commands can be recalled and edited using the up and down arrow keys – they do not need to be typed out in full each time. CHOCOLATE is the least significant variable in the model so we will remove this variable from the model. Storing the output of the glm() function is useful as it allows us to use the update() function to add, remove, or modify variables without having to describe the model in full:   salex.lreg &lt;- update(salex.lreg, . ~ . - CHOCOLATE) summary(salex.lreg) ## ## Call: ## glm(formula = ILL ~ EGGS + MUSHROOM + PEPPER + PASTA + RICE + ## LETTUCE + COLESLAW, family = binomial(logit), data = salex) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.92561 -0.49859 0.07555 0.38723 2.07200 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.0223 0.6623 -3.053 0.00226 ** ## EGGS 3.5890 1.2188 2.945 0.00323 ** ## MUSHROOM -3.5992 1.6885 -2.132 0.03305 * ## PEPPER 2.3544 1.4275 1.649 0.09910 . ## PASTA 1.7770 1.1215 1.585 0.11308 ## RICE 0.1170 1.1388 0.103 0.91819 ## LETTUCE 3.4109 1.2316 2.770 0.00561 ** ## COLESLAW 0.7630 1.0224 0.746 0.45547 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 92.122 on 70 degrees of freedom ## Residual deviance: 41.273 on 63 degrees of freedom ## (6 observations deleted due to missingness) ## AIC: 57.273 ## ## Number of Fisher Scoring iterations: 7 RICE is now the least significant variable in the model so we will remove this variable from the model:   salex.lreg &lt;- update(salex.lreg, . ~ . - RICE) summary(salex.lreg) ## ## Call: ## glm(formula = ILL ~ EGGS + MUSHROOM + PEPPER + PASTA + LETTUCE + ## COLESLAW, family = binomial(logit), data = salex) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8877 -0.4999 0.0786 0.3897 2.0697 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.0169 0.6600 -3.056 0.00224 ** ## EGGS 3.6142 1.1944 3.026 0.00248 ** ## MUSHROOM -3.5508 1.6134 -2.201 0.02774 * ## PEPPER 2.3002 1.3200 1.743 0.08141 . ## PASTA 1.8230 1.0280 1.773 0.07617 . ## LETTUCE 3.4199 1.2273 2.787 0.00533 ** ## COLESLAW 0.7611 1.0203 0.746 0.45571 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 92.122 on 70 degrees of freedom ## Residual deviance: 41.283 on 64 degrees of freedom ## (6 observations deleted due to missingness) ## AIC: 55.283 ## ## Number of Fisher Scoring iterations: 6 COLESLAW is now the least significant variable in the model so we will remove this variable from the model:   salex.lreg &lt;- update(salex.lreg, . ~ . - COLESLAW) summary(salex.lreg) ## ## Call: ## glm(formula = ILL ~ EGGS + MUSHROOM + PEPPER + PASTA + LETTUCE, ## family = binomial(logit), data = salex) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.98481 -0.50486 0.08871 0.36910 2.06065 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.9957 0.6545 -3.049 0.00230 ** ## EGGS 3.8152 1.1640 3.278 0.00105 ** ## MUSHROOM -3.4008 1.5922 -2.136 0.03269 * ## PEPPER 2.3520 1.3269 1.773 0.07631 . ## PASTA 1.9706 0.9922 1.986 0.04701 * ## LETTUCE 3.4786 1.2246 2.841 0.00450 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 92.982 on 71 degrees of freedom ## Residual deviance: 41.895 on 66 degrees of freedom ## (5 observations deleted due to missingness) ## AIC: 53.895 ## ## Number of Fisher Scoring iterations: 6 PEPPER is now the least significant variable in the model so we will remove this variable from the model:   salex.lreg &lt;- update(salex.lreg, . ~ . - PEPPER) summary(salex.lreg) ## ## Call: ## glm(formula = ILL ~ EGGS + MUSHROOM + PASTA + LETTUCE, family = binomial(logit), ## data = salex) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0920 -0.5360 0.1109 0.4876 2.0056 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.8676 0.6128 -3.048 0.002306 ** ## EGGS 3.7094 1.0682 3.473 0.000515 *** ## MUSHROOM -1.6165 1.0829 -1.493 0.135524 ## PASTA 1.8440 0.9193 2.006 0.044864 * ## LETTUCE 3.2458 1.1698 2.775 0.005527 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 94.659 on 73 degrees of freedom ## Residual deviance: 45.578 on 69 degrees of freedom ## (3 observations deleted due to missingness) ## AIC: 55.578 ## ## Number of Fisher Scoring iterations: 6 MUSHROOM is now the least significant variable in the model so we will remove this variable from the model:   salex.lreg &lt;- update(salex.lreg, . ~ . - MUSHROOM) summary(salex.lreg) ## ## Call: ## glm(formula = ILL ~ EGGS + PASTA + LETTUCE, family = binomial(logit), ## data = salex) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2024 -0.5108 0.2038 0.4304 2.0501 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.9710 0.6146 -3.207 0.00134 ** ## EGGS 2.6391 0.7334 3.599 0.00032 *** ## PASTA 1.6646 0.8376 1.987 0.04689 * ## LETTUCE 3.1956 1.1516 2.775 0.00552 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 97.648 on 75 degrees of freedom ## Residual deviance: 50.529 on 72 degrees of freedom ## (1 observation deleted due to missingness) ## AIC: 58.529 ## ## Number of Fisher Scoring iterations: 6   There are now no non-significant variables in the model. Unfortunately R does not present information on the model coefficients in terms of odds ratios and confidence intervals but we can write a function to calculate them for us. The first step in doing this is to realise that the salex.lreg object contains essential information about the fitted model. To calculate odds ratios and confidence intervals we need the regression coefficients and their standard errors. Both:   summary(salex.lreg)$coefficients ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.970967 0.6145691 -3.207071 0.0013409398 ## EGGS 2.639115 0.7333899 3.598515 0.0003200388 ## PASTA 1.664581 0.8375970 1.987330 0.0468858898 ## LETTUCE 3.195594 1.1516159 2.774879 0.0055222320   and:   coef(summary(salex.lreg)) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.970967 0.6145691 -3.207071 0.0013409398 ## EGGS 2.639115 0.7333899 3.598515 0.0003200388 ## PASTA 1.664581 0.8375970 1.987330 0.0468858898 ## LETTUCE 3.195594 1.1516159 2.774879 0.0055222320   extract the data that we require. The preferred method is to use the coef() function. This is because some fitted models may return coefficients in a more complicated manner than (e.g.) those created by the glm() function. The coef() function provides a standard way of extracting this data from all classes of fitted objects. We can store the coefficients data in a separate object to make it easier to work with:   salex.lreg.coeffs &lt;- coef(summary(salex.lreg)) salex.lreg.coeffs ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.970967 0.6145691 -3.207071 0.0013409398 ## EGGS 2.639115 0.7333899 3.598515 0.0003200388 ## PASTA 1.664581 0.8375970 1.987330 0.0468858898 ## LETTUCE 3.195594 1.1516159 2.774879 0.0055222320   We can extract information from this object by addressing each piece of information by its row and column position in the object. For example:   salex.lreg.coeffs[2,1] ## [1] 2.639115   Is the regression coefficient for EGGS, and:   salex.lreg.coeffs[3,2] ## [1] 0.837597   is the standard error of the regression coefficient for PASTA. Similarly:   salex.lreg.coeffs[ ,1] ## (Intercept) EGGS PASTA LETTUCE ## -1.970967 2.639115 1.664581 3.195594 Returns the regression coefficients for all of the variables in the model, and:   salex.lreg.coeffs[ ,2] ## (Intercept) EGGS PASTA LETTUCE ## 0.6145691 0.7333899 0.8375970 1.1516159   Returns the standard errors of the regression coefficients. The table below shows the indices that address each cell in the table of regression coefficients:   matrix(salex.lreg.coeffs, nrow = 4, ncol = 4) ## [,1] [,2] [,3] [,4] ## [1,] -1.970967 0.6145691 -3.207071 0.0013409398 ## [2,] 2.639115 0.7333899 3.598515 0.0003200388 ## [3,] 1.664581 0.8375970 1.987330 0.0468858898 ## [4,] 3.195594 1.1516159 2.774879 0.0055222320   We can use this information to calculate odds ratio sand 95% confidence intervals:   or &lt;- exp(salex.lreg.coeffs[ ,1]) lci &lt;- exp(salex.lreg.coeffs[ ,1] - 1.96 * salex.lreg.coeffs[ ,2]) uci &lt;- exp(salex.lreg.coeffs[ ,1] + 1.96 * salex.lreg.coeffs[ ,2])   and make a single object that contains all of the required information:   lreg.or &lt;- cbind(or, lci, uci) lreg.or ## or lci uci ## (Intercept) 0.1393221 0.0417723 0.4646777 ## EGGS 14.0008053 3.3256684 58.9423019 ## PASTA 5.2834608 1.0231552 27.2832114 ## LETTUCE 24.4246856 2.5559581 233.4018193   We seldom need to report estimates and confidence intervals to more than two decimal places. We can use the round() function to remove the excess digits:   round(lreg.or, digits = 2) ## or lci uci ## (Intercept) 0.14 0.04 0.46 ## EGGS 14.00 3.33 58.94 ## PASTA 5.28 1.02 27.28 ## LETTUCE 24.42 2.56 233.40   We have now gone through all the necessary calculations step-by-step but it would be nice to have a function that did it all for us that we could use whenever we needed to. First we will create a template for the function:   lreg.or &lt;- function(model, digits = 2) {}   and then use the fix() function to edit the lreg.or() function:   fix(lreg.or)   We can now edit this function to add a calculation of odds ratios and 95% confidence intervals:   function(model, digits = 2) { lreg.coeffs &lt;- coef(summary(model)) OR &lt;- exp(lreg.coeffs[ ,1]) LCI &lt;- exp(lreg.coeffs[ ,1] - 1.96 * lreg.coeffs[ ,2]) UCI &lt;- exp(lreg.coeffs[ ,1] + 1.96 * lreg.coeffs[ ,2]) lreg.or &lt;- round(cbind(OR, LCI, UCI), digits = digits) lreg.or } Once you have made the changes shown above, check your work, save the file, and quit the editor. We can test our function:   lreg.or(salex.lreg)   Which produces the following output:   lreg.or(salex.lreg) ## OR LCI UCI ## (Intercept) 0.14 0.04 0.46 ## EGGS 14.00 3.33 58.94 ## PASTA 5.28 1.02 27.28 ## LETTUCE 24.42 2.56 233.40   The digits parameter of the lreg.or() function, which has digits = 2 as its default value, allows us to specify the precision with which the estimates and their confidence intervals are reported:   lreg.or(salex.lreg, digits = 4) ## OR LCI UCI ## (Intercept) 0.1393 0.0418 0.4647 ## EGGS 14.0008 3.3257 58.9423 ## PASTA 5.2835 1.0232 27.2832 ## LETTUCE 24.4247 2.5560 233.4018   Before we continue, it is probably a good idea to save this function for later use:   save(lreg.or, file = &quot;lregor.r&quot;) Which can be reloaded whenever it is needed:   load(&quot;lregor.r&quot;)   An alternative to using logistic regression with data that contains associations that may be due to confounding is to use stratified analysis (i.e. Mantel-Haenszel techniques). With several potential confounders, a stratified analysis results in the analysis of many tables which can be difficult to interpret. For example, four potential confounders, each with two levels would produce sixteen tables. In such situations, logistic regression might be a better approach. In order to illustrate Mantel-Haenszel techniques in R we will work with a simpler dataset. On Saturday, 21st April 1990, a luncheon was held in the home of Jean Bateman. There was a total of forty-five guests which included thirty-five members of the Department of Epidemiology and Population Sciences at the London School of Hygiene and Tropical Medicine. On Sunday morning, 22nd April 1990, Jean awoke with symptoms of gastrointestinal illness; her husband awoke with similar symptoms. The possibility of an outbreak related to the luncheon was strengthened when several of the guests telephoned Jean on Sunday and reported illness. On Monday, 23rd April 1990, there was an unusually large number of department members absent from work and reporting illness. Data from this outbreak is stored in the file bateman.dat. The variables in the file bateman.dat are:   ILL Ill? CHEESE Cheddar cheese CRABDIP Crab dip CRISPS Crisps BREAD French bread CHICKEN Chicken (roasted, served warm) RICE Rice (boiled, served warm) CAESAR Caesar salad TOMATO Tomato salad ICECREAM Vanilla ice-cream CAKE Chocolate cake JUICE Orange juice WINE White wine COFFEE Coffee Data is available for all forty-five guests at the luncheon. All of the variables are coded 1=yes, 2=no. Retrieve and attach the bateman dataset in R:   bateman &lt;- read.table(&quot;bateman.dat&quot;, header = TRUE) bateman attach(bateman) ## ILL CHEESE CRABDIP CRISPS BREAD CHICKEN RICE CAESAR TOMATO ICECREAM CAKE ## 1 1 1 1 1 2 1 1 1 1 1 1 ## 2 2 1 1 1 2 1 2 2 2 1 1 ## 3 1 2 2 1 2 1 2 1 2 1 1 ## 4 1 1 2 1 1 1 2 1 2 1 1 ## 5 1 1 1 1 2 1 1 1 1 2 1 ## 6 1 1 1 1 1 1 2 1 1 2 1 ## JUICE WINE COFFEE ## 1 1 1 1 ## 2 1 1 2 ## 3 2 1 2 ## 4 2 1 2 ## 5 1 1 1 ## 6 1 2 2 ## The following objects are masked from salex: ## ## CRISPS, ILL, RICE, TOMATO   We will use our tab2by2() function to analyse this data. Retrieve this function:   load(&quot;tab2by2.r&quot;) Use the tab2by2() function to analyse the data:   tab2by2(CHEESE, ILL) tab2by2(CRABDIP, ILL) tab2by2(CRISPS, ILL) tab2by2(BREAD, ILL) tab2by2(CHICKEN, ILL) tab2by2(RICE, ILL) tab2by2(CAESAR, ILL) tab2by2(TOMATO, ILL) tab2by2(ICECREAM, ILL) tab2by2(CAKE, ILL) tab2by2(JUICE, ILL) tab2by2(WINE, ILL) tab2by2(COFFEE, ILL)   tab2by2(CHEESE, ILL) ## ## outcome ## exposure 1 2 ## 1 15 7 ## 2 14 9 ## ## Relative Risk : 1.12013 ## 95% CI : 0.7253229 1.729838 ## ## Sample Odds Ratio : 1.377551 ## 95% CI : 0.4037553 4.699992 ## ## MLE Odds Ratio : 1.367743 ## 95% CI : 0.3427732 5.649399 tab2by2(CRABDIP, ILL) ## ## outcome ## exposure 1 2 ## 1 18 9 ## 2 11 7 ## ## Relative Risk : 1.090909 ## 95% CI : 0.6921784 1.719329 ## ## Sample Odds Ratio : 1.272727 ## 95% CI : 0.3682028 4.3993 ## ## MLE Odds Ratio : 1.265848 ## 95% CI : 0.3042941 5.188297 tab2by2(CRISPS, ILL) ## ## outcome ## exposure 1 2 ## 1 21 12 ## 2 8 4 ## ## Relative Risk : 0.9545455 ## 95% CI : 0.5930168 1.536478 ## ## Sample Odds Ratio : 0.875 ## 95% CI : 0.2170373 3.527619 ## ## MLE Odds Ratio : 0.8775841 ## 95% CI : 0.1587568 4.184763 tab2by2(BREAD, ILL) ## ## outcome ## exposure 1 2 ## 1 9 8 ## 2 20 8 ## ## Relative Risk : 0.7411765 ## 95% CI : 0.4469843 1.228997 ## ## Sample Odds Ratio : 0.45 ## 95% CI : 0.1280647 1.581232 ## ## MLE Odds Ratio : 0.4584416 ## 95% CI : 0.1072622 1.897017 tab2by2(CHICKEN, ILL) ## ## outcome ## exposure 1 2 ## 1 25 11 ## 2 4 5 ## ## Relative Risk : 1.5625 ## 95% CI : 0.7293337 3.347448 ## ## Sample Odds Ratio : 2.840909 ## 95% CI : 0.637796 12.65415 ## ## MLE Odds Ratio : 2.76979 ## 95% CI : 0.4912167 16.93409 tab2by2(RICE, ILL) ## ## outcome ## exposure 1 2 ## 1 22 10 ## 2 7 6 ## ## Relative Risk : 1.276786 ## 95% CI : 0.7330759 2.223756 ## ## Sample Odds Ratio : 1.885714 ## 95% CI : 0.5027038 7.073586 ## ## MLE Odds Ratio : 1.85813 ## 95% CI : 0.4026256 8.531602 tab2by2(CAESAR, ILL) ## ## outcome ## exposure 1 2 ## 1 26 5 ## 2 3 11 ## ## Relative Risk : 3.913978 ## 95% CI : 1.418617 10.7987 ## ## Sample Odds Ratio : 19.06667 ## 95% CI : 3.866585 94.02038 ## ## MLE Odds Ratio : 17.33517 ## 95% CI : 3.179027 133.7994 tab2by2(TOMATO, ILL) ## ## outcome ## exposure 1 2 ## 1 24 6 ## 2 5 10 ## ## Relative Risk : 2.4 ## 95% CI : 1.14769 5.018775 ## ## Sample Odds Ratio : 8 ## 95% CI : 1.97785 32.35836 ## ## MLE Odds Ratio : 7.553116 ## 95% CI : 1.642249 41.02567 tab2by2(ICECREAM, ILL) ## ## outcome ## exposure 1 2 ## 1 20 9 ## 2 9 7 ## ## Relative Risk : 1.226054 ## 95% CI : 0.7463643 2.01404 ## ## Sample Odds Ratio : 1.728395 ## 95% CI : 0.4889138 6.110177 ## ## MLE Odds Ratio : 1.7069 ## 95% CI : 0.4021245 7.255001 tab2by2(CAKE, ILL) ## ## outcome ## exposure 1 2 ## 1 22 11 ## 2 7 5 ## ## Relative Risk : 1.142857 ## 95% CI : 0.6689315 1.95255 ## ## Sample Odds Ratio : 1.428571 ## 95% CI : 0.3678242 5.548347 ## ## MLE Odds Ratio : 1.416945 ## 95% CI : 0.2847257 6.685098 tab2by2(JUICE, ILL) ## ## outcome ## exposure 1 2 ## 1 8 5 ## 2 21 11 ## ## Relative Risk : 0.9377289 ## 95% CI : 0.5701453 1.542301 ## ## Sample Odds Ratio : 0.8380952 ## 95% CI : 0.2206785 3.182927 ## ## MLE Odds Ratio : 0.8414367 ## 95% CI : 0.185464 4.101313 tab2by2(WINE, ILL) ## ## outcome ## exposure 1 2 ## 1 22 12 ## 2 7 4 ## ## Relative Risk : 1.016807 ## 95% CI : 0.6099343 1.695094 ## ## Sample Odds Ratio : 1.047619 ## 95% CI : 0.2543383 4.315141 ## ## MLE Odds Ratio : 1.046515 ## 95% CI : 0.1855742 5.186546 tab2by2(COFFEE, ILL) ## ## outcome ## exposure 1 2 ## 1 17 11 ## 2 12 5 ## ## Relative Risk : 0.860119 ## 95% CI : 0.5607997 1.319196 ## ## Sample Odds Ratio : 0.6439394 ## 95% CI : 0.1772875 2.338901 ## ## MLE Odds Ratio : 0.6502015 ## 95% CI : 0.1388979 2.729586 Two variables (CAESAR and TOMATO) are associated with ILL. These two variables are also associated with each other:   tab2by2(CAESAR, TOMATO) chisq.test(table(CAESAR, TOMATO)) fisher.test(table(CAESAR, TOMATO)) tab2by2(CAESAR, TOMATO) ## ## outcome ## exposure 1 2 ## 1 27 4 ## 2 3 11 ## ## Relative Risk : 4.064516 ## 95% CI : 1.477162 11.1838 ## ## Sample Odds Ratio : 24.75 ## 95% CI : 4.738936 129.2616 ## ## MLE Odds Ratio : 22.10962 ## 95% CI : 3.850174 183.4671   chisq.test(table(CAESAR, TOMATO)) ## Warning in chisq.test(table(CAESAR, TOMATO)): Chi-squared approximation may be ## incorrect ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: table(CAESAR, TOMATO) ## X-squared = 15.877, df = 1, p-value = 6.759e-05 fisher.test(table(CAESAR, TOMATO)) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(CAESAR, TOMATO) ## p-value = 3.442e-05 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 3.850174 183.467108 ## sample estimates: ## odds ratio ## 22.10962   This suggests the potential for one of these associations to be due to confounding. We can perform a simple stratified analysis using the table() function:   table(CAESAR, ILL, TOMATO) ## , , TOMATO = 1 ## ## ILL ## CAESAR 1 2 ## 1 23 4 ## 2 1 2 ## ## , , TOMATO = 2 ## ## ILL ## CAESAR 1 2 ## 1 3 1 ## 2 2 9 table(TOMATO, ILL, CAESAR) ## , , CAESAR = 1 ## ## ILL ## TOMATO 1 2 ## 1 23 4 ## 2 3 1 ## ## , , CAESAR = 2 ## ## ILL ## TOMATO 1 2 ## 1 1 2 ## 2 2 9   It would be useful to calculate odds ratios for each stratum. We can define a simple function to calculate an odds ratio from a two-by-two table:   or &lt;- function(x) {(x[1,1] / x[1,2]) / (x[2,1] / x[2,2])}   We can use apply() to apply the or() function to the two-by-two table in each stratum:   tabC &lt;- table(CAESAR, ILL, TOMATO) apply(tabC, 3, or) ## 1 2 ## 11.5 13.5 tabT &lt;- table(TOMATO, ILL, CAESAR) apply(tabT, 3, or) ## 1 2 ## 1.916667 2.250000   The 3 instructs the apply() function to apply the or() function to the third dimension of the table objects (i.e. levels of the potential confounder in tabC and tabT). The mantelhaen.test() function performs the stratified analysis:   mantelhaen.test(tabC) ## ## Mantel-Haenszel chi-squared test with continuity correction ## ## data: tabC ## Mantel-Haenszel X-squared = 5.752, df = 1, p-value = 0.01647 ## alternative hypothesis: true common odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.878994 83.156212 ## sample estimates: ## common odds ratio ## 12.5 mantelhaen.test(tabT) ## ## Mantel-Haenszel chi-squared test with continuity correction ## ## data: tabT ## Mantel-Haenszel X-squared = 0.049144, df = 1, p-value = 0.8246 ## alternative hypothesis: true common odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.3156862 13.4192331 ## sample estimates: ## common odds ratio ## 2.058219   It is likely that CAESAR salad was a vehicle of food-poisoning, and that TOMATO salad was not a vehicle of food-poisoning. Many of those at the luncheon ate both CAESAR salad and TOMATO salad. CAESAR confounded the relationship between TOMATO and ILL. This resulted in a spurious association between TOMATO and ILL. It only makes sense to calculate a common odds ratio in the absence of interaction. We can check for interaction ‘by eye’ by examining and comparing the odds ratios for each stratum as we did above. There does appear to be an interaction between CAESAR, WINE, and ILL:   tabW &lt;- table(CAESAR, ILL, WINE) apply(tabW, 3, or) ## 1 2 ## 63.0 2.5   Woolf’s test for interaction (also known as Woolf’s test for the homogeneity of odds ratios) provides a formal test for interaction. R does not provide a function to perform Woolf’s test for the homogeneity of odds ratios but it is possible to write a function to perform this test. First we will create a template for the function:   woolf.test &lt;- function(x) {}   And then use the fix() function to edit the woolf.test() function:   fix(woolf.test)   We can now edit this function to make it do something useful:   function(x) { x &lt;- x + 0.5 k &lt;- dim(x)[3] or &lt;- apply(x, 3, function(x) {(x[1, 1] / x[1, 2]) / (x[2, 1] / x[2, 2])}) w &lt;- apply(x, 3, function(x) {1 / sum(1 / x)}) chi.sq &lt;- sum(w * (log(or) - weighted.mean(log(or), w))^2) p &lt;- pchisq(chi.sq, df = k - 1, lower.tail = FALSE) cat(&quot;\\nWoolf&#39;s X2 :&quot;, chi.sq, &quot;\\np-value :&quot;, p, &quot;\\n&quot;) } Once you have made the changes shown above, check your work, save the file, and quit the editor. We can use the woolf.test() function to test for a three-way interaction between CAESAR, WINE, and ILL:   woolf.test(tabW)   Which returns:   ## ## Woolf&#39;s X2 : 3.319492 ## p-value : 0.06846297   Which is weak evidence of an interaction. We should test for interaction between CAESAR, TOMATO, and ILL before accepting the results reported by the mantelhaen.test() function:   woolf.test(tabC) ## ## Woolf&#39;s X2 : 0.0001233783 ## p-value : 0.9911376   We can repeat this analysis using logistic regression. We need to change the coding of the variables to 0 and 1 before specifying the model:   detach(bateman) bateman &lt;- 2 - bateman bateman bateman.lreg &lt;- glm(formula = ILL ~ CAESAR + TOMATO, family = binomial(logit), data = bateman) summary(bateman.lreg) bateman.lreg &lt;- update(bateman.lreg, . ~ . - TOMATO) summary(bateman.lreg) ## ## Call: ## glm(formula = ILL ~ CAESAR + TOMATO, family = binomial(logit), ## data = bateman) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.960 -0.641 0.563 0.563 1.835 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.4780 0.7101 -2.082 0.03739 * ## CAESAR 2.5202 0.9653 2.611 0.00904 ** ## TOMATO 0.7197 0.9552 0.753 0.45116 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 58.574 on 44 degrees of freedom ## Residual deviance: 41.408 on 42 degrees of freedom ## AIC: 47.408 ## ## Number of Fisher Scoring iterations: 4 ## ## Call: ## glm(formula = ILL ~ CAESAR, family = binomial(logit), data = bateman) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9103 -0.6945 0.5931 0.5931 1.7552 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.2993 0.6513 -1.995 0.046066 * ## CAESAR 2.9479 0.8141 3.621 0.000293 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 58.574 on 44 degrees of freedom ## Residual deviance: 41.940 on 43 degrees of freedom ## AIC: 45.94 ## ## Number of Fisher Scoring iterations: 4 Interactions are specified using the multiply (*) symbol in the model formula:   bateman.lreg &lt;- glm(formula = ILL ~ CAESAR + WINE + CAESAR * WINE, family = binomial(logit), data = bateman) summary(bateman.lreg) ## ## Call: ## glm(formula = ILL ~ CAESAR + WINE + CAESAR * WINE, family = binomial(logit), ## data = bateman) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0393 -0.4590 0.5168 0.5168 2.1460 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.092e-15 1.000e+00 0.000 1.0000 ## CAESAR 9.163e-01 1.304e+00 0.703 0.4822 ## WINE -2.197e+00 1.453e+00 -1.512 0.1305 ## CAESAR:WINE 3.227e+00 1.787e+00 1.806 0.0709 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 58.574 on 44 degrees of freedom ## Residual deviance: 38.508 on 41 degrees of freedom ## AIC: 46.508 ## ## Number of Fisher Scoring iterations: 4   Before we continue, it is probably a good idea to save the woolf.test() function for later use:   save(woolf.test, file = &quot;woolf.r&quot;)   3.1 Matched data Matching is another way to control for the effects of potential confounding variables. Matching is usually performed during data-collection as part of the design of a study. In a matched case-control studies, each case is matched with one or more controls which are chosen to have the same values over a set of potential confounding variables. In order to illustrate how matched data may be analysed using tabulation and stratification in R we will start with the simple case of one-to-one matching (i.e. each case has a single matched control):   octe &lt;- read.table(&quot;octe.dat&quot;, header = TRUE) octe[1:10, ] ## ID CASE OC ## 1 1 1 1 ## 2 1 2 1 ## 3 2 1 1 ## 4 2 2 1 ## 5 3 1 1 ## 6 3 2 1 ## 7 4 1 1 ## 8 4 2 1 ## 9 5 1 1 ## 10 5 2 1   This data is from a matched case-control study investigating the association between oral contraceptive use and thromboembolism. The cases are 175 women aged between 15 and 44 years admitted to hospital for thromboembolism and discharged alive. The controls are female patients admitted for conditions believed to be unrelated to oral contraceptive use. Cases and controls were matched on age, ethnic group, marital status, parity, income, place of residence, and date of hospitalisation. The variables in the dataset are: ID Identifier for the matched sets of cases and controls CASE Case (1) or control (2) OC Used oral contraceptives in the previous month (1=yes, 2=no)   The dataset consists of 350 records:   nrow(octe) ## [1] 350   There are 175 matched sets of cases and controls:   length(unique(octe$ID)) ## [1] 175   In each matched set of cases and controls there is one case and one control:   table(octe$ID, octe$CASE) ## ## 1 2 ## 1 1 1 ## 2 1 1 ## 3 1 1 ## 4 1 1 ## 5 1 1 ## 6 1 1 ## 7 1 1 ## 8 1 1 ## 9 1 1 ## 10 1 1 This data may be analysed using McNemar’s chi-squared test which use the number of discordant (i.e. relative to exposure) pairs of matched cases and controls. To find the number of discordant pairs we need to split the dataset into cases and controls:   octe.cases &lt;- subset(octe, CASE == 1) octe.controls &lt;- subset(octe, CASE == 2)   Sorting these two datasets (i.e. octe.cases and octe.controls) by the ID variable simplifies the analysis:   octe.cases &lt;- octe.cases[order(octe.cases$ID), ] octe.controls &lt;- octe.controls[order(octe.controls$ID), ]   Since the two datasets (i.e. octe.cases and octe.controls) are now sorted by the ID variable we can use the table() function to retrieve the number if concordant and discordant pairs and store them in a table object:   tab &lt;- table(octe.cases$OC, octe.controls$OC) tab ## ## 1 2 ## 1 10 57 ## 2 13 95   This table object (i.e. tab) can then be passed to the mcnemar.test() function:   mcnemar.test(tab) ## ## McNemar&#39;s Chi-squared test with continuity correction ## ## data: tab ## McNemar&#39;s chi-squared = 26.414, df = 1, p-value = 2.755e-07   The mcnemar.test() function does not provide an estimate of the odds ratio. This is the ratio of the discordant pairs:   r &lt;- tab[1,2] s &lt;- tab[2,1] rdp &lt;- r / s rdp ## [1] 4.384615   A confidence interval can also be calculated:   ci.p &lt;- binom.test(r, r + s)$conf.int ci.rdp &lt;- ci.p / (1 - ci.p) ci.rdp ## [1] 2.371377 8.731311 ## attr(,&quot;conf.level&quot;) ## [1] 0.95   This provides a 95% confidence interval. Other (e.g. 99%) confidence intervals can be produced by specifying appropriate values for the conf.level parameter of the binom.test() function:   ci.p &lt;- binom.test(r, r + s, conf.level = 0.99)$conf.int ci.rdp &lt;- ci.p / (1 - ci.p) ci.rdp ## [1] 2.010478 10.949095 ## attr(,&quot;conf.level&quot;) ## [1] 0.99 An alternative way of analysing this data is to use the mantelhaen.test() function:   tab &lt;- table(octe$OC, octe$CASE, octe$ID) mantelhaen.test(tab) ## ## Mantel-Haenszel chi-squared test with continuity correction ## ## data: tab ## Mantel-Haenszel X-squared = 26.414, df = 1, p-value = 2.755e-07 ## alternative hypothesis: true common odds ratio is not equal to 1 ## 95 percent confidence interval: ## 2.400550 8.008521 ## sample estimates: ## common odds ratio ## 4.384615   The Mantel-Haenszel approach is preferred because it can be used with data from matched case-control studies that match more than one control to each case. Multiple matching is useful when the condition being studied is rare or at the early stages of an outbreak (i.e. when cases are hard to find and controls are easy to find). We will now work with some data where each case has one or more controls:   tsstamp &lt;- read.table(&quot;tsstamp.dat&quot;, header = TRUE) tsstamp ## ID CASE RBTAMP ## 1 1 1 1 ## 2 1 2 1 ## 3 1 2 1 ## 4 2 1 1 ## 5 2 2 2 ## 6 2 2 1 ## 7 2 2 1 ## 8 3 1 2 ## 9 3 2 2 ## 10 3 2 1 ## 11 4 1 1 ## 12 4 2 2 ## 13 5 1 1 ## 14 5 2 1 ## 15 5 2 2 ## 16 6 1 1 ## 17 6 2 1 ## 18 7 1 1 ## 19 7 2 2 ## 20 7 2 2 ## 21 8 1 1 ## 22 8 2 1 ## 23 8 2 2 ## 24 9 1 2 ## 25 9 2 1 ## 26 9 2 2 ## 27 9 2 2 ## 28 10 1 1 ## 29 10 2 2 ## 30 10 2 2 ## 31 11 1 1 ## 32 11 2 2 ## 33 11 2 2 ## 34 12 1 1 ## 35 12 2 2 ## 36 12 2 2 ## 37 12 2 2 ## 38 13 1 1 ## 39 13 2 2 ## 40 13 2 2 ## 41 14 1 2 ## 42 14 2 2 ## 43 14 2 2   This data is from a matched case-control study investigating the association between the use of different brands of tampon and toxic shock syndrome undertaken during an outbreak. Only a subset of the original dataset is used here. The variables in the dataset are: ID Identifier for the matched sets of cases and controls CASE Case (1) or control (2) RBTAMP Used Rely brand tampons (1=yes, 2=no)   The dataset consists of forty-three (43) records:   nrow(tsstamp) ## [1] 43   There are fourteen (14) matched sets of cases and controls:   length(unique(tsstamp$ID)) ## [1] 14   Each matched set of cases and controls consists of one case and one or more controls:   table(tsstamp$ID, tsstamp$CASE) ## ## 1 2 ## 1 1 2 ## 2 1 3 ## 3 1 2 ## 4 1 1 ## 5 1 2 ## 6 1 1 ## 7 1 2 ## 8 1 2 ## 9 1 3 ## 10 1 2 ## 11 1 2 ## 12 1 3 ## 13 1 2 ## 14 1 2   The McNemar’s chi-squared test is not useful for this data as it is limited to the special case of one-to-one matching. Analysing this data using a simple tabulation such as:   fisher.test(table(tsstamp$RBTAMP, tsstamp$CASE)) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(tsstamp$RBTAMP, tsstamp$CASE) ## p-value = 0.007805 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.542686 53.734756 ## sample estimates: ## odds ratio ## 7.709932   ignores the matched nature of the data and is, therefore, also not useful for this data. The matched nature of the data may be accounted by stratifying on the variable that identifies the matched sets of cases and controls (i.e. the ID variable) using the mantelhaen.test() function:   mantelhaen.test(table(tsstamp$RBTAMP, tsstamp$CASE, tsstamp$ID)) ## ## Mantel-Haenszel chi-squared test with continuity correction ## ## data: table(tsstamp$RBTAMP, tsstamp$CASE, tsstamp$ID) ## Mantel-Haenszel X-squared = 5.9384, df = 1, p-value = 0.01481 ## alternative hypothesis: true common odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.589505 43.191463 ## sample estimates: ## common odds ratio ## 8.285714   Analysis of several risk factors or adjustment for confounding variables not matched for in the design of a matched case-control study cannot be performed using tabulation-based procedures such as the McNemar's chi- squared test and Mantel-Haenszel procedures. In these situations a special form of logistic regression, called conditional logistic regression, should be used. We can now quit R:   q()   For this exercise there is no need to save the workspace image so click the No or Don’t Save button (GUI) or enter n when prompted to save the workspace image (terminal). 3.2 Summary R provides functions for many kinds of complex statistical analysis. We have looked at using the generalised linear model glm() function to perform logistic regression. We have looked ar the mantelhaen.test() function to perform stratified analyses and the mantelhaen.test() and mcnemar.test() functions to analyse data from matched case-control studies. R can be extended by writing new functions. New functions can perform simple or complex data analysis. New functions can be composed of parts of existing function. New functions can be saved and used in subsequent R sessions. By building your own functions you can use R to build your own statistical analysis system. "],["exercise4.html", "Exercise 4 Analysing some data with R 4.1 Summary", " Exercise 4 Analysing some data with R In this exercise we will use the R functions we have already used and the functions we have added to R to analyse a small dataset. First we will start R and retrieve our functions:   load(&quot;tab2by2.r&quot;) load(&quot;lregor.r&quot;)   And then retrieve and attach the sample dataset:   gudhiv &lt;- read.table(&quot;gudhiv.dat&quot;, header = TRUE, na.strings = &quot;X&quot;) attach(gudhiv) This data is from a cross-sectional study of 435 male patients who presented with sexually transmitted infections at an outpatient clinic in The Gambia between August 1988 and June 1990. Several studies have documented an association between genital ulcer disease (GUD) and HIV infection. A study of Gambian prostitutes documented an association between seropositivity for HIV-2 and antibodies against Treponema pallidum (a serological test for syphilis). Prostitutes are not the ideal population for such studies as they may have experienced multiple sexually transmitted infections and it is difficult to quantify the number of times they may have had sex with HIV-2 seropositive customers. A sample of males with sexually transmitted infections is easier to study as they have probably had fewer sexual partners than prostitutes and much less contact with sexually transmitted infection pathogens. In such a sample it is also easier to find subjects and collect data. The variables in the dataset are:   MARRIED Married (1=yes, 0=no) GAMBIAN Gambian Citizen (1=yes, 0=no) GUD History of GUD or syphilis (1=yes, 0=no) UTIGC History of urethral discharge (1=yes, 0=no) CIR Circumcised (1=yes, 0=no) TRAVOUT Travelled outside of Gambia and Senegal (1=yes, 0=no) SEXPRO Ever had sex with a prostitute (1=yes, 0=no) INJ12M Injection in previous 12 months (1=yes, 0=no) PARTNERS Sexual partners in previous 12 months (number) HIV HIV-2 positive serology (1=yes, 0=no)   Data is available for all 435 patients enrolled in the study. We will start our analysis by examining pairwise associations between the binary exposure variables and the HIV variable using the tab2by2() function that we wrote earlier:   tab2by2(MARRIED, HIV) tab2by2(GAMBIAN, HIV) tab2by2(GUD, HIV) tab2by2(UTIGC, HIV) tab2by2(CIR, HIV) tab2by2(TRAVOUT, HIV) tab2by2(SEXPRO, HIV) tab2by2(INJ12M, HIV) tab2by2(MARRIED, HIV) ## ## outcome ## exposure 0 1 ## 0 321 13 ## 1 93 8 ## ## Relative Risk : 1.043751 ## 95% CI : 0.9818512 1.109554 ## ## Sample Odds Ratio : 2.124069 ## 95% CI : 0.8545749 5.279433 ## ## MLE Odds Ratio : 2.119801 ## 95% CI : 0.7380371 5.714354 tab2by2(GAMBIAN, HIV) ## ## outcome ## exposure 0 1 ## 0 73 4 ## 1 341 17 ## ## Relative Risk : 0.9953155 ## 95% CI : 0.9400068 1.053879 ## ## Sample Odds Ratio : 0.909824 ## 95% CI : 0.2974059 2.783333 ## ## MLE Odds Ratio : 0.9100104 ## 95% CI : 0.2853202 3.826485 tab2by2(GUD, HIV) ## ## outcome ## exposure 0 1 ## 0 339 12 ## 1 72 9 ## ## Relative Risk : 1.086538 ## 95% CI : 1.003531 1.176412 ## ## Sample Odds Ratio : 3.53125 ## 95% CI : 1.434372 8.693509 ## ## MLE Odds Ratio : 3.517408 ## 95% CI : 1.258556 9.491924 tab2by2(UTIGC, HIV) ## ## outcome ## exposure 0 1 ## 0 261 12 ## 1 151 9 ## ## Relative Risk : 1.013027 ## 95% CI : 0.9678841 1.060275 ## ## Sample Odds Ratio : 1.296358 ## 95% CI : 0.5338453 3.147997 ## ## MLE Odds Ratio : 1.295532 ## 95% CI : 0.4703496 3.438842 tab2by2(CIR, HIV) ## ## outcome ## exposure 0 1 ## 0 10 3 ## 1 392 17 ## ## Relative Risk : 0.8025903 ## 95% CI : 0.5955085 1.081682 ## ## Sample Odds Ratio : 0.1445578 ## 95% CI : 0.0364195 0.5737851 ## ## MLE Odds Ratio : 0.1460183 ## 95% CI : 0.03322189 0.899754 tab2by2(TRAVOUT, HIV) ## ## outcome ## exposure 0 1 ## 0 152 2 ## 1 256 19 ## ## Relative Risk : 1.060268 ## 95% CI : 1.02181 1.100173 ## ## Sample Odds Ratio : 5.640625 ## 95% CI : 1.295879 24.55218 ## ## MLE Odds Ratio : 5.624226 ## 95% CI : 1.32716 50.45859 tab2by2(SEXPRO, HIV) ## ## outcome ## exposure 0 1 ## 0 268 13 ## 1 143 8 ## ## Relative Risk : 1.007093 ## 95% CI : 0.9621259 1.054161 ## ## Sample Odds Ratio : 1.153308 ## 95% CI : 0.4671083 2.847562 ## ## MLE Odds Ratio : 1.152912 ## 95% CI : 0.4042323 3.083152 tab2by2(INJ12M, HIV) ## ## outcome ## exposure 0 1 ## 0 146 7 ## 1 268 14 ## ## Relative Risk : 1.004097 ## 95% CI : 0.9610996 1.049018 ## ## Sample Odds Ratio : 1.089552 ## 95% CI : 0.4301305 2.759916 ## ## MLE Odds Ratio : 1.089351 ## 95% CI : 0.4006202 3.263814   Note that our tab2by2() function returns misleading risk ratio estimates and confidence intervals for this dataset. This is because the function expects the exposure and outcome variables to be ordered with exposure-present and outcome-present as the first category (e.g. 1 = present, 2 = absent). This coding is reversed (i.e. 0 = absent, 1 = present) in the gudhiv dataset. We can produce risk ratio estimates for variables in the gudhiv data using the tab2by2() function and a simple transformation of the exposure and outcome variables. For example:   tab2by2(2 - GUD, 2 - HIV) ## ## outcome ## exposure 1 2 ## 1 9 72 ## 2 12 339 ## ## Relative Risk : 3.25 ## 95% CI : 1.417411 7.451965 ## ## Sample Odds Ratio : 3.53125 ## 95% CI : 1.434372 8.693509 ## ## MLE Odds Ratio : 3.517408 ## 95% CI : 1.258556 9.491924   The odds ratio estimates returned by the tab2by2() function, with or without this transformation, are correct. The GUD and TRAVOUT variables are associated with HIV. PARTNERS is a continuous variable and we should examine its distribution before doing anything with it:   table(PARTNERS) hist(PARTNERS) table(PARTNERS) ## PARTNERS ## 1 2 3 4 5 6 7 8 9 ## 61 129 133 71 25 6 6 2 2   The distribution of PARTNERS is severely non-normal. Instead of attempting to transform the variable we will produce summary statistics for each level of the HIV variable and perform a non-parametric test:   by(PARTNERS, HIV, summary) kruskal.test(PARTNERS ~ HIV) ## HIV: 0 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 2.00 3.00 2.72 3.00 8.00 ## ------------------------------------------------------------ ## HIV: 1 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 4.000 5.000 5.381 7.000 9.000 ## ## Kruskal-Wallis rank sum test ## ## data: PARTNERS by HIV ## Kruskal-Wallis chi-squared = 32.036, df = 1, p-value = 1.514e-08 An alternative way of looking at the data is as a tabulation:   table(PARTNERS, HIV) ## HIV ## PARTNERS 0 1 ## 1 60 1 ## 2 128 1 ## 3 131 2 ## 4 68 3 ## 5 21 4 ## 6 3 3 ## 7 2 4 ## 8 1 1 ## 9 0 2   You can use the plot() function to represent this table graphically:   plot(table(PARTNERS, HIV), color = c(&quot;lightgreen&quot;, &quot;red&quot;)) There appears to be an association between the number of sexual PARTNERS in the previous twelve months and positive HIV serology. The proportion with positive HIV serology increases as the number of sexual partners increases:   prop.table(table(PARTNERS, HIV), 1) * 100 ## HIV ## PARTNERS 0 1 ## 1 98.3606557 1.6393443 ## 2 99.2248062 0.7751938 ## 3 98.4962406 1.5037594 ## 4 95.7746479 4.2253521 ## 5 84.0000000 16.0000000 ## 6 50.0000000 50.0000000 ## 7 33.3333333 66.6666667 ## 8 50.0000000 50.0000000 ## 9 0.0000000 100.0000000   The ‘1’ instructs the prop.table() function to calculate row proportions. You can also use the plot() function to represent this table graphically:   plot(prop.table(table(PARTNERS, HIV), 1) * 100, color = c(&quot;lightgreen&quot;, &quot;red&quot;)) The chi-square test for trend is an appropriate test to perform on this data. The prop.trend.test() function that performs the chi-square test for trend requires you to specify the number of events and the number of trials. In this table:   table(PARTNERS, HIV) ## HIV ## PARTNERS 0 1 ## 1 60 1 ## 2 128 1 ## 3 131 2 ## 4 68 3 ## 5 21 4 ## 6 3 3 ## 7 2 4 ## 8 1 1 ## 9 0 2   The number of events in each row is in the second column (labelled 1) and the number of trials is the total number of cases in each row of the table. We can extract this data from a table object:   tab &lt;- table(PARTNERS, HIV) events &lt;- tab[ ,2] trials &lt;- tab[ ,1] + tab[ ,2] tab &lt;- table(PARTNERS, HIV) events &lt;- tab[ ,2] trials &lt;- tab[ ,1] + tab[ ,2]   Another way of creating the trials object would be to use the apply() function to sum the rows of the tab object:   trials &lt;- apply(tab, 1, sum) Pass this data to the prop.trend.test() function:   prop.trend.test(events, trials) ## ## Chi-squared Test for Trend in Proportions ## ## data: events out of trials , ## using scores: 1 2 3 4 5 6 7 8 9 ## X-squared = 76.389, df = 1, p-value &lt; 2.2e-16   With a linear trend such as this we can use PARTNERS in a logistic model without recoding or creating indicator variables. We can now specify and fit the logistic regression model:   gudhiv.lreg &lt;- glm(formula = HIV ~ GUD + TRAVOUT + PARTNERS, family = binomial(logit)) summary(gudhiv.lreg) ## ## Call: ## glm(formula = HIV ~ GUD + TRAVOUT + PARTNERS, family = binomial(logit)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.70415 -0.19849 -0.11148 -0.06247 3.11742 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -9.4854 1.4663 -6.469 9.86e-11 *** ## GUD 1.3869 0.5937 2.336 0.0195 * ## TRAVOUT 2.0867 0.9547 2.186 0.0288 * ## PARTNERS 1.1605 0.2050 5.662 1.50e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 167.364 on 425 degrees of freedom ## Residual deviance: 99.377 on 422 degrees of freedom ## (9 observations deleted due to missingness) ## AIC: 107.38 ## ## Number of Fisher Scoring iterations: 8   We can use the lreg.or() function that we wrote earlier to calculate and display odds ratios and confidence intervals:   lreg.or(gudhiv.lreg) ## OR LCI UCI ## (Intercept) 0.00 0.00 0.00 ## GUD 4.00 1.25 12.81 ## TRAVOUT 8.06 1.24 52.35 ## PARTNERS 3.19 2.14 4.77   PARTNERS is incorporated into the logistic model as a continuous variable. The odds ratio reported for PARTNERS is the odds ratio associated with a unit increase in the number of sexual PARTNERS. A man reporting five sexual partners, for example, was over three times as likely (odds ratio = 3.19) to have a positive HIV-2 serology than a man reporting four sexual partners. An alternative approach would be to have created an indicator variables:   part.gt.5 &lt;- ifelse(PARTNERS &gt; 5, 1, 0)   This creates a new variable (part.gt.5) that indicates whether or not an individual subject reported having more than five sexual partners in the previous twelve months:   table(PARTNERS, part.gt.5) ## part.gt.5 ## PARTNERS 0 1 ## 1 61 0 ## 2 129 0 ## 3 133 0 ## 4 71 0 ## 5 25 0 ## 6 0 6 ## 7 0 6 ## 8 0 2 ## 9 0 2   You can also inspect this on a case-by-case basis:   cbind(PARTNERS, part.gt.5) ## PARTNERS part.gt.5 ## [1,] 2 0 ## [2,] 2 0 ## [3,] 1 0 ## [4,] 2 0 ## [5,] 3 0 ## [6,] 2 0   We can now specify and fit the logistic regression model using our indicator variable:   gudhiv.lreg &lt;- glm(formula = HIV ~ GUD + TRAVOUT + part.gt.5, family = binomial(logit)) summary(gudhiv.lreg) lreg.or(gudhiv.lreg) ## ## Call: ## glm(formula = HIV ~ GUD + TRAVOUT + part.gt.5, family = binomial(logit)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6092 -0.2205 -0.2205 -0.0719 3.4521 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.9559 0.9850 -6.046 1.48e-09 *** ## GUD 1.4930 0.5805 2.572 0.0101 * ## TRAVOUT 2.2514 0.9319 2.416 0.0157 * ## part.gt.5 4.6791 0.7560 6.189 6.05e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 167.36 on 425 degrees of freedom ## Residual deviance: 106.43 on 422 degrees of freedom ## (9 observations deleted due to missingness) ## AIC: 114.43 ## ## Number of Fisher Scoring iterations: 7 ## OR LCI UCI ## (Intercept) 0.00 0.00 0.02 ## GUD 4.45 1.43 13.89 ## TRAVOUT 9.50 1.53 59.02 ## part.gt.5 107.67 24.47 473.84   We can now quit R:   q()   For this exercise there is no need to save the workspace image so click the No or Don’t Save button (GUI) or enter n when prompted to save the workspace image (terminal). 4.1 Summary Using built-in functions and our own functions we can use R to analyse epidemiological data. The power of R is that it can be easily extended. Many user-contributed functions (usually packages of related functions) are available for download over the Internet. We will use one of these packages in the next exercise. "],["exercise5.html", "Exercise 5 Extending R with packages 5.1 Summary", " Exercise 5 Extending R with packages R has no built-in functions for survival analysis but, because it is an extensible system, survival analysis is available as an add-in package. You can find a list of add-in packages at the R website. http://www.r-project.org/ Add-in packages are installed from the Internet. There are a series of R functions that enable you to download and install add-in packages. The survival package adds functions to R that enable it to analyse survival data. This package may be downloaded and installed using install.packages(\"survival\") or from the Packages or Packages &amp; Data menu if you are using a GUI version of R. Packages are loaded into R as they are needed using the library() function. Start R and load the survival package:   library(survival)   Before we go any further we should retrieve a dataset:   ca &lt;- read.table(&quot;ca.dat&quot;, header = TRUE) attach(ca)   The columns in this dataset on the survival of cancer patients in two different treatment groups are as follows:   time Survival or censoring time (months) status Censoring status (1=dead, 0=censored) group Treatment group (1 / 2)   We next need to create a survival object from the time and status variables using the Surv() function:   response &lt;- Surv(time, status)   We can then specify the model for the survival analysis. In this case we state that survival (response) is dependent upon the treatment group:   ca.surv &lt;- survfit(response ~ group)   The summary() function applied to a survfit object lists the survival probabilities at each time point with 95% confidence intervals:   summary(ca.surv) ## Call: survfit(formula = response ~ group) ## ## group=1 ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 8 22 1 0.955 0.0444 0.8714 1.000 ## 9 21 1 0.909 0.0613 0.7966 1.000 ## 13 19 1 0.861 0.0744 0.7270 1.000 ## 14 17 1 0.811 0.0856 0.6591 0.997 ## 18 16 1 0.760 0.0940 0.5963 0.968 ## 19 15 1 0.709 0.1005 0.5373 0.936 ## 21 14 1 0.659 0.1053 0.4814 0.901 ## 23 13 1 0.608 0.1087 0.4282 0.863 ## 30 10 1 0.547 0.1136 0.3643 0.822 ## 31 9 1 0.486 0.1161 0.3046 0.776 ## 32 8 1 0.426 0.1164 0.2489 0.727 ## 34 7 1 0.365 0.1146 0.1971 0.675 ## 48 5 1 0.292 0.1125 0.1371 0.621 ## 56 3 1 0.195 0.1092 0.0647 0.585 ## ## group=2 ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 4 24 1 0.9583 0.0408 0.88163 1.000 ## 5 23 2 0.8750 0.0675 0.75221 1.000 ## 6 21 1 0.8333 0.0761 0.69681 0.997 ## 7 20 1 0.7917 0.0829 0.64478 0.972 ## 8 19 2 0.7083 0.0928 0.54795 0.916 ## 9 17 1 0.6667 0.0962 0.50240 0.885 ## 11 16 1 0.6250 0.0988 0.45845 0.852 ## 12 15 1 0.5833 0.1006 0.41598 0.818 ## 21 12 1 0.5347 0.1033 0.36614 0.781 ## 23 11 1 0.4861 0.1047 0.31866 0.742 ## 27 10 1 0.4375 0.1049 0.27340 0.700 ## 28 9 1 0.3889 0.1039 0.23032 0.657 ## 30 8 1 0.3403 0.1017 0.18945 0.611 ## 32 7 1 0.2917 0.0981 0.15088 0.564 ## 33 6 1 0.2431 0.0930 0.11481 0.515 ## 37 5 1 0.1944 0.0862 0.08157 0.464 ## 41 4 2 0.0972 0.0650 0.02624 0.360 ## 43 2 1 0.0486 0.0473 0.00722 0.327 ## 45 1 1 0.0000 NaN NA NA   Printing the ca.surv object provides another view of the results:   ca.surv ## Call: survfit(formula = response ~ group) ## ## n events median 0.95LCL 0.95UCL ## group=1 22 14 31 21 NA ## group=2 24 22 23 11 37   The plot() function with a survfit object displays the survival curves:   plot(ca.surv, xlab = &quot;Months&quot;, ylab = &quot;Survival&quot;)   We can make it easier to distinguish between the two lines by specifying a width for each line using thelwd parameter of the plot() function:   plot(ca.surv, xlab = &quot;Months&quot;, ylab = &quot;Survival&quot;, lwd = c(1, 2))   It would also be useful to add a legend:   legend(125, 1, names(ca.surv$strata), lwd = c(1, 2))   If there is only one survival curve to plot then plotting a survfit object will plot the survival curve with 95% confidence limits. You can specify that confidence limits should be plotted when there is more than one survival curve but the results can be disappointing:   plot(ca.surv, conf.int = TRUE)   Plots can be improved by specifying different colours for each curve:   plot(ca.surv, conf.int = TRUE, col = c(&quot;red&quot;, &quot;darkgreen&quot;))   We can perform a formal test of the two survival times using the survdiff() function:   survdiff(response ~ group) ## Call: ## survdiff(formula = response ~ group) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## group=1 22 14 21.1 2.38 6.26 ## group=2 24 22 14.9 3.36 6.26 ## ## Chisq= 6.3 on 1 degrees of freedom, p= 0.01   We can now quit R:   q()   For this exercise there is no need to save the workspace image so click the No or Don’t Save button (GUI) or enter n when prompted to save the workspace image (terminal). 5.1 Summary R can be extended by adding additional packages. Some packages are included with the standard R installation but many others are available and may be downloaded from the Internet. You can find a list of add-in packages at the R website: http://www.r-project.org/ Packages may also be downloaded and installed from this site using the install.packages() function or from the Packages or Packages &amp; Data menu if you are using a GUI version of R. Packages are loaded into R as they are needed using the library() function. You can use the search() function to display a list of loaded packages and attached data.frames. "],["exercise6.html", "Exercise 6 Making your own objects behave like R objects 6.1 Summary", " Exercise 6 Making your own objects behave like R objects In the previous exercises we concentrated on writing functions that take some input data, analyse it, and display the results of the analysis. The standard R functions we have used all do this. The fisher.test() function, for example, takes a table object (or the names of two variables) as input and calculates and displays the p- value for Fisher’s exact test and the odds ratio and associated confidence interval for two-by-two tables:   fem &lt;- read.table(&quot;fem.dat&quot;, header = TRUE) attach(fem) ## The following objects are masked from fem (pos = 7): ## ## AGE, ANX, DEP, ID, IQ, LIFE, SEX, SLP, WT fisher.test(SEX, LIFE) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: SEX and LIFE ## p-value = 0.03175 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.080298 14.214482 ## sample estimates: ## odds ratio ## 3.620646   The results of the fisher.test() function may also be saved for later use:   ft &lt;- fisher.test(SEX, LIFE) ft ## ## Fisher&#39;s Exact Test for Count Data ## ## data: SEX and LIFE ## p-value = 0.03175 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.080298 14.214482 ## sample estimates: ## odds ratio ## 3.620646   The fisher.test() function returns an object of the class htest:   class(ft) ## [1] &quot;htest&quot;   which is a list containing the output of the fisher.test() function. Each item of output is stored as a different named item in the list:   names(ft) str(ft) ## [1] &quot;p.value&quot; &quot;conf.int&quot; &quot;estimate&quot; &quot;null.value&quot; &quot;alternative&quot; ## [6] &quot;method&quot; &quot;data.name&quot; ## List of 7 ## $ p.value : num 0.0318 ## $ conf.int : num [1:2] 1.08 14.21 ## ..- attr(*, &quot;conf.level&quot;)= num 0.95 ## $ estimate : Named num 3.62 ## ..- attr(*, &quot;names&quot;)= chr &quot;odds ratio&quot; ## $ null.value : Named num 1 ## ..- attr(*, &quot;names&quot;)= chr &quot;odds ratio&quot; ## $ alternative: chr &quot;two.sided&quot; ## $ method : chr &quot;Fisher&#39;s Exact Test for Count Data&quot; ## $ data.name : chr &quot;SEX and LIFE&quot; ## - attr(*, &quot;class&quot;)= chr &quot;htest&quot;   Each of these items can be referred to by name:   ft$estimate ft$conf.int ## odds ratio ## 3.620646 ## [1] 1.080298 14.214482 ## attr(,&quot;conf.level&quot;) ## [1] 0.95   When you display the output of the fisher.test() function either by calling the function directly:   fisher.test(SEX, LIFE) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: SEX and LIFE ## p-value = 0.03175 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.080298 14.214482 ## sample estimates: ## odds ratio ## 3.620646   or by typing the name of an object created using the fisher.test() function:   ft ## ## Fisher&#39;s Exact Test for Count Data ## ## data: SEX and LIFE ## p-value = 0.03175 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.080298 14.214482 ## sample estimates: ## odds ratio ## 3.620646   The print() function takes over and formatted output is produced. The print() function knows about htest class objects and produces output of the correct format for that class of object. This means that any function that produces an htest object (or any other standard R object) does not need to include R commands to produce formatted output. All hypothesis testing functions supplied with R produce objects of the htest class and use the print() function to produce formatted output. For example:   tt &lt;- t.test(WT ~ LIFE) class(tt) tt ## [1] &quot;htest&quot; ## ## Welch Two Sample t-test ## ## data: WT by LIFE ## t = 0.60608, df = 98.866, p-value = 0.5459 ## alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0 ## 95 percent confidence interval: ## -0.3326225 0.6251763 ## sample estimates: ## mean in group 1 mean in group 2 ## 0.7867213 0.6404444   You can use this feature of R in your own functions. We will explore this by writing a function to test the null hypothesis that the variance to mean ratio of a vector of numbers is equal to one. Such a test might be used to investigate the spatial distribution (e.g. over natural sampling units such as households) of cases of a disease. Create a new function using the function() function:   v2m.test &lt;- function(data) {}   And start the function editor:   fix(v2m.test) Now edit this function to make it do something useful:   function(data) { nsu &lt;- length(data) obs &lt;- sum(data) m &lt;- obs / nsu v &lt;- var(data) vmr &lt;- v / m chi2 &lt;- sum((data - m)^2) / m df &lt;- nsu - 1 p &lt;- 1 - pchisq(chi2, df) names(chi2) &lt;- &quot;Chi-square&quot; names(df) &lt;- &quot;df&quot; names(vmr) &lt;- &quot;Variance : mean ratio&quot; v2m &lt;- list(method = &quot;Variance to mean test&quot;, data.name = deparse(substitute(data)), statistic = chi2, parameter = df, p.value = p, estimate = vmr) class(v2m) &lt;- &quot;htest&quot; return(v2m) }   Once you have made the changes shown above, check your work, save the file, and quit the editor. Before proceeding we should examine the v2m.test() function to make sure we understand what is happening: The first eight lines after the opening curly bracket ({) contain the required calculations. The next three lines use the names() function to give our variables names that will make sense in formatted output. The next line creates a list of items that the function returns using some of the names used by htest class objects. The next line tells R that the list object called v2m is of the class htest. The next line causes the function to return the v2m object (i.e. a list of class htest containing the named items method, data.name, statistic, parameter, p.value, and estimate). The final line ends the function definition. Note that objects of class htest may contain items with the following names:   Item Usage method Text description of the test used to title output data.name Name(s) of data or variables used for the test null.value The null value statistic Value of test statistic parameter A test parameter such as the degrees of freedom of the test statistic p.value The p-value of the test estimate An estimate (e.g. the mean) conf.int Confidence interval of estimate alternative Text describing the alternative hypothesis note Text note   We are now ready to test the v2m.test() function. This table:   Number of cases : 0 1 2 3 4 6 Number of households : 24 29 26 14 5 2   shows the number of cases of chronic (stunting) undernutrition found in a random sample of 100 households. We can reproduce the data behind this table using a combination of the c() and rep() functions:   stunt &lt;- c(rep(0,24), rep(1,29), rep(2,26), rep(3,14), rep(4,5), rep(5,0), rep(6,2)) table(stunt) ## stunt ## 0 1 2 3 4 6 ## 24 29 26 14 5 2   And use it to test our new v2m.test() function:   v2m.test(stunt)   Which should produce the following output:   v2m.test(stunt) ## ## Variance to mean test ## ## data: stunt ## Chi-square = 110.16, df = 99, p-value = 0.2083 ## sample estimates: ## Variance : mean ratio ## 1.11274   If your vm2.test() function does not produce this output then use the fix() function:   fix(v2m.test)   to check and edit the vm2.test() function and try again. The important thing to note from this exercise is that R allows us to specify a class for the output of our functions. This means that we can use standard R classes and functions to (e.g.) produce formatted output without us having to write commands to format the output ourselves. More importantly, it also means that we can write functions that return values when we need them to return values but can also produce formatted output when we need them to produce formatted output. Our v2m.test() function can produce values for later use:   vm &lt;- v2m.test(stunt) vm$p.value ## [1] 0.2083442   or produce formatted output:   v2m.test(stunt) ## ## Variance to mean test ## ## data: stunt ## Chi-square = 110.16, df = 99, p-value = 0.2083 ## sample estimates: ## Variance : mean ratio ## 1.11274   This way of working is not limited to using standard R classes and functions. R also allows us to define our own classes. We will explore this by defining functions and a new class to deal with two-by-two tables. We need to create two functions: One function will handle the calculations. A second function function will produce formatted output when required. Create a new function using the function() function:   rr22 &lt;- function(exposure, outcome) {}   And start the function editor:   fix(rr22)   Now edit this function to make it do something useful:   function(exposure, outcome) { tab &lt;- table(exposure, outcome) a &lt;- tab[1,1] b &lt;- tab[1,2] c &lt;- tab[2,1] d &lt;- tab[2,2] rr &lt;- (a / (a + b)) / (c / (c + d)) se.log.rr &lt;- sqrt((b / a) / (a + b) + (d / c) / (c + d)) lci &lt;- exp(log(rr) - 1.96 * se.log.rr) uci &lt;- exp(log(rr) + 1.96 * se.log.rr) rr22.output &lt;- list(estimate = rr, ci = c(lci, uci)) class(rr22.output) &lt;- &quot;rr22&quot; return(rr22.output) }   Once you have made the changes shown above, save the file and quit the editor. The rr22() function is similar to the tab2by2() function that you created in the second exercise of this tutorial except that the function now returns a list of values instead of formatted output:   fem &lt;- read.table(&quot;fem.dat&quot;, header = TRUE) attach(fem) rr22.test &lt;- rr22(SEX, LIFE) names(rr22.test) rr22.test$estimate rr22.test$conf.int rr22.test$conf.int[1] rr22.test$conf.int[2] ## The following objects are masked from fem (pos = 3): ## ## AGE, ANX, DEP, ID, IQ, LIFE, SEX, SLP, WT ## The following objects are masked from fem (pos = 8): ## ## AGE, ANX, DEP, ID, IQ, LIFE, SEX, SLP, WT ## [1] &quot;estimate&quot; &quot;ci&quot; ## [1] 2.054167 ## NULL ## NULL ## NULL   The function returns a list of class rr22:   class(rr22.test) ## [1] &quot;rr22&quot;   The displayed output from the rr22() function is, however, not pretty:   print(rr22.test) rr22(SEX, LIFE) ## $estimate ## [1] 2.054167 ## ## $ci ## [1] 0.966417 4.366232 ## ## attr(,&quot;class&quot;) ## [1] &quot;rr22&quot; ## $estimate ## [1] 2.054167 ## ## $ci ## [1] 0.966417 4.366232 ## ## attr(,&quot;class&quot;) ## [1] &quot;rr22&quot;   This can be fixed by creating a new function:   print.rr22 &lt;- function(x) {}   And start the function editor:   fix(print.rr22)   Now edit this function to make it do something useful:   function(x) { cat(&quot;RR : &quot;, x$estimate, &quot;\\n&quot;, &quot;95% CI : &quot;, x$ci[1], &quot;; &quot;, x$ci[2], &quot;\\n&quot;, sep = &quot;&quot;) }   Once you have made the changes shown above, check your work, save the file, and quit the editor. The function name print.rr22() indicates that this function contains the print method for objects of class rr22. All objects of class rr22 will use the function print.rr22() instead of the standard R print() function to produce formatted output:   rr22(SEX, LIFE) rr22.test &lt;- rr22(SEX, LIFE) rr22.test print(rr22.test) ## RR : 2.054167 ## 95% CI : 0.966417; 4.366232 ## RR : 2.054167 ## 95% CI : 0.966417; 4.366232 ## RR : 2.054167 ## 95% CI : 0.966417; 4.366232   Note that we can still extract returned values from an rr22 class object:   rr22.test$estimate   The print.rr22() function only controls the way an entire rr22 object is displayed. You might like to use the save() function to save the v2m.test(), rr22(), and print.rr22() functions before quitting R. We can now quit R:   q()   For this exercise there is no need to save the workspace image so click the No or Don’t Save button (GUI) or enter n when prompted to save the workspace image (terminal). 6.1 Summary R objects can be assigned a class or type. Objects of a specific class or type may share functions that extract and manipulate data common to members of that class. This allows you to write functions that handle data that is common to all members of that class (e.g. to produce formatted output for hypothesis testing functions). R provides a set of ready-made classes (e.g. htest) which can be used by standard R functions such as the print() and summary() functions. R allows you to create new classes and class-specific functions that can extract and manipulate data common to the new classes. Classes allows you to create versatile functions that return values when you need them to return values but can also produce formatted output when you need them to produce formatted output. Classes allow you to write functions that can be chained together so that the output of one function is the input of another function. "],["exercise7.html", "Exercise 7 Writing your own graphical functions 7.1 Summary", " Exercise 7 Writing your own graphical functions R provides a pretty full set of graphical functions for plotting data as well as plot() methods for a wide variety of statistical functions. There will be times, however, when you will need to write you own graphical functions to present and analyse data in a specific way. In this exercise we will create a function that produces a plot that may be used for assessing agreement between two methods of clinical measurement as described in: Bland JM, Altman DG. Statistical Methods for Assessing Agreement Between Two Methods of Clinical Measurement. The Lancet. 1986;1: 307–310. Which involves plotting the difference of two measurements against the mean of the two measurements and calculating and displaying limits of agreement. Start R and retrieve and attach the sample dataset:   ba &lt;- read.table(&quot;ba.dat&quot;, header = TRUE) attach(ba)   The ba data.frame contains measurements (in litres per minute) taken with a Wright peak flow meter and a Mini-Wright peak flow meter. This is the same data that is presented in the referenced Lancet article:   ## Wright Mini ## 1 494 512 ## 2 395 430 ## 3 516 520 ## 4 434 428 ## 5 476 500 ## 6 557 600 ## 7 413 364 ## 8 442 380 ## 9 650 658 ## 10 433 445 ## 11 417 432 ## 12 656 626 ## 13 267 260 ## 14 478 477 ## 15 178 259 ## 16 423 350 ## 17 427 451   You can examine the ba data.frame using the print() and summary() functions:   print(ba) ba summary(ba) ## Wright Mini ## 1 494 512 ## 2 395 430 ## 3 516 520 ## 4 434 428 ## 5 476 500 ## 6 557 600 ## 7 413 364 ## 8 442 380 ## 9 650 658 ## 10 433 445 ## 11 417 432 ## 12 656 626 ## 13 267 260 ## 14 478 477 ## 15 178 259 ## 16 423 350 ## 17 427 451 ## Wright Mini ## 1 494 512 ## 2 395 430 ## 3 516 520 ## 4 434 428 ## 5 476 500 ## 6 557 600 ## 7 413 364 ## 8 442 380 ## 9 650 658 ## 10 433 445 ## 11 417 432 ## 12 656 626 ## 13 267 260 ## 14 478 477 ## 15 178 259 ## 16 423 350 ## 17 427 451 ## Wright Mini ## Min. :178.0 Min. :259.0 ## 1st Qu.:417.0 1st Qu.:380.0 ## Median :434.0 Median :445.0 ## Mean :450.4 Mean :452.5 ## 3rd Qu.:494.0 3rd Qu.:512.0 ## Max. :656.0 Max. :658.0   The function() function allows us to create new functions in R:   ba.plot &lt;- function(a, b) {}   This creates an empty function called ba.plot() that expects two parameters called a and b. We could type the whole function in at the R command prompt but it is easier to use a text editor:   fix(ba.plot)   We will start be writing a basic function which we will gradually improve throughout this exercise. Edit the ba.plot() function to read:   function(a, b) { mean.two &lt;- (a + b) / 2 diff.two &lt;- a - b plot(mean.two, diff.two) }   Once you have made the changes shown above, check your work, save the file, and quit the editor. The function calculates the mean and the difference of the two measures and then plots the results. Lets try the ba.plot() function with the test data:   ba.plot(Wright, Mini)   The resulting plot is rather plain and lacks meaningful titles and axis labels. Use the fix() function to edit the ba.plot() function:   fix(ba.plot)   Edit the function to read:   function(a, b, title = &quot;Bland and Altman Plot&quot;) { a.txt &lt;- deparse(substitute(a)) b.txt &lt;- deparse(substitute(b)) x.lab &lt;- paste(&quot;Mean of&quot;, a.txt, &quot;and&quot;, b.txt) y.lab &lt;- paste(a.txt, &quot;-&quot;, b.txt) mean.two &lt;- (a + b) / 2 diff.two &lt;- a - b plot(mean.two, diff.two, xlab = x.lab, ylab = y.lab, main = title) }   Once you have made the changes shown above, check your work, save the file, and quit the editor. We have added a new parameter (title) to the function and given this a default value of Bland and Altman Plot. Adding title as a parameter means that we will be able to specify a title for the plot when we call the function. We have also used the function combination deparse(substitute()) to retrieve the names of the vectors passed to parameters a and b. The paste() function pastes pieces of text together. It is used here to create the text for the axis labels used with the plot() function. Lets try the ba.plot() function with the test data:   ba.plot(Wright, Mini)   We may also specify a title for the plot using the title parameter:   ba.plot(Wright, Mini, title = &quot;PEFR data&quot;)   We can now edit the function to calculate and plot mean, difference, and the limits of agreement. Use the fix() function to edit the ba.plot() function:   fix(ba.plot)   Edit the function to read:   function(a, b, title = &quot;Bland and Altman Plot&quot;) { a.txt &lt;- deparse(substitute(a)) b.txt &lt;- deparse(substitute(b)) x.lab &lt;- paste(&quot;Mean of&quot;, a.txt, &quot;and&quot;, b.txt) y.lab &lt;- paste(a.txt, &quot;-&quot;, b.txt) mean.two &lt;- (a + b) / 2 diff.two &lt;- a - b plot(mean.two, diff.two, xlab = x.lab, ylab = y.lab, main = title) mean.diff &lt;- mean(diff.two) sd.diff &lt;- sd(diff.two) upper &lt;- mean.diff + 1.96 * sd.diff lower &lt;- mean.diff - 1.96 * sd.diff lines(x = range(mean.two), y = c(mean.diff, mean.diff), lty = 3) lines(x = range(mean.two), y = c(upper, upper), lty = 3) lines(x = range(mean.two), y = c(lower, lower), lty = 3) }   Once you have made the changes shown above, check your work, save the file and quit the editor. We have used the mean() and sd() functions to calculate the mean and standard deviation of the difference between the two measures and calculated the limits of agreement (upper and lower) assuming that the differences are Normally distributed. The lines() function is then used to plot the mean and the limits of agreement on top of the existing scatter plot. The parameter lty = 3 used with the lines() function specifies dotted lines. R provided a great number of graphical parameters that can be used to customise plots. You can see a list of these parameters using:   help(par)   These parameters can be specified for almost all graphical functions. Lets try the ba.plot() function with the test data:   ba.plot(Wright, Mini, title = &quot;Difference vs. mean for PEFR data&quot;)   The function is almost complete. All that remains to do is to label the lines with the values of the mean difference and the limits of agreement. Use the fix() function to edit the ba.plot() function:   fix(ba.plot)   Edit the function to read:   function(a, b, title = &quot;Bland and Altman Plot&quot;) { a.txt &lt;- deparse(substitute(a)) b.txt &lt;- deparse(substitute(b)) x.lab &lt;- paste(&quot;Mean of&quot;, a.txt, &quot;and&quot;, b.txt) y.lab &lt;- paste(a.txt, &quot;-&quot;, b.txt) mean.two &lt;- (a + b) / 2 diff.two &lt;- a - b plot(mean.two, diff.two, xlab = x.lab, ylab = y.lab, main = title) mean.diff &lt;- mean(diff.two) sd.diff &lt;- sd(diff.two) upper &lt;- mean.diff + 1.96 * sd.diff lower &lt;- mean.diff - 1.96 * sd.diff lines(x = range(mean.two), y = c(mean.diff, mean.diff), lty = 3) lines(x = range(mean.two), y = c(upper, upper), lty = 3) lines(x = range(mean.two), y = c(lower, lower), lty = 3) m.text &lt;- round(mean.diff, digits = 1) u.text &lt;- round(upper , digits = 1) l.text &lt;- round(lower, digits = 1) text(max(mean.two), mean.diff, m.text, adj = c(1,1)) text(max(mean.two), upper, u.text, adj = c(1,1)) text(max(mean.two), lower, l.text, adj = c(1,1)) }   Once you have made the changes shown above, check your work, save the file, and quit the editor. We have used the round() function to limit the display of the mean difference and the limits of agreement to one decimal place and used the text() function to display these (rounded) values. The adj parameter to the text() function controls the position and justification of text. Let’s try the ba.plot() function with the test data:   ba.plot(Wright, Mini, title = &quot;PEFR data&quot;)   The graphical function is now complete. One improvement that we could make is for the function to produce a chart and return the values of the mean difference and the limits of agreement. We would do this in exactly the same way as we would with a non-graphical function. We would return the mean difference and the limits of agreement as members of a list. We could also specify a class for the returned list and create a class specific print() function (or method) to produce nicely formatted output. Use the fix() function to edit the ba.plot() function:   fix(ba.plot)   Edit the function to read:   function(a, b, title = &quot;Bland and Altman Plot&quot;) { a.txt &lt;- deparse(substitute(a)) b.txt &lt;- deparse(substitute(b)) x.lab &lt;- paste(&quot;Mean of&quot;, a.txt, &quot;and&quot;, b.txt) y.lab &lt;- paste(a.txt, &quot;-&quot;, b.txt) mean.two &lt;- (a + b) / 2 diff.two &lt;- a - b plot(mean.two, diff.two, xlab = x.lab, ylab = y.lab, main = title) mean.diff &lt;- mean(diff.two) sd.diff &lt;- sd(diff.two) upper &lt;- mean.diff + 1.96 * sd.diff lower &lt;- mean.diff - 1.96 * sd.diff lines(x = range(mean.two), y = c(mean.diff, mean.diff), lty = 3) lines(x = range(mean.two), y = c(upper, upper), lty = 3) lines(x = range(mean.two), y = c(lower, lower), lty = 3) m.text &lt;- round(mean.diff, digits = 1) u.text &lt;- round(upper , digits = 1) l.text &lt;- round(lower, digits = 1) text(max(mean.two), mean.diff, m.text, adj = c(1,1)) text(max(mean.two), upper, u.text, adj = c(1,1)) text(max(mean.two), lower, l.text, adj = c(1,1)) ba &lt;- list(mean = mean.diff, limits = c(lower, upper)) class(ba) &lt;- &quot;ba&quot; return(ba) }   Once you have made the changes shown above, save the file and quit the editor. Create a print() function for objects of the ba class:   print.ba &lt;- function(x) {}   Use the fix() function to edit the new function:   fix(print.ba)   Edit the function to read:   function(x) { cat(&quot;Mean difference : &quot;, x$mean, &quot;\\n&quot;, &quot;Limits of agreement : &quot;, x$limits[1], &quot;; &quot;, x$limits[2], &quot;\\n&quot;, sep = &quot;&quot;) }   Once you have made the changes shown above, check your work, save the file, and quit the editor. Let’s try the ba.plot() function with the test data:   ba.plot(Wright, Mini, title = &quot;PEFR data&quot;) ## Mean difference : -2.117647 ## Limits of agreement : -78.0973; 73.86201   The function produces the plot and returns the mean difference and limits of agreement as a list of class ba which is formatted and printed by the print.ba() function. We can manipulate the returned values just as we would with any other function:   ba.test &lt;- ba.plot(Wright, Mini) print(ba.test) ba.test ba.test$mean ba.test$limits ba.test$limits[1] ba.test$limits[2] ## Mean difference : -2.117647 ## Limits of agreement : -78.0973; 73.86201 ## Mean difference : -2.117647 ## Limits of agreement : -78.0973; 73.86201 ## [1] -2.117647 ## [1] -78.09730 73.86201 ## [1] -78.0973 ## [1] 73.86201   You might like to use the save() function to save the ba.plot() and print.ba() functions before quitting R. We can now quit R:   q()   For this exercise there is no need to save the workspace image so click the No or Don’t Save button (GUI) or enter n when prompted to save the workspace image (terminal). 7.1 Summary R allows you to create functions that produce graphical output. R allows you to create functions that produce graphical output and return values. R objects can be assigned a class or type. R allows you to create new classes and class-specific functions that can extract and manipulate data common to the new classes. Classes allows you to create versatile functions that return values when we need them to return values but can also produce formatted output when we need them to produce formatted output. Classes allow you to write functions that can be chained together so that the output of one function is the input of another function. "],["exercise8.html", "Exercise 8 More graphical functions 8.1 Population pyramid 8.2 Pareto chart 8.3 Adding confidence intervals or error bars on plots 8.4 Mesh map 8.5 Combining plots 8.6 Summary", " Exercise 8 More graphical functions Graphical functions in R are just like any other function in R in the sense that R provides you with a set of functions which can be altered or added to. In this exercise we will experiment with some of the graphical functions provided by R to demonstrate the flexibility of graphical functions in R. We will then use the graphical functions that we experiment with to create some useful graphical functions of our own. The first function that we will develop will be a function that is capable of plotting two data series on a single graph. We will take this exercise slowly in order to introduce some further graphical functions. Before we go any further we should start R and retrieve a dataset:   mal &lt;- read.table(&quot;malaria.dat&quot;, header = TRUE) attach(mal)   The file malaria.dat contains data on rainfall (in mm) and the number of cases of malaria reported from health centres in an administrative district of Ethiopia between July 1997 and July 1999. The columns in this dataset are as follows:   Time Month and year (as text) Cases Number of cases of malaria reported Rain Rainfall in mm Examine the dataset:   mal ## Time Cases Rain ## 1 Jul-97 997 68.5 ## 2 Aug-97 824 162.1 ## 3 Sep-97 573 138.8 ## 4 Oct-97 586 222.2 ## 5 Nov-97 523 115.5 ## 6 Dec-97 968 37.2 ## 7 Jan-98 985 96.6 ## 8 Feb-98 745 99.1 ## 9 Mar-98 520 51.2 ## 10 Apr-98 406 80.0 ## 11 May-98 523 112.4 ## 12 Jun-98 560 183.7 ## 13 Jul-98 671 101.0 ## 14 Aug-98 667 252.5 ## 15 Sep-98 768 40.8 ## 16 Oct-98 990 193.7 ## 17 Nov-98 775 17.5 ## 18 Dec-98 833 0.0 ## 19 Jan-99 672 33.5 ## 20 Feb-99 505 0.0 ## 21 Mar-99 320 106.8 ## 22 Apr-99 274 117.4 ## 23 May-99 263 175.8 ## 24 Jun-99 264 187.5 ## 25 Jul-99 179 283.5 First we will plot the number of cases of malaria seen over time using the plot() function:   plot(Cases, type = &quot;l&quot;)   The problem with this plot is that it does not treat the data as a time series. Adding the Time variable to the plot does not solve the problem:   plot(Time, Cases, type = &quot;l&quot;) plot(as.factor(Time), Cases, type = &quot;l&quot;) Because Time is a factor variable. If you convert Time to a character variable using as.character() or prevent R from converting Time to a factor using the as.is parameter to the read.table() function the plot() function will return an error because it expects a numeric x-axis variable. We should, instead, specify a time series (ts) class object. Rather than change the original data, we will create a new object using the ts() function:   cases.ts &lt;- ts(Cases, start = c(1997, 7), frequency = 12)   Examine the cases.ts object:   cases.ts ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1997 997 824 573 586 523 968 ## 1998 985 745 520 406 523 560 671 667 768 990 775 833 ## 1999 672 505 320 274 263 264 179   We can now plot cases.ts as a time series:   plot(cases.ts) We might want to explore the association between the Rain and Cases variables. A simple scatter plot is not particularly informative:   plot(Rain, Cases) It is better to treat both variables as time series (which they are) and use the built-in plot() methods for objects of class ts:   rain.cases.ts &lt;- ts(cbind(Rain, Cases), start = c(1997,7), frequency = 12) plot(rain.cases.ts)   The association between the Rain and Cases variables is now clearer with the number of malaria cases peaking shortly after peaks in rainfall. The plot() function when used with objects of class ts produces useful output but it is not particularly flexible and the output is, sometimes, not particularly pretty. We can however use basic graphical functions to produce multiple plots. First we will set the mfrow graphical parameter using the par() function:   par(mfrow = c(2, 1))   The par() function sets a graphical parameter. The mfrow parameter is used to set the number of charts that will appear on a page in rows and columns. We have specified two rows with one chart per row. Test this by plotting two charts:   plot(Rain, type = &quot;l&quot;) plot(Cases, type = &quot;l&quot;) We will want to have tick-marks on the x-axis of each for each record. We can set the number of tick-marks on axes by setting the lab graphical parameter using the par() function:   par(lab = c(length(Time), 10, 7))   The par() function sets a graphical parameter. The lab parameter is used to set the number tick-marks on the x and y axes and the label size. We have specified a tick-mark on the x-axis for each record (i.e. using length(Time)), ten tick-marks on the y-axis, and a label length of seven. Test this by plotting two charts:   plot(Rain, type = &quot;l&quot;) plot(Cases, type = &quot;l&quot;)   The problem with these charts is that the month and year are not displayed on the x-axis. We can get round this by plotting a chart without axes and then specifying the axes and labels directly:   plot(Rain, type = &quot;l&quot;, axes = FALSE, xlab = &quot;Time&quot;, ylab = &quot;mm&quot;, main = &quot;Rainfall&quot;) axis(side = 1, labels = as.character(Time), at = 1:length(Time)) axis(side = 2) plot(Cases, type = &quot;l&quot;, axes = FALSE, xlab = &quot;Time&quot;, ylab = &quot;n&quot;, main = &quot;Cases&quot;) axis(side = 1, labels = as.character(Time), at = 1:length(Time)) axis(side = 2)   The resulting charts now look much better (you may need to resize the plot to display the x-axis labels correctly) but it would be nice to be able draw the two lines on a single chart. Before proceeding we will use the par() function to specify one plot per window (using the mfrow parameter) and set the default number of tick-marks on the axes (using the lab parameter):   par(mfrow = c(1, 1)) par(lab = c(5, 5, 7))   And then use the plot() and lines() function to draw the two lines on the same graph:   plot(Cases, type = &quot;l&quot;) lines(Rain, lty = 2)   The problem with this is that the ranges of the two variables are different and the plot() function automatically sets the y-axis to the range of the specified variable. To fix this problem we need to set the limits of the y-axis to the minimum and maximum value of both of variables using the ylim parameter of the plot() function:   plot(Cases, type = &quot;l&quot;, ylim = c(min(Cases, Rain), max(Cases, Rain))) lines(Rain, lty = 2)   We can improve the chart by adding a legend:   legend(18, 1000, legend = c(&quot;Cases&quot;, &quot;Rainfall (mm)&quot;), lty = c(1,2)) We could continue to improve the chart (e.g. by adding labels for the x-axis tick-marks taken from the Time variable, specifying more meaningful axis labels, and specifying a title) but the chart would be more useful if each variable made full use of the plotting area. We can do this by plotting one chart on top of another by using the new graphical parameter:   par(lab = c(length(Time), 5, 7)) plot(Cases, type = &quot;l&quot;, lty = 1, axes = FALSE) axis(side = 2) par(new = TRUE) plot(Rain, type = &quot;l&quot;, lty = 2, axes = FALSE) axis(side = 4) axis(side = 1)   This chart is much clearer but there are still some improvements that could be made: The chart should have a title. We can do this using the main parameter of either of the plot() functions. The y-axis labels are displayed on top of each other beside the left-hand y-axis. We can solve this problem by preventing the second plot() function from displaying a y-axis label (i.e. by specifying an empty character string for the ylab parameter). We will need to make room on the right-hand side of the chart for an axis label (i.e. by setting the mar (margin) graphical parameter) and place the label there ourselves (using the mtext() function). The x-axis should display the month and year which are held as character strings in the Time variable. We can do this using the labels parameter of the axis() function after setting the appropriate number of tick-marks using the lab graphical parameter. The x-axis should be properly labelled. We can do this using the xlab parameters of the plot() functions. An empty string must be specified for one of the plot() functions in order to prevent the default label from being displayed. Try this now:   par(mar = c(5, 5, 4, 5)) par(lab = c(length(Time), 5, 7)) plot(Cases, type = &quot;l&quot;, lty = 1, axes = FALSE, xlab = &quot;&quot;, ylab = &quot;&quot;, main = &quot;Malaria cases and rainfall&quot;) axis(side = 2) mtext(text = &quot;Malaria cases&quot;, side = 2, line = 3) par(new = TRUE) plot(Rain, type = &quot;l&quot;, lty = 2, axes = FALSE, xlab = &quot;Month &amp; Year&quot;, ylab = &quot;&quot;) axis(side = 4) mtext(text = &quot;Rainfall (mm)&quot;, side = 4, line = 3) axis(side = 1, labels = as.character(Time), at = 1:length(Time)) Now that we know how to create a two-axis chart, we can write a function that we will be able to use whenever we need to plot two variables on the same chart. Create a new function called plot2var():   plot2var &lt;- function() {}   This creates an empty function called plot2var(). Use the fix() function to edit the plot2var() function:   fix(plot2var)   Edit the function to read:   function(y1, y2, x.ticks, x.lab = deparse(substitute(x.ticks)), y1.lab = deparse(substitute(y1)), y2.lab = deparse(substitute(y2)), main = paste(y1.lab, &quot;&amp;&quot;, y2.lab)) { old.par.mar &lt;- par(&quot;mar&quot;) old.par.lab &lt;- par(&quot;lab&quot;) par(mar = c(5, 5, 4, 5)) if(!missing(x.ticks)) {par(lab = c(length(x.ticks), 5, 7))} plot(y1, type = &quot;l&quot;, lty = 1, axes = FALSE, xlab = &quot;&quot;, ylab = &quot;&quot;, main = main) axis(side = 2) mtext(text = y1.lab, side = 2, line = 3) par(new = TRUE) plot(y2, type = &quot;l&quot;, lty = 2, axes = FALSE, ylab = &quot;&quot;, xlab = x.lab) axis(side = 4) mtext(text = y2.lab, side = 4, line = 3) if(!missing(x.ticks)) { axis(side = 1, labels = as.character(x.ticks), at = 1:length(x.ticks)) } else {axis(side = 1)} par(mar = old.par.mar) par(lab = old.par.lab) }   Once you have made the changes shown above, save the file and quit the editor. Note that with this function we have given some of the parameters default values in the function definition and we have also used the if() function to check whether the user specified a value for the x.ticks parameter. We also save and restore the graphical parameters mar and lab so as to prevent changes to these parameters in the plot2var() function affecting other graphical functions. Let’s try the plot2var() function with the test data:   plot2var(Rain, Cases) plot2var(Rain, Cases, Time)   Note how the function has used default values for the axis labels and chart title. We can override these default values if we want to:   plot2var(Rain, Cases, Time, x.lab = &quot;Month and Year&quot;, y1.lab = &quot;Rainfall (mm)&quot;, y2.lab = &quot;Cases of malaria&quot;)   You might like to use the save() function to save the plot2var() function. As an exercise you might want to edit the plot2var() function to automatically add a legend to the two-axis chart using the legend() function with y1.lab and y2.lab. Before continuing we should detach the mal data.frame:   detach(mal) 8.1 Population pyramid A common chart type that is not available in many statistical applications and in R is the population pyramid. Before we go any further we should retrieve a dataset:   pop &lt;- read.table(&quot;pop.dat&quot;, header = TRUE) attach(pop) ## The following objects are masked from fem (pos = 4): ## ## AGE, SEX ## The following objects are masked from fem (pos = 5): ## ## AGE, SEX ## The following objects are masked from fem (pos = 10): ## ## AGE, SEX   The file pop.dat contains data on the age (in months) and sex of 438 children aged between six and sixty months collected as part of a nutritional anthropometry survey of the Khosh Valley in Northeast Afghanistan. The columns in this dataset are as follows:   AGE Age of the child in months SEX Sex of the child (M/F)   Examine the first twenty records of the dataset:   pop[1:20, ] ## AGE SEX ## 1 7 M ## 2 42 M ## 3 60 M ## 4 60 F ## 5 48 M ## 6 60 F ## 7 18 M ## 8 48 M ## 9 60 F ## 10 36 M ## 11 24 F ## 12 60 M ## 13 60 M ## 14 48 F ## 15 18 M ## 16 60 M ## 17 6 M ## 18 7 M ## 19 12 M ## 20 60 M   The first step is to make groups from the AGE variable since many ages are biased towards full years:   table(AGE) barplot(table(AGE), col = &quot;white&quot;) ## AGE ## 6 7 8 9 10 12 13 14 15 17 18 22 23 24 25 26 30 34 36 38 40 42 48 54 60 ## 7 5 8 15 3 21 1 3 5 1 45 1 1 48 1 2 24 1 80 2 1 9 67 4 83   So we will centre the age-groups around the months representing full years:   age.group &lt;- cut(AGE, c(0, 17, 29, 41, 53, 99)) We can check that the grouping operation has worked as expected by tabulating AGE and age.group:   table(AGE, age.group) ## age.group ## AGE (0,17] (17,29] (29,41] (41,53] (53,99] ## 6 7 0 0 0 0 ## 7 5 0 0 0 0 ## 8 8 0 0 0 0 ## 9 15 0 0 0 0 ## 10 3 0 0 0 0 ## 12 21 0 0 0 0 ## 13 1 0 0 0 0 ## 14 3 0 0 0 0 ## 15 5 0 0 0 0 ## 17 1 0 0 0 0 ## 18 0 45 0 0 0 ## 22 0 1 0 0 0 ## 23 0 1 0 0 0 ## 24 0 48 0 0 0 ## 25 0 1 0 0 0 ## 26 0 2 0 0 0 ## 30 0 0 24 0 0 ## 34 0 0 1 0 0 ## 36 0 0 80 0 0 ## 38 0 0 2 0 0 ## 40 0 0 1 0 0 ## 42 0 0 0 9 0 ## 48 0 0 0 67 0 ## 54 0 0 0 0 4 ## 60 0 0 0 0 83 We now use the table() function to produce the summary data for the population pyramid:   table(age.group, SEX) ## SEX ## age.group F M ## (0,17] 34 35 ## (17,29] 54 44 ## (29,41] 49 59 ## (41,53] 39 37 ## (53,99] 41 46   We will construct our population pyramid using the barplot() function:   barplot(table(age.group, SEX)) The default behaviour of the barplot() function is to produce stacked bars. We can set the beside parameter to display the bars side-by-side:   barplot(table(age.group, SEX), beside = TRUE)   We can also use the horiz parameter to present the data as horizontal bars:   barplot(table(age.group, SEX), beside = TRUE, horiz = TRUE) In order to centre the bars around zero we need to make one column of the summary data table contain negative numbers:   tab &lt;- table(age.group, SEX) tab ## SEX ## age.group F M ## (0,17] 34 35 ## (17,29] 54 44 ## (29,41] 49 59 ## (41,53] 39 37 ## (53,99] 41 46 tab[ ,1] &lt;- -tab[ ,1] tab ## SEX ## age.group F M ## (0,17] -34 35 ## (17,29] -54 44 ## (29,41] -49 59 ## (41,53] -39 37 ## (53,99] -41 46 barplot(tab, beside = TRUE, horiz = TRUE)   This is looking better. We still need to shift the second set of bars down beside the first set of bars using thespace parameter:   barplot(tab, beside = TRUE, horiz = TRUE, space = c(0, -nrow(tab)))   The axis labels are wrong but we can fix that using the names.arg parameter:   bar.names &lt;- c(dimnames(tab)$age.group, dimnames(tab)$age.group) barplot(tab, beside = TRUE, horiz = TRUE, space = c(0, -nrow(tab)), names.arg = bar.names) The chart can still be improved upon by making the fill-colour of each bar white and by expanding the x-axis slightly:   barplot(tab, beside = TRUE, horiz = TRUE, space = c(0, -nrow(tab)), col = &quot;white&quot;, xlim = c(min(tab) * 1.2, max(tab) * 1.2), names.arg = bar.names)   The chart would be better if the x-axis displayed only positive numbers:   barplot(tab, beside = TRUE, horiz = TRUE, space = c(0, -nrow(tab)), col = &quot;white&quot;, xlim = c(min(tab) * 1.2, max(tab) * 1.2), names.arg = bar.names, axes = FALSE) axis(side = 1, labels = abs(axTicks(side = 1)), at = (axTicks(side = 1))) Now we know how to create a population pyramid, we can write a function that we will be able to use whenever we need to plot a population pyramid. Create a new function called pyramid.plot():   pyramid.plot &lt;- function() {}   This creates an empty function called pyramid.plot(). Use the fix() function to edit the pyramid.plot() function:   fix(pyramid.plot) Edit the function to read:   function(x, g, main = paste(&quot;Pyramid plot of&quot;, deparse(substitute(x)), &quot;by&quot;, deparse(substitute(g))), xlab = paste(deparse(substitute(g)), &quot;(&quot;, levels(g)[1], &quot;/&quot;,levels(g)[2],&quot;)&quot;), ylab = deparse(substitute(x))) { tab &lt;- table(x, g) tab[ ,1] &lt;- -tab[ ,1] barplot(tab, horiz = TRUE, beside = TRUE, space = c(0, -nrow(tab)), names.arg = c(dimnames(tab)$x, dimnames(tab)$x), xlim = c(min(tab) * 1.2, max(tab) * 1.2), col = &quot;white&quot;, main = main, xlab = xlab, ylab = ylab, axes = FALSE) axis(side = 1, labels = abs(axTicks(side = 1)), at = (axTicks(side = 1))) }   Note that with this function we have given some of the parameters default values in the function definition. Giving default values to parameters is useful because it means that you do not need to specify parameters such as titles and axis labels unless you want to. Many R functions use default parameters which are usually set to the most frequently used values. Once you have made the changes shown above, check your work, save the file, and quit the editor. Let’s try the pyramid.plot() function with the test data:   pyramid.plot(age.group, SEX)   Note how the function has used default values for the axis labels and chart titles. We can override these default values if we want to:   pyramid.plot(age.group, SEX, ylab = &quot;Months&quot;, xlab = &quot;Sex F / M&quot;, main = &quot;Children by age and sex&quot;)   You might like to use the save() function to save the pyramid.plot() function. 8.2 Pareto chart Another type of chart that is missing from many statistical applications is the Pareto chart which is a bar chart where the bars are sorted by the bar value with the largest bar drawn first. Such a chart is easier to interpret than a pie chart, particularly when there are more than a few categories being plotted. Before we go any further we should detach the pop data.frame and retrieve a new dataset:   detach(pop) sssw &lt;- read.table(&quot;sssw.dat&quot;, header = TRUE) attach(sssw)   The file sssw.dat contains data on the marital status, home circumstances, and ethnic group of 152 persons recruited into a study into the levels of stress experienced by student social workers in the United Kingdom. The columns in this dataset are as follows:   marital Marital status coded as: 1 = Married 2 = Single 3 = Divorced 4 = Separated 5 = Cohabiting 6 = Widowed living Living with… coded as: 1 = Alone 2 = Parents or siblings 3 = Partner 4 = Partner and children 5 = Children 6 = Friends or colleagues ethnic Ethnic group coded as: 1 = African 2 = West-Indian 3 = Indian 4 = Pakistani 5 = Bangladeshi 6 = East African Asian 7 = Chinese 8 = Cypriot 9 = Black European 10 = White European 11 = Other Examine the dataset:   sssw[1:20, ] ## marital living ethnic ## 1 2 5 2 ## 2 1 4 2 ## 3 1 1 11 ## 4 1 4 10 ## 5 2 3 10 ## 6 2 6 10 ## 7 1 4 10 ## 8 1 4 10 ## 9 2 2 4 ## 10 2 6 10 ## 11 2 5 10 ## 12 1 3 1 ## 13 5 3 10 ## 14 1 4 10 ## 15 2 6 3 ## 16 1 4 10 ## 17 1 3 3 ## 18 5 3 10 ## 19 3 5 10 ## 20 5 3 10   Producing a bar chart from this data is simple as long as we remember to pass summary data (i.e. created using the table() function) to the barplot() function instead of the variable name:   barplot(table(marital)) barplot(table(living)) barplot(table(ethnic)) barplot(table(marital)) barplot(table(living)) barplot(table(ethnic))   Creating a Pareto chart only requires us to sort the summary data. We do this using the rev() and sort() functions:   barplot(rev(sort(table(marital)))) Having to specify rev(sort(table(variable))) each time we want to produce a Pareto plot is rather tedious but now that we know how to create a Pareto chart, we can write a function that we will be able to use whenever we need to plot a Pareto chart. Create a new function called pareto():   pareto &lt;- function() {}   This creates an empty function called pareto(). Use the fix() function to edit the pareto() function:   fix(pareto) Edit the function to read:   function(x, xlab = deparse(substitute(x)), ylab = &quot;Count&quot;, main = paste(&quot;Pareto Chart of&quot;, deparse(substitute(x)))) { barplot(rev(sort(table(x))), xlab = xlab, ylab = ylab, main = main, col = &quot;white&quot;) }   Once you have made the changes shown above, check your work, save the file, and quit the editor. Let’s try the pareto() function with the test data:   pareto(marital)   Note how the function has used default values for the axis labels and chart titles. We can override these default values if we want to:   pareto(marital, ylab = &quot;n&quot;, xlab = &quot;Marital Status&quot;, main = &quot;Marital Status&quot;) Note that we can use value labels if the variable we plot is a factor with value labels as levels rather than a simple numeric vector:   ms &lt;- as.factor(marital) levels(ms) &lt;- c(&quot;Married&quot;, &quot;Single&quot;, &quot;Divorced&quot;, &quot;Separated&quot;, &quot;Cohabiting&quot;, &quot;Widowed&quot;) table(ms) pareto(ms, ylab = &quot;n&quot;, xlab = &quot;Marital Status&quot;, main = &quot;Marital Status&quot;) ## ms ## Married Single Divorced Separated Cohabiting Widowed ## 32 90 5 4 21 0   You may need to resize the plot to display the x-axis labels correctly. You might like to use the save() function to save the pareto() function. 8.3 Adding confidence intervals or error bars on plots You may want to plot your data with confidence intervals or error bars. R does not have a function to do this but it is a relatively simple matter to write a function to do so. On the way we will use some of Rs data management functions as well. Before we go any further we should detach the sssw data.frame and retrieve a new dataset:   detach(sssw) diets &lt;- read.table(&quot;diets.dat&quot;, header = TRUE)   The file diets.dat contains data from a trial of two different diets undertaken at an adult therapeutic feeding centre in Somalia. The columns in this dataset are as follows:   day The day after start of diet that measurements were taken oedema Type of undernutrition coded as: 1 = Oedematous; 2 = Marasmic diet The trial diets coded as: LP = Low protein; HP = High protein wt Mean weight change weight velocity) in g/kg/day since the previous measurement sd Standard deviation of weight change in g/kg/day n Number of subjects at each observation   Examine the dataset:   diets ## day oedema diet wt sd n ## 1 3 1 HP 1.1 11.5 37 ## 2 6 1 HP 1.0 11.0 37 ## 3 9 1 HP 1.2 8.4 37 ## 4 12 1 HP -1.0 7.4 37 ## 5 15 1 HP -0.9 3.2 37 ## 6 18 1 HP -1.7 5.8 37 ## 7 21 1 HP -1.7 4.8 37 ## 8 24 1 HP -1.7 4.8 37 ## 9 27 1 HP -1.7 5.3 37 ## 10 30 1 HP -2.5 6.8 37 ## 11 33 1 HP -1.3 8.4 37 ## 12 3 2 HP 5.1 15.1 291 ## 13 6 2 HP 5.1 13.4 291 ## 14 9 2 HP 4.5 6.5 291 ## 15 12 2 HP 5.2 9.9 291 ## 16 15 2 HP 4.6 8.2 291 ## 17 18 2 HP 5.1 13.4 291 ## 18 21 2 HP 5.0 13.4 291 ## 19 24 2 HP 5.2 15.1 291 ## 20 27 2 HP 5.1 11.6 291 ## 21 30 2 HP 5.0 9.2 291 ## 22 33 2 HP 5.2 8.2 291 ## 23 3 1 LP -3.0 15.7 65 ## 24 6 1 LP -2.5 11.9 65 ## 25 9 1 LP -2.0 13.7 65 ## 26 12 1 LP 2.0 11.0 65 ## 27 15 1 LP 2.1 13.2 65 ## 28 18 1 LP 1.7 16.2 65 ## 29 21 1 LP 4.0 11.0 65 ## 30 24 1 LP 6.5 14.7 65 ## 31 27 1 LP 6.4 10.2 65 ## 32 30 1 LP 6.6 8.0 65 ## 33 33 1 LP 6.5 11.0 65 ## 34 3 2 LP 5.1 11.1 86 ## 35 6 2 LP 6.0 8.2 86 ## 36 9 2 LP 5.1 8.2 86 ## 37 12 2 LP 6.5 11.9 86 ## 38 15 2 LP 6.4 9.1 86 ## 39 18 2 LP 5.9 13.8 86 ## 40 21 2 LP 6.1 16.5 86 ## 41 24 2 LP 4.0 9.1 86 ## 42 27 2 LP 3.1 12.8 86 ## 43 30 2 LP 4.0 7.3 86 ## 44 33 2 LP 5.0 9.1 86 Note that the dataset contains a summary of the results from the four arms of the trial:   Arm Oedema Therapeutic diet 1 Present High protein 2 Present Low protein 3 Absent High protein 4 Absent Low protein   with observations at 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, and 33 days after admission. We can calculate a confidence interval for the mean weight velocity (wt) using the data in the sd and n variables. We will use the transform() function to do this:   diets &lt;- transform(diets, lci = wt - sd / sqrt(n), uci = wt + sd / sqrt(n))   In this case we are calculating confidence intervals as plus or minus one standard error of the mean. The transform() function is very useful as it can add columns directly to a data.frame or transform data already stored in a data.frame. Examine the diets data.frame:   diets ## day oedema diet wt sd n lci uci ## 1 3 1 HP 1.1 11.5 37 -0.7905884 2.99058835 ## 2 6 1 HP 1.0 11.0 37 -0.8083889 2.80838886 ## 3 9 1 HP 1.2 8.4 37 -0.1809515 2.58095149 ## 4 12 1 HP -1.0 7.4 37 -2.2165525 0.21655251 ## 5 15 1 HP -0.9 3.2 37 -1.4260768 -0.37392324 ## 6 18 1 HP -1.7 5.8 37 -2.6535141 -0.74648587 ## 7 21 1 HP -1.7 4.8 37 -2.4891151 -0.91088486 ## 8 24 1 HP -1.7 4.8 37 -2.4891151 -0.91088486 ## 9 27 1 HP -1.7 5.3 37 -2.5713146 -0.82868537 ## 10 30 1 HP -2.5 6.8 37 -3.6179131 -1.38208689 ## 11 33 1 HP -1.3 8.4 37 -2.6809515 0.08095149 ## 12 3 2 HP 5.1 15.1 291 4.2148223 5.98517768 ## 13 6 2 HP 5.1 13.4 291 4.3144781 5.88552191 ## 14 9 2 HP 4.5 6.5 291 4.1189633 4.88103675 ## 15 12 2 HP 5.2 9.9 291 4.6196517 5.78034828 ## 16 15 2 HP 4.6 8.2 291 4.1193075 5.08069251 ## 17 18 2 HP 5.1 13.4 291 4.3144781 5.88552191 ## 18 21 2 HP 5.0 13.4 291 4.2144781 5.78552191 ## 19 24 2 HP 5.2 15.1 291 4.3148223 6.08517768 ## 20 27 2 HP 5.1 11.6 291 4.4199960 5.78000404 ## 21 30 2 HP 5.0 9.2 291 4.4606864 5.53931355 ## 22 33 2 HP 5.2 8.2 291 4.7193075 5.68069251 ## 23 3 1 LP -3.0 15.7 65 -4.9473453 -1.05265467 ## 24 6 1 LP -2.5 11.9 65 -3.9760133 -1.02398666 ## 25 9 1 LP -2.0 13.7 65 -3.6992759 -0.30072414 ## 26 12 1 LP 2.0 11.0 65 0.6356179 3.36438208 ## 27 15 1 LP 2.1 13.2 65 0.4627415 3.73725850 ## 28 18 1 LP 1.7 16.2 65 -0.3093627 3.70936270 ## 29 21 1 LP 4.0 11.0 65 2.6356179 5.36438208 ## 30 24 1 LP 6.5 14.7 65 4.6766894 8.32331060 ## 31 27 1 LP 6.4 10.2 65 5.1348457 7.66515429 ## 32 30 1 LP 6.6 8.0 65 5.6077221 7.59227788 ## 33 33 1 LP 6.5 11.0 65 5.1356179 7.86438208 ## 34 3 2 LP 5.1 11.1 86 3.9030562 6.29694378 ## 35 6 2 LP 6.0 8.2 86 5.1157713 6.88422874 ## 36 9 2 LP 5.1 8.2 86 4.2157713 5.98422874 ## 37 12 2 LP 6.5 11.9 86 5.2167900 7.78321000 ## 38 15 2 LP 6.4 9.1 86 5.4187218 7.38127824 ## 39 18 2 LP 5.9 13.8 86 4.4119077 7.38809227 ## 40 21 2 LP 6.1 16.5 86 4.3207592 7.87924076 ## 41 24 2 LP 4.0 9.1 86 3.0187218 4.98127824 ## 42 27 2 LP 3.1 12.8 86 1.7197405 4.48025950 ## 43 30 2 LP 4.0 7.3 86 3.2128208 4.78717924 ## 44 33 2 LP 5.0 9.1 86 4.0187218 5.98127824   Two new columns (lci and uci) have been added. Now that we have calculated the confidence intervals we should, for convenience, split the diets data.frame into four separate data.frames (one for each arm of the trial):   oed.hp &lt;- subset(diets, oedema == 1 &amp; diet == &quot;HP&quot;) oed.lp &lt;- subset(diets, oedema == 1 &amp; diet == &quot;LP&quot;) mar.hp &lt;- subset(diets, oedema == 2 &amp; diet == &quot;HP&quot;) mar.lp &lt;- subset(diets, oedema == 2 &amp; diet == &quot;LP&quot;)   Check that each data.frame contains the data that you expect it to:   oed.hp oed.lp mar.hp mar.lp oed.hp ## day oedema diet wt sd n lci uci ## 1 3 1 HP 1.1 11.5 37 -0.7905884 2.99058835 ## 2 6 1 HP 1.0 11.0 37 -0.8083889 2.80838886 ## 3 9 1 HP 1.2 8.4 37 -0.1809515 2.58095149 ## 4 12 1 HP -1.0 7.4 37 -2.2165525 0.21655251 ## 5 15 1 HP -0.9 3.2 37 -1.4260768 -0.37392324 ## 6 18 1 HP -1.7 5.8 37 -2.6535141 -0.74648587 ## 7 21 1 HP -1.7 4.8 37 -2.4891151 -0.91088486 ## 8 24 1 HP -1.7 4.8 37 -2.4891151 -0.91088486 ## 9 27 1 HP -1.7 5.3 37 -2.5713146 -0.82868537 ## 10 30 1 HP -2.5 6.8 37 -3.6179131 -1.38208689 ## 11 33 1 HP -1.3 8.4 37 -2.6809515 0.08095149 oed.lp ## day oedema diet wt sd n lci uci ## 23 3 1 LP -3.0 15.7 65 -4.9473453 -1.0526547 ## 24 6 1 LP -2.5 11.9 65 -3.9760133 -1.0239867 ## 25 9 1 LP -2.0 13.7 65 -3.6992759 -0.3007241 ## 26 12 1 LP 2.0 11.0 65 0.6356179 3.3643821 ## 27 15 1 LP 2.1 13.2 65 0.4627415 3.7372585 ## 28 18 1 LP 1.7 16.2 65 -0.3093627 3.7093627 ## 29 21 1 LP 4.0 11.0 65 2.6356179 5.3643821 ## 30 24 1 LP 6.5 14.7 65 4.6766894 8.3233106 ## 31 27 1 LP 6.4 10.2 65 5.1348457 7.6651543 ## 32 30 1 LP 6.6 8.0 65 5.6077221 7.5922779 ## 33 33 1 LP 6.5 11.0 65 5.1356179 7.8643821 mar.hp ## day oedema diet wt sd n lci uci ## 12 3 2 HP 5.1 15.1 291 4.214822 5.985178 ## 13 6 2 HP 5.1 13.4 291 4.314478 5.885522 ## 14 9 2 HP 4.5 6.5 291 4.118963 4.881037 ## 15 12 2 HP 5.2 9.9 291 4.619652 5.780348 ## 16 15 2 HP 4.6 8.2 291 4.119307 5.080693 ## 17 18 2 HP 5.1 13.4 291 4.314478 5.885522 ## 18 21 2 HP 5.0 13.4 291 4.214478 5.785522 ## 19 24 2 HP 5.2 15.1 291 4.314822 6.085178 ## 20 27 2 HP 5.1 11.6 291 4.419996 5.780004 ## 21 30 2 HP 5.0 9.2 291 4.460686 5.539314 ## 22 33 2 HP 5.2 8.2 291 4.719307 5.680693 mar.lp ## day oedema diet wt sd n lci uci ## 34 3 2 LP 5.1 11.1 86 3.903056 6.296944 ## 35 6 2 LP 6.0 8.2 86 5.115771 6.884229 ## 36 9 2 LP 5.1 8.2 86 4.215771 5.984229 ## 37 12 2 LP 6.5 11.9 86 5.216790 7.783210 ## 38 15 2 LP 6.4 9.1 86 5.418722 7.381278 ## 39 18 2 LP 5.9 13.8 86 4.411908 7.388092 ## 40 21 2 LP 6.1 16.5 86 4.320759 7.879241 ## 41 24 2 LP 4.0 9.1 86 3.018722 4.981278 ## 42 27 2 LP 3.1 12.8 86 1.719741 4.480259 ## 43 30 2 LP 4.0 7.3 86 3.212821 4.787179 ## 44 33 2 LP 5.0 9.1 86 4.018722 5.981278   We can now plot the data for one arm of the trial:   plot(oed.hp$day, oed.hp$wt, type = &quot;l&quot;) We can add error bars using the arrows() function:   arrows(oed.hp$day, oed.hp$lci, oed.hp$day, oed.hp$uci, code = 3, angle = 90, length = 0.1)   The scale of the y axis is wrong because the plot() function automatically scales axes to the ranges of the x and y data it is given. We can fix this by specifying a different set of limits (from lci and uci) for the y axis using the ylim parameter:   plot(oed.hp$day, oed.hp$wt, type = &quot;l&quot;, ylim = c(min(oed.hp$lci), max(oed.hp$uci))) arrows(oed.hp$day, oed.hp$lci, oed.hp$day, oed.hp$uci, code = 3, angle = 90, length = 0.1)   The plot might also be improved by adding plotting symbols:   points(oed.hp$day, oed.hp$wt, pch = 21, bg = &quot;white&quot;) The axis titles (oed.hp$day and oed.hp$wt) are the column (variable) names. We can specify titles and axes labels in the call to the plot() function:   plot(oed.hp$day, oed.hp$wt, type = &quot;l&quot;, ylim = c(min(oed.hp$lci), max(oed.hp$uci)), main = &quot;HP Diet. Oedematous Cases&quot;, xlab = &quot;Days of treatment&quot;, ylab = &quot;Weight gain (g/kg/d)&quot;) arrows(oed.hp$day, oed.hp$lci, oed.hp$day, oed.hp$uci, code = 3, angle = 90, length = 0.1) points(oed.hp$day, oed.hp$wt, pch = 21, bg = &quot;white&quot;)   Now that we know how to plot error bars, we can write a function that we will be able to use whenever we need to plot data with error bars. Before continuing we will consider what the new function should be able to do. This will help us when it comes to writing the function. Our new function should: Take four numeric vectors (x, y, lower CI for y, and upper CI for y) and plot them. Be able to plot the data points as unconnected points or as points joined by lines. Calculate appropriate limits for the y-axis. Produce a plot without axes so that more than one data series may be plotted on the same chart. 5. Provide sensible default values for axis limits and labels. From this list we know that we need the function to take several parameters:   Name Purpose Default value x Data to plot None y Data to plot None y.lci Data to plot None y.uci Data to plot None ylim Limits for y axis c(min(y.lci), max(y.uci)) xlab Label for x axis deparse(substitute(x)) ylab Label for y axis deparse(substitute(y)) main Chart title paste(ylab, \"by\", xlab) type Type of plot \"l\" lty Line type 1 col Line and point colour \"black\" axes Draw x and y axes TRUE pch Type of points to plot 1 bg Fill colour of point white   The parameter names have been chosen to be the same as the parameter names to plot() and points(). This makes the function easier to use. It also makes the function easier to write. Create a new function called plot.ci():   plot.ci &lt;- function() {}   This creates an empty function called plot.ci(). Use the fix() function to edit the plot.ci() function:   fix(plot.ci) Edit the function to read:   function(x, y, y.lci, y.uci, ylim = c(min(y.lci), max(y.uci)), xlab = deparse(substitute(x)), ylab = deparse(substitute(y)), main = paste(ylab, &quot;by&quot;, xlab), type = &quot;l&quot;, lty = 1, col = &quot;black&quot;, axes = TRUE, pch = 21, bg = &quot;white&quot;) { plot(x, y, type = type, ylim = ylim, xlab = xlab, ylab = ylab, main = main, lty = lty, col = col, axes = axes) arrows(x, y.lci, x, y.uci, code = 3, angle = 90, length = 0.1, lty = lty, col = col) points(x, y, pch = pch, bg = bg, col = col) }   Once you have made the changes shown above, check your work, save the file, and quit the editor. Let’s try the plot.ci() function with the test data:   plot.ci(oed.hp$day, oed.hp$wt, oed.hp$lci, oed.hp$uci)   Note how the function has used default values for the axis labels, chart titles, chart limits etc. We can override these default values if we need to:   plot.ci(oed.hp$day, oed.hp$wt, oed.hp$lci, oed.hp$uci, ylim = c(-6, 10), xlab = &quot;Day&quot;, ylab = &quot;Weight gain (g/kg/day)&quot;, main = &quot;Oedematous&quot;, col = &quot;red&quot;)   Do not close the plot window. We should also check that we can plot another data series on this chart:   par(new = TRUE) plot.ci(oed.lp$day, oed.lp$wt, oed.lp$lci, oed.lp$uci, axes = FALSE, pch = 22, xlab = &quot;&quot;, ylab = &quot;&quot;, main = &quot;&quot;, col = &quot;darkgreen&quot;)   We can also add a legend:   legend(5, 8, legend = c(&quot;HP&quot;, &quot;LP&quot;), lty = c(1, 1), pch = c(21, 22), col = c(&quot;red&quot;, &quot;darkgreen&quot;))   We should also check that we can produce plots of unconnected points:   plot.ci(oed.hp$day, oed.hp$wt, oed.hp$lci, oed.hp$uci, type = &quot;p&quot;)   Try plotting the data for the marasmic patients using the plot.ci() function. You might like to use the save() function to save the plot.ci() function. We can use a similar technique to add error bars to different types of plot. If we plot the weight velocities for oedematous patients receiving the high protein diet as a bar chart: barplot(oed.hp$wt, names.arg = oed.hp$day, col = &quot;white&quot;, ylim = c(min(oed.hp$lci), max(oed.hp$uci)))   we could add error bars using the arrows() function as we did with a line plot:   arrows(oed.hp$day, oed.hp$lci, oed.hp$day, oed.hp$uci, code = 3, angle = 90, length = 0.1)   but this does not produce the expected results because the centres of the bars are not placed on the chart at the positions held in oed.hp$day. This is easily fixed as barplot() returns a numeric vector (or matrix, when beside = TRUE) containing the co-ordinates of the bar midpoints: bar.positions &lt;- barplot(oed.hp$wt, names.arg = oed.hp$day, ylim = c(min(oed.hp$lci), max(oed.hp$uci)), col = &quot;white&quot;) bar.positions ## [,1] ## [1,] 0.7 ## [2,] 1.9 ## [3,] 3.1 ## [4,] 4.3 ## [5,] 5.5 ## [6,] 6.7 ## [7,] 7.9 ## [8,] 9.1 ## [9,] 10.3 ## [10,] 11.5 ## [11,] 12.7   We can now use the information stored in bar.positions to specify the positions of the error bars:   arrows(bar.positions, oed.hp$lci, bar.positions, oed.hp$uci, code = 3, angle = 90, length = 0.1)   Armed with this information, we can write a function that we will be able to use whenever we need to plot a bar chart with error bars. Create a new function called barplot.ci():   barplot.ci &lt;- function() {}   This creates an empty function called barplot.ci(). Use the fix() function to edit the barplot.ci() function:   fix(barplot.ci) Edit the function to read:   function(y, bar.names, lci, uci, ylim = c(min(lci), max(uci)), xlab = deparse(substitute(bar.names)), ylab = deparse(substitute(y)), main = paste(ylab, &quot;by&quot;, xlab)) { bp &lt;- barplot(y, names.arg = bar.names, ylim = ylim, xlab = xlab, ylab = ylab, main = main, col = &quot;white&quot;) arrows(bp, lci, bp, uci, code = 3, angle = 90, length = 0.1) }   Once you have made the changes shown above, check your work, save the file, and quit the editor. Let’s try the barplot.ci() function with the test data:   barplot.ci(oed.hp$wt, oed.hp$day, oed.hp$lci, oed.hp$uci) Try plotting the weight velocities for the marasmic patients receiving the high protein diet using the barplot.ci() function:   barplot.ci(mar.hp$wt, mar.hp$day, mar.hp$lci, mar.hp$uci)   The chart looks wrong. This is because we have set the wrong limits for the y axis. The bars are drawn from zero to the data point but we have specified a limit for the y axis that is not constrained to include zero. This is easy to fix. Edit the barplot.ci() function to read:   function(y, bar.names, lci, uci, ylim = c(min(0, lci), max(0, uci)), xlab = deparse(substitute(bar.names)), ylab = deparse(substitute(y)), main = paste(ylab, &quot;by&quot;, xlab)) { bp &lt;- barplot(y, names.arg = bar.names, ylim = ylim, xlab = xlab, ylab = ylab, main = main, col = &quot;white&quot;) arrows(bp, lci, bp, uci, code = 3, angle = 90, length = 0.1) } Once you have made the changes shown above, check your work, save the file, and quit the editor. Try plotting the data for the marasmic patients using the barplot.ci() function:   barplot.ci(mar.hp$wt, mar.hp$day, mar.hp$lci, mar.hp$uci)   Check that the function still operates as expected with the data for oedematous patients:   barplot.ci(oed.hp$wt, oed.hp$day, oed.hp$lci, oed.hp$uci)   The end of one of the error bars touches the x axis. This can also be fixed by slightly widening the limits for the y axis. It might also be useful to plot the centre position of each error bar. We can use the points() function to do this. Edit the barplot.ci() function to read:   function(y, bar.names, lci, uci, ylim = c(min(0, lci), max(0, uci)), xlab = deparse(substitute(bar.names)), ylab = deparse(substitute(y)), main = paste(ylab, &quot;by&quot;, xlab)) { ylim &lt;- ylim * 1.1 bp &lt;- barplot(y, names.arg = bar.names, ylim = ylim, xlab = xlab, ylab = ylab, main = main, col = &quot;white&quot;) arrows(bp, lci, bp, uci, code = 3, angle = 90, length = 0.1) points(bp, y) } Once you have made the changes shown above, check your work, save the file, and quit the editor. Check that the function works as expected:   barplot.ci(oed.hp$wt, oed.hp$day, oed.hp$lci, oed.hp$uci) barplot.ci(mar.hp$wt, mar.hp$day, mar.hp$lci, mar.hp$uci) The barplot.ci() function now works as expected with both sets of data. It is important when developing your own functions, to test them with different data so as to ensure that they work correctly with a wide range if data. You might like to use the save() function to save the barplot.ci() function. 8.4 Mesh map The fact that R provides flexible graphical functions means that, with little extra work, you can use these functions to present your data in appropriate and interesting ways rather than having to rely on a limited set of basic chart types. In this exercise we will use the plot() function to produce a simple mesh-map. The file cover.dat contains data from a coverage survey for a therapeutic feeding program (TFP) in central Malawi undertaken in March 2003. Data were collected using the centric systematic area sampling method to define sampling locations: A number of communities located closest to the centres of thirty 10 x 10 kilometre grid squares were sampled using active (investigative) case-finding. The columns in this dataset are as follows:   x x position of grid square y y position of grid square cases Number of cases found in sampled communities in.program Number of cases (from above) enrolled in the TFP   Retrieve the dataset:   cs &lt;- read.table(&quot;cover.dat&quot;, header = TRUE) Examine the dataset:   cs ## x y cases in.program ## 1 1 7 7 2 ## 2 2 5 4 0 ## 3 2 6 4 1 ## 4 2 7 3 1 ## 5 2 8 3 1 ## 6 2 9 5 1 ## 7 3 3 3 0 ## 8 3 4 2 0 ## 9 3 5 3 0 ## 10 3 6 3 1 ## 11 3 7 5 2 ## 12 3 8 2 0 ## 13 3 9 4 1 ## 14 3 10 5 0 ## 15 4 4 5 2 ## 16 4 5 8 1 ## 17 4 6 6 0 ## 18 4 7 6 1 ## 19 4 8 3 1 ## 20 4 9 5 1 ## 21 4 10 6 2 ## 22 5 4 5 1 ## 23 5 5 3 1 ## 24 5 6 4 0 ## 25 5 7 6 3 ## 26 5 8 4 0 ## 27 6 3 8 2 ## 28 6 4 5 2 ## 29 6 6 6 1 ## 30 6 7 3 1 We should calculate the observed coverage for each grid square:   cs$cvr &lt;- cs$in.program / cs$cases cs ## x y cases in.program cvr ## 1 1 7 7 2 0.2857143 ## 2 2 5 4 0 0.0000000 ## 3 2 6 4 1 0.2500000 ## 4 2 7 3 1 0.3333333 ## 5 2 8 3 1 0.3333333 ## 6 2 9 5 1 0.2000000 ## 7 3 3 3 0 0.0000000 ## 8 3 4 2 0 0.0000000 ## 9 3 5 3 0 0.0000000 ## 10 3 6 3 1 0.3333333 ## 11 3 7 5 2 0.4000000 ## 12 3 8 2 0 0.0000000 ## 13 3 9 4 1 0.2500000 ## 14 3 10 5 0 0.0000000 ## 15 4 4 5 2 0.4000000 ## 16 4 5 8 1 0.1250000 ## 17 4 6 6 0 0.0000000 ## 18 4 7 6 1 0.1666667 ## 19 4 8 3 1 0.3333333 ## 20 4 9 5 1 0.2000000 ## 21 4 10 6 2 0.3333333 ## 22 5 4 5 1 0.2000000 ## 23 5 5 3 1 0.3333333 ## 24 5 6 4 0 0.0000000 ## 25 5 7 6 3 0.5000000 ## 26 5 8 4 0 0.0000000 ## 27 6 3 8 2 0.2500000 ## 28 6 4 5 2 0.4000000 ## 29 6 6 6 1 0.1666667 ## 30 6 7 3 1 0.3333333 Note that some grid squares have zero coverage. It might be useful to use specific plotting characters (e.g. open and filled squares) to indicate zero and non-zero coverage. We can use the ifelse() function to do this:   cs$cvr.pch &lt;- ifelse(cs$cvr == 0, 0, 15) cs ## x y cases in.program cvr cvr.pch ## 1 1 7 7 2 0.2857143 15 ## 2 2 5 4 0 0.0000000 0 ## 3 2 6 4 1 0.2500000 15 ## 4 2 7 3 1 0.3333333 15 ## 5 2 8 3 1 0.3333333 15 ## 6 2 9 5 1 0.2000000 15 ## 7 3 3 3 0 0.0000000 0 ## 8 3 4 2 0 0.0000000 0 ## 9 3 5 3 0 0.0000000 0 ## 10 3 6 3 1 0.3333333 15 ## 11 3 7 5 2 0.4000000 15 ## 12 3 8 2 0 0.0000000 0 ## 13 3 9 4 1 0.2500000 15 ## 14 3 10 5 0 0.0000000 0 ## 15 4 4 5 2 0.4000000 15 ## 16 4 5 8 1 0.1250000 15 ## 17 4 6 6 0 0.0000000 0 ## 18 4 7 6 1 0.1666667 15 ## 19 4 8 3 1 0.3333333 15 ## 20 4 9 5 1 0.2000000 15 ## 21 4 10 6 2 0.3333333 15 ## 22 5 4 5 1 0.2000000 15 ## 23 5 5 3 1 0.3333333 15 ## 24 5 6 4 0 0.0000000 0 ## 25 5 7 6 3 0.5000000 15 ## 26 5 8 4 0 0.0000000 0 ## 27 6 3 8 2 0.2500000 15 ## 28 6 4 5 2 0.4000000 15 ## 29 6 6 6 1 0.1666667 15 ## 30 6 7 3 1 0.3333333 15 A quick way of seeing the code associated with each plotting symbol is:   plot(0:25, 0:25, pch = 0:25, cex = 2)   The size of the plotting symbol may be used to indicate the level of coverage in each quadrat but we must ensure that the symbol used for zero-coverage is not invisibly small:   cs$cvr.cex &lt;- ifelse(cs$cvr == 0, 1, 10 * cs$cvr) cs ## x y cases in.program cvr cvr.pch cvr.cex ## 1 1 7 7 2 0.2857143 15 2.857143 ## 2 2 5 4 0 0.0000000 0 1.000000 ## 3 2 6 4 1 0.2500000 15 2.500000 ## 4 2 7 3 1 0.3333333 15 3.333333 ## 5 2 8 3 1 0.3333333 15 3.333333 ## 6 2 9 5 1 0.2000000 15 2.000000 ## 7 3 3 3 0 0.0000000 0 1.000000 ## 8 3 4 2 0 0.0000000 0 1.000000 ## 9 3 5 3 0 0.0000000 0 1.000000 ## 10 3 6 3 1 0.3333333 15 3.333333 ## 11 3 7 5 2 0.4000000 15 4.000000 ## 12 3 8 2 0 0.0000000 0 1.000000 ## 13 3 9 4 1 0.2500000 15 2.500000 ## 14 3 10 5 0 0.0000000 0 1.000000 ## 15 4 4 5 2 0.4000000 15 4.000000 ## 16 4 5 8 1 0.1250000 15 1.250000 ## 17 4 6 6 0 0.0000000 0 1.000000 ## 18 4 7 6 1 0.1666667 15 1.666667 ## 19 4 8 3 1 0.3333333 15 3.333333 ## 20 4 9 5 1 0.2000000 15 2.000000 ## 21 4 10 6 2 0.3333333 15 3.333333 ## 22 5 4 5 1 0.2000000 15 2.000000 ## 23 5 5 3 1 0.3333333 15 3.333333 ## 24 5 6 4 0 0.0000000 0 1.000000 ## 25 5 7 6 3 0.5000000 15 5.000000 ## 26 5 8 4 0 0.0000000 0 1.000000 ## 27 6 3 8 2 0.2500000 15 2.500000 ## 28 6 4 5 2 0.4000000 15 4.000000 ## 29 6 6 6 1 0.1666667 15 1.666667 ## 30 6 7 3 1 0.3333333 15 3.333333   We can now plot the data:   par(pty=&quot;s&quot;) plot(cs$x, cs$y, cex = cs$cvr.cex, pch = cs$cvr.pch)   There are some problems with this plot: The axes and labels distract from the data. The distance between grid-square centres is wider in the x than in the y direction. The colour (black) of the plotting symbols is too strong. All of these problems can be fixed by specifying values for plot() function parameters:   plot(cs$x, cs$y, cex = cs$cvr.cex, pch = cs$cvr.pch, xlab = &quot;&quot;, ylab = &quot;&quot;, axes = FALSE, xlim = c(0,10), ylim = c(0,10), col = gray(0.5))   An alternative way of plotting this data is to use shades of grey (rather than the size of the plotting symbol) to represent the level of coverage in each grid-square:   plot(cs$x, cs$y, cex = 5, xlab = &quot;&quot;, ylab = &quot;&quot;, axes = FALSE, pch = 15, xlim = c(0, 10), ylim = c(0, 10), col = gray(1 - cs$cvr))   In this context it is useful to show the approximate location of therapeutic feeding centres:   plot(cs$x, cs$y, cex = cs$cvr.cex, pch = cs$cvr.pch, xlab = &quot;&quot;, ylab = &quot;&quot;, axes = FALSE, xlim = c(0,10), ylim = c(0,10), col = gray(0.5)) points(c(2.5, 4.5, 5.5), c(6.5, 8.25, 5), pch = 19, cex = 2)   The techniques introduced in this section allow you to write custom graphical functions but they can also be used to change the default behaviour of standard graphical functions. 8.5 Combining plots In Exercise 1 Getting acquainted with R, we saw how the plot() function could be applied to a fitted object:   fem &lt;- read.table(&quot;fem.dat&quot;, header = TRUE) attach(fem) fem.lm &lt;- lm(WT ~ AGE) plot(fem.lm) fem &lt;- read.table(&quot;fem.dat&quot;, header = TRUE) attach(fem) ## The following objects are masked from fem (pos = 4): ## ## AGE, ANX, DEP, ID, IQ, LIFE, SEX, SLP, WT ## The following objects are masked from fem (pos = 5): ## ## AGE, ANX, DEP, ID, IQ, LIFE, SEX, SLP, WT ## The following objects are masked from fem (pos = 10): ## ## AGE, ANX, DEP, ID, IQ, LIFE, SEX, SLP, WT fem.lm &lt;- lm(WT ~ AGE) plot(fem.lm)   Each of the diagnostic plots are presented as a separate chart. We could use the mfrow parameter of the par() function to present all four diagnostic plots on a single chart:   par(mfrow = c(2, 2)) plot(fem.lm)   It might improve the appearance of the chart if each of the diagnostic plots were square rather than rectangular:   par(mfrow = c(2, 2)) par(pty = &quot;s&quot;) plot(fem.lm)   It might improve the appearance of the chart if smaller text and symbols were used:   par(mfrow = c(2, 2)) par(pty = &quot;s&quot;) par(cex = 0.5) plot(fem.lm)   Graphical parameters set using the par() function affect all subsequent plot commands and must be reset explicitly:   par(mfrow = c(1, 1), pty = &quot;m&quot;, cex = 1) plot(fem.lm) It is possible to save graphical parameters into an R object and use this object to restore original graphical parameters:   old.par &lt;- par() par(mfrow = c(2, 2), pty = &quot;s&quot;) plot(fem.lm) par(old.par) plot(fem.lm) The ability to save and apply graphical parameters means that you can create a library of graphical parameter sets that can be applied with the par() function as required:   default.par &lt;- par() par(mfrow = c(2, 2), pty = &quot;s&quot;) plot.lm.par &lt;- par() par(default.par) plot(fem.lm) par(plot.lm.par) plot(fem.lm) par(default.par) plot(fem.lm)   R produces warning messages when you save and restore graphical parameters in this way. This is because some graphical parameters are read only and cannot be changed using the par() function. This has no effect other than to cause R to issue warning messages. If you do not like the warning messages then you can use the par() function with the no.readonly parameter set to TRUE:   default.par &lt;- par(no.readonly = TRUE) par(mfrow = c(2, 2), pty = &quot;s&quot;) plot.lm.par &lt;- par(no.readonly = TRUE) par(default.par) plot(fem.lm) par(plot.lm.par) plot(fem.lm) par(default.par) plot(fem.lm)   Graphical parameter sets, like any other R object, may be saved and loaded using the save() and load() functions. 8.6 Summary R allows you to create functions that produce graphical output. R graphical functions are flexible so that you can create functions that can produce chart types that are not available in R or many other statistical applications. Standard plots may also be customised using the par() function. R allows you to specify default values for function parameters making functions calls easier by removing the requirement to specify values for every function parameter. "],["exercise9.html", "Exercise 9 Computer intensive methods 9.1 Estimation 9.2 Hypothesis testing 9.3 Simulating processes 9.4 Cellular automata machines 9.5 Summary", " Exercise 9 Computer intensive methods 9.1 Estimation Estimation involves the calculation of a measure with some sense of precision based upon sampling variation. Only a few estimators (e.g. the sample mean from a normal population) have exact formulae that may be used to estimate sampling variation. Typically, estimates of variability are based upon approximations informed by expected or postulated properties of the sampled population. The development of variance formulae for some measures may require in-depth statistical and mathematical knowledge or may even be impossible to derive. Bootstrap methods are computer-intensive methods that can provide estimates and measures of precision (e.g. confidence intervals) without resort to theoretical models, higher mathematics, or assumptions about the sampled population. They rely on repeated sampling, sometimes called resampling, of the observed data. As a simple example of how such methods work, we will start by using bootstrap methods to estimate the mean from a normal population. We will work with a very simple dataset which we will enter directly: x &lt;- c(7.3, 10.4, 14.0, 12.2, 8.4) We can summarise this data quite easily: mean(x) ## [1] 10.46 The sample() function can be used to select a bootstrap replicate: sample(x, length(x), replace = TRUE) ## [1] 12.2 14.0 14.0 8.4 14.0 Enter this command several times to see some more bootstrap replicates. Remember that previous commands can be recalled and edited using the up and down arrow keys – they do not need to be typed out in full each time. The length() parameter is not required for taking bootstrap replicates and can be omitted. It is possible to apply a summary measure to a replicate: mean(sample(x, replace = TRUE)) ## [1] 8.14 Enter this command several times. A bootstrap estimate of the mean of x can be made by repeating this process many times and taking the median of the means for each replicate. One way of doing this is to create a matrix where each column contains a bootstrap replicate and then use the apply() and mean() functions to get at the estimate. First create the matrix of replicates. Here we take ten replicates: x1 &lt;- matrix(sample(x, length(x) * 10, replace = TRUE), nrow = length(x), ncol = 10) x1 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 8.4 14.0 14.0 12.2 8.4 10.4 7.3 12.2 12.2 14.0 ## [2,] 7.3 12.2 14.0 7.3 8.4 10.4 8.4 10.4 12.2 8.4 ## [3,] 8.4 8.4 12.2 12.2 14.0 7.3 14.0 14.0 7.3 8.4 ## [4,] 10.4 8.4 12.2 12.2 10.4 8.4 8.4 10.4 7.3 7.3 ## [5,] 7.3 10.4 14.0 7.3 7.3 8.4 14.0 7.3 14.0 10.4 Then calculate and store the means of each replicate. We can do this using the apply() function to apply the mean() function to the columns of matrix x1: x2 &lt;- apply(x1, 2, mean) x2 ## [1] 8.36 10.68 13.28 10.24 9.70 8.98 10.42 10.86 10.60 9.70 The bootstrap estimate of the mean is: median(x2) ## [1] 10.33 The bootstrap estimate may differ somewhat from the mean of x: mean(x) ## [1] 10.46 The situation is improved by increasing the number of replicates. Here we take 5000 replicates: x1 &lt;- matrix(sample(x, length(x) * 5000, replace = TRUE), nrow = length(x), ncol = 5000) x2 &lt;- apply(x1, 2, mean) median(x2) ## [1] 10.46 This is a pretty useless example as estimating the mean / standard deviation, or standard error of the mean of a sample from a normal population can be done using standard formulae. The utility of bootstrap methods is that they can be applied to summary measures that are not as well understood as the arithmetic mean. The bootstrap method also has the advantage of retaining simplicity even with complicated measures. To illustrate this, we will work through an example of using the bootstrap to estimate the harmonic mean. Again, we will work with a simple dataset which we will enter directly: d &lt;- c(43.64, 50.67, 33.56, 27.75, 43.35, 29.56, 38.83, 35.95, 20.01) The data represents distance (in kilometres) from a point source of environmental pollution for nine female patients with oral / pharyngeal cancer. The harmonic mean is considered to be a sensitive measure of spatial clustering. The first step is to construct a function to calculate the harmonic mean: h.mean &lt;- function(x) {length(x) / sum(1 / x)} Calling this function with the sample data: h.mean(d) ## [1] 33.46646 Should return an estimated harmonic mean distance of 33.47 kilometres. This is simple. The problem is that calculating the variance of this estimate is complicated using standard methods. This problem is relatively simple to solve using bootstrap methods: replicates &lt;- 5000 n &lt;- length(d) x1 &lt;- matrix(sample(d, n * replicates, replace = TRUE), nrow = n, ncol = replicates) x2 &lt;- apply(x1, 2, h.mean) median(x2) ## [1] 33.57148 A 95% confidence interval can be extracted from x2 using the quantile() function: quantile(x2, c(0.025, 0.975)) ## 2.5% 97.5% ## 27.40260 40.49843 A 99% confidence interval can also be extracted from x2 using the quantile() function: quantile(x2, c(0.005, 0.995)) ## 0.5% 99.5% ## 25.98150 42.41896 As a final example of the bootstrap method we will use the method to obtain an estimate of an odds ratio from a two-by-two table. We will work with the salex dataset which we used in exercise 2 and exercise 3: salex &lt;- read.table(&quot;salex.dat&quot;, header = TRUE, na.strings = &quot;9&quot;) table(salex$EGGS, salex$ILL) ## ## 1 2 ## 1 40 6 ## 2 10 20 We should set up our estimator function to calculate an odds ratio from a two-by-two table: or &lt;- function(x) {(x[1,1] / x[1,2]) / (x[2,1] / x[2,2])} We should test this: or(table(salex$EGGS, salex$ILL)) ## [1] 13.33333 The problem is to take a bootstrap replicate from two vectors in a data.frame. This can be achieved by using sample() to create a vector of row indices and then use this sample of indices to select replicates from the data.frame: boot &lt;- NULL for(i in 1:1000) { sampled.rows &lt;- sample(1:nrow(salex), replace = TRUE) x &lt;- salex[sampled.rows, &quot;EGGS&quot;] y &lt;- salex[sampled.rows, &quot;ILL&quot;] boot[i] &lt;- or(table(x, y)) } The vector boot now contains the odds ratios calculated from 1000 replicates. Estimates of the odds ratio and its 95% confidence interval may be obtained using the median() and quantile() functions median(boot) quantile(boot, c(0.025, 0.975)) ## [1] 13.75417 ## 2.5% 97.5% ## 4.525504 52.238095 This approach may fail when some tables have cells that contain zero. Infinite values arise due to division by zero when calculating the odds ratio for some replicates. We can avoid this problem by selecting only those values of boot that are not (!=) infinite (Inf): boot &lt;- boot[boot != Inf] median(boot) quantile(boot, c(0.025, 0.975)) ## [1] 13.66071 ## 2.5% 97.5% ## 4.52166 49.70500 Another way to avoid this problem is to use an adjusted odds ratio calculated by adding 0.5 to each cell of the two-by-two table: boot &lt;- NULL for(i in 1:1000) { sampled.rows &lt;- sample(1:nrow(salex), replace = TRUE) x &lt;- salex[sampled.rows, &quot;EGGS&quot;] y &lt;- salex[sampled.rows, &quot;ILL&quot;] boot[i] &lt;- or(table(x, y) + 0.5) } median(boot) quantile(boot, c(0.025, 0.975)) ## [1] 12.68582 ## 2.5% 97.5% ## 4.309023 45.388176 This procedure is preferred when working with sparse tables. 9.2 Hypothesis testing Computer-intensive methods also offer a general approach to statistical hypothesis testing. To illustrate this we will use computer based simulation to investigate spatial clustering around a point. Before continuing, we will retrieve a dataset: waste &lt;- read.table(&quot;waste.dat&quot;, header = TRUE) The file waste.dat contains the location of twenty-three recent cases of childhood cancer in 5 by 5 km square surrounding an industrial waste disposal site. The columns in the dataset are: x The x location of cases y The y location of cases The x and y variables have been transformed to lie between 0 and 1 with the industrial waste disposal site centrally located (i.e. at x = 0.5, y = 0.5). Plot the data and the location of the industrial waste disposal site on the same chart: plot(waste$x, waste$y, xlim = c(0, 1), ylim = c(0, 1)) points(0.5, 0.5, pch = 3) We can calculate the distance of each point from the industrial waste disposal site using Pythagoras’ Theorem: distance.obs &lt;- sqrt((waste$x - 0.5) ^ 2 + (waste$y - 0.5) ^ 2) The observed mean distance or each case from the industrial waste disposal site is: mean(distance.obs) ## [1] 0.3444118 To test whether this distance is unlikely to have arisen by chance (i.e. evidence of spatial clustering) we need to simulate the distribution of distances when no spatial pattern exists: r &lt;- 10000 x.sim &lt;- matrix(runif(r * 23), 23, r) y.sim &lt;- matrix(runif(r * 23), 23, r) distance.run &lt;- sqrt((x.sim - 0.5)^2 + (y.sim - 0.5)^2) distance.sim &lt;- apply(distance.run, 2, mean) hist(distance.sim, breaks = 20) abline(v = mean(distance.obs), lty = 3) The probability (i.e. the p-value) of observing a mean distance smaller than the observed mean distance under the null hypothesis can be estimated as the number of estimates of the mean distance under the null hypothesis falling below the observed mean divided by the total number of estimates of the mean distance under the null hypothesis: m &lt;- mean(distance.obs) z &lt;- ifelse(distance.sim &lt; m, 1, 0) sum(z) / r ## [1] 0.0991 You might like to repeat this exercise using the harmonic mean distance and the median distance. We can check if this method is capable of detecting a simple cluster using simulated data: x &lt;- rnorm(23, mean = 0.5, sd = 0.2) y &lt;- rnorm(23, mean = 0.5, sd = 0.2) plot(x, y, xlim = c(0, 1), ylim = c(0, 1)) points(0.5, 0.5, pch = 3) We need to recalculate the distance of each simulated case from the industrial waste disposal site: distance.obs &lt;- sqrt((x - 0.5) ^ 2 + (y - 0.5) ^ 2) The observed mean distance of each case from the industrial waste disposal site is: mean(distance.obs) ## [1] 0.2329254 We can use the the simulated null hypothesis data to test for spatial clustering: m &lt;- mean(distance.obs) z &lt;- ifelse(distance.sim &lt; m, 1, 0) sum(z) / r ## [1] 0 We should also check if the procedure can detect a plume of cases, such as might be created by a prevailing wind at a waste incineration site, in a similar way: x &lt;- rnorm(23, 0.25, 0.1) + 0.5 y &lt;- rnorm(23, 0.25, 0.1) + 0.5 plot(x, y, xlim = c(0, 1), ylim = c(0, 1)) points(0.5, 0.5, pch = 3) distance.obs &lt;- sqrt((x - 0.5)^2 + (y - 0.5)^2) m &lt;- mean(distance.obs) z &lt;- ifelse(distance.sim &lt; m, 1, 0) sum(z) / r ## [1] 0.4575 The method is not capable of detecting plumes. You might like to try adapting the simulation code presented here to provide a method capable of detecting plumes of cases. 9.3 Simulating processes In the previous example we simulated the expected distribution of data under the null hypothesis. Computer based simulations are not limited to simulating data. They can also be used to simulate processes. In this example we will simulate the behaviour of the lot quality assurance sampling (LQAS) survey method when sampling constraints lead to a loss of sampling independence. In this example the sampling process is simulated and applied to real-world data. LQAS is a small-sample classification technique that is widely used in manufacturing industry to judge the quality of a batch of manufactured items. In this context, LQAS is used to identify batches that are likely to contain an unacceptably large number of defective items. In the public health context, LQAS may be used to identify communities with unacceptably low levels of service (e.g. vaccine) coverage or worrying levels of disease prevalence. The LQAS method produces data that is easy to analyse. Data analysis is performed as data is collected and consists solely of counting the number of defects (e.g. children with a specific disease) in the sample and checking whether a predetermined threshold value has been exceeded. This combination of data collection and data analysis is called a sampling plan. LQAS sampling plans are developed by specifying: A TRIAGE SYSTEM: A classification system that defines high, moderate, and low categories of the prevalence of the phenomenon of interest. ACCEPTABLE PROBABILITIES OF ERROR: There are two probabilities of error. These are termed provider probability of error (PPE) and consumer probability of error (CPE): Provider Probability of Error (PPE): The risk that the survey will indicate that prevalence is high when it is, in fact, low. PPE is analogous to type I (\\(\\alpha\\)) error in statistical hypothesis testing. Consumer Probability of Error (CPE): The risk that the survey will indicate that prevalence is low when it is, in fact, high. CPE is analogous to type II (\\(\\beta\\)) error in statistical hypothesis testing. Once the upper and lower levels of the triage system and acceptable levels of error have been decided, a set of probability tables are constructed that are used to select a maximum sample size (n) and the number of defects or cases (d) that are allowed in the sample of n subjects before deciding that a population is a high prevalence population. The combination of maximum sample size (n) and number of defects (d) form the stopping rules of the sampling plan. Sampling stops when either the maximum sample size (n) is met or the allowable number of defects (d) is exceeded: If d is exceeded then the population is classified as high prevalence. If n is met without d being exceeded then the population is classified as low prevalence. The values of n and d used in a sampling plan depend upon the threshold values used in the triage system and the acceptable levels of error. The values of n and d used in a sampling plan are calculated using binomial probabilities. For example, the probabilities of finding 14 or fewer cases (\\(d = 14\\)) in a sample of 50 individuals (\\(n = 50\\)) from populations with prevalences of either 20% or 40% are: pbinom(q = 14, size = 50, prob = 0.2) pbinom(q = 14, size = 50, prob = 0.4) ## [1] 0.9392779 ## [1] 0.05395503 The sampling plan with \\(n = 50\\) and \\(d = 14\\) is, therefore, a reasonable candidate for a sampling plan intended to distinguish between populations with prevalences of less than or equal to 20% and populations with prevalences greater than or equal to 40%. There is no middle ground with LQAS sampling plans. Population are always classified as either high or low prevalence. Populations with prevalences between the upper and lower standards of the triage system are classified as high or low prevalence populations. The probability of a moderate prevalence population being classified as high or low prevalence is proportional to the proximity of the prevalence in that population to the triage standards. Moderate prevalence populations close to the upper standard will tend to be classified as high prevalence populations. Moderate prevalence populations close to the lower standard will tend to be classified as low prevalence populations. This behaviour is summarised by the operating characteristic (OC) curve for the sampling plan. For example: plot(seq(0, 0.6, 0.01), pbinom(14, 50, seq(0, 0.6, 0.01), lower.tail = FALSE), main = &quot;OC Curve for n = 50, d = 14&quot;, xlab = &quot;Proportion diseased&quot;, ylab = &quot;Probability&quot;, type = &quot;l&quot;, lty = 2) The data we will use for the simulation is stored in forty-eight separate files. These files contain the returns from whole community screens for active trachoma (TF/TI) in children undertaken as part of trachoma control activities in five African countries. Each file has the file suffix .sim. The name of the file reflects the country in which the data were collected. The data files are: File name Files Origin egyptXX.sim 10 Egypt gambiaXX.sim 10 Gambia ghanaXX.sim 3 Ghana tanzaniaXX.sim 14 Tanzania togoXX.sim 11 Togo All of these data files have the same structure. The variables in the data files are: hh Household identifier sex Sex of child (1=male, 2=female) age Age of child in years tfti Child has active (TF/TI) trachoma (0=no, 1=yes) Each row in these files represents a single child. For example: x &lt;- read.table(&quot;gambia09.sim&quot;, header = TRUE) x[1:10, ] ## hh sex age tfti ## 1 4008001 1 6 1 ## 2 4008001 1 3 1 ## 3 4008002 1 8 1 ## 4 4008002 1 6 0 ## 5 4008003 1 8 0 ## 6 4008003 1 1 0 ## 7 4008004 1 7 0 ## 8 4008004 1 4 0 ## 9 4008004 1 2 0 ## 10 4008004 1 2 0 Any rapid survey method that is appropriate for general use in developing countries is restricted to sampling households rather than individuals. Sampling households in order to sample individuals violates a principal requirement for a sample to be representative of the population from which it is drawn (i.e. that individuals are selected independently of each other). This lack of statistical independence amongst sampled individuals may invalidate standard approaches to selecting sampling plans leading to increased probabilities of error. This is likely to be a particular problem if cases tend to be clustered within households. Trachoma is an infectious disease that confers no lasting immunity in the host. Cases are, therefore, very likely to be clustered within households. One solution to this problem would be to sample (i.e. at random) a single child from each of the sampled households. This is not appropriate for active trachoma as the examination procedure often causes distress to younger children. This may influence survey staff to select older children, who tend to have a lower risk of infection, for examination. Sampling is, therefore, constrained to sampling all children in selected households. The purpose of the simulations presented here is to determine whether the LQAS method is robust to: The loss of sampling independence introduced by sampling households at random and examining all children in selected households for active trachoma. And: The slight increase in the maximum sample size (n) introduced by examining all children in selected households for active trachoma. Each row in the datasets we will be using represents an individual child. Since we will simulate sampling households rather than individual children we need to be able to convert the datasets from one row per child to one row per household. We will write a function to do this. Create a new function called ind2hh(): ind2hh &lt;- function() {} This creates an empty function called ind2hh(). Use the fix() function to edit the ind2hh() function: fix(ind2hh) Edit the function to read: function(data) { n.kids &lt;- n.cases &lt;- NULL id &lt;- unique(data$hh) for(household in id) { temp &lt;- subset(data, data$hh == household) n.kids &lt;- c(n.kids, nrow(temp)) n.cases &lt;- c(n.cases, sum(temp$tfti)) } result &lt;- as.data.frame(cbind(id, n.kids, n.cases)) return(result) } Once you have made the changes shown above, check your work, save the file, and quit the editor. Now we have created the ind2hh() function we should test it for correct operation. We will create a simple test data.frame (test.df) for this purpose: test.df &lt;- as.data.frame(cbind(c(1, 1, 2, 2, 2), c(1, 1, 1, 0 ,0))) names(test.df) &lt;- c(&quot;hh&quot;, &quot;tfti&quot;) test.df ## hh tfti ## 1 1 1 ## 2 1 1 ## 3 2 1 ## 4 2 0 ## 5 2 0 The expected operation of the ind2hh() function given test.df as input is: hh tfti 1 1 1 1 2 1 2 0 2 0 becomes id n.kids n.case 1 2 2 2 3 1 Confirm this behaviour: test.df ind2hh(test.df) ## hh tfti ## 1 1 1 ## 2 1 1 ## 3 2 1 ## 4 2 0 ## 5 2 0 ## id n.kids n.cases ## 1 1 2 2 ## 2 2 3 1 We can apply this function to the datasets as required. For example: x &lt;- read.table(&quot;gambia09.sim&quot;, header = TRUE) x x.hh &lt;- ind2hh(x) x.hh ## hh sex age tfti ## 1 4008001 1 6 1 ## 2 4008001 1 3 1 ## 3 4008002 1 8 1 ## 4 4008002 1 6 0 ## 5 4008003 1 8 0 ## 6 4008003 1 1 0 ## 7 4008004 1 7 0 ## 8 4008004 1 4 0 ## 9 4008004 1 2 0 ## 10 4008004 1 2 0 ## 11 4008004 1 4 0 ## 12 4008005 1 1 0 ## 13 4008006 1 7 0 ## 14 4008006 1 5 0 ## 15 4008007 1 5 0 ## 16 4008007 1 9 0 ## 17 4008007 1 9 0 ## 18 4008008 1 9 1 ## 19 4008008 1 8 0 ## 20 4008008 1 1 0 ## 21 4008009 1 4 1 ## 22 4008009 1 2 0 ## 23 4008010 1 7 1 ## 24 4008010 1 7 0 ## 25 4008010 1 9 0 ## 26 4008010 1 2 0 ## 27 4008011 1 5 0 ## 28 4008011 1 8 0 ## 29 4008011 1 5 0 ## 30 4008012 1 9 1 ## 31 4008013 1 3 1 ## 32 4008013 1 7 1 ## 33 4008013 1 5 1 ## 34 4008013 1 2 0 ## 35 4008013 1 3 0 ## 36 4008013 1 7 0 ## 37 4008014 1 9 1 ## 38 4008014 1 1 1 ## 39 4008014 1 7 1 ## 40 4008015 1 5 1 ## 41 4008015 1 6 1 ## 42 4008015 1 1 1 ## 43 4008015 1 5 1 ## 44 4008015 1 7 0 ## 45 4008015 1 2 0 ## 46 4008015 1 9 0 ## 47 4008016 1 9 1 ## 48 4008016 1 7 1 ## 49 4008016 1 9 1 ## 50 4008016 1 4 1 ## 51 4008016 1 7 1 ## 52 4008016 1 5 1 ## 53 4008016 1 8 1 ## 54 4008017 1 5 1 ## 55 4008017 1 8 0 ## 56 4008018 1 5 0 ## 57 4008018 1 2 0 ## 58 4008019 1 4 1 ## 59 4008019 1 6 0 ## 60 4008020 1 1 0 ## 61 4008020 1 4 1 ## 62 4008020 1 2 0 ## 63 4008021 1 3 0 ## 64 4008021 1 7 0 ## 65 4008021 1 9 1 ## 66 4008021 1 2 0 ## 67 4008022 1 5 0 ## 68 4008022 1 8 0 ## 69 4008022 1 5 0 ## 70 4008023 1 6 1 ## 71 4008023 1 3 1 ## 72 4008024 1 2 1 ## 73 4008024 1 5 1 ## 74 4008024 1 2 0 ## 75 4008024 1 3 0 ## 76 4008024 1 8 0 ## 77 4008025 1 3 0 ## 78 4008025 1 8 0 ## 79 4008026 1 6 1 ## 80 4008026 1 8 0 ## 81 4008026 1 1 0 ## 82 4008026 1 7 0 ## 83 4008027 1 4 0 ## 84 4008027 1 7 0 ## 85 4008027 1 2 0 ## 86 4008027 1 2 0 ## 87 4008027 1 1 0 ## 88 4008028 1 4 0 ## 89 4008028 1 5 0 ## 90 4008028 1 7 0 ## 91 4008029 1 5 0 ## 92 4008029 1 3 0 ## 93 4008029 1 6 0 ## 94 4108001 1 6 1 ## 95 4108001 1 3 1 ## 96 4108002 1 8 1 ## 97 4108002 1 6 0 ## 98 4108003 1 8 0 ## 99 4108003 1 1 0 ## 100 4108004 1 7 0 ## 101 4108004 1 4 0 ## 102 4108004 1 2 0 ## 103 4108004 1 2 0 ## 104 4108004 1 4 0 ## 105 4108005 1 1 0 ## 106 4108006 1 7 0 ## 107 4108006 1 5 0 ## 108 4108007 1 5 0 ## 109 4108007 1 9 0 ## 110 4108007 1 9 0 ## 111 4108008 1 9 1 ## 112 4108008 1 8 0 ## 113 4108008 1 1 0 ## 114 4108009 1 4 1 ## 115 4108009 1 2 0 ## 116 4108010 1 7 1 ## 117 4108010 1 7 0 ## 118 4108010 1 9 0 ## 119 4108010 1 2 0 ## 120 4108011 1 5 0 ## 121 4108011 1 8 0 ## 122 4108011 1 5 0 ## 123 4108012 1 9 1 ## 124 4108013 1 3 1 ## 125 4108013 1 7 1 ## 126 4108013 1 5 1 ## 127 4108013 1 2 0 ## 128 4108013 1 3 0 ## 129 4108013 1 7 0 ## 130 4108014 1 9 1 ## 131 4108014 1 1 1 ## 132 4108014 1 7 1 ## 133 4108015 1 5 1 ## 134 4108015 1 6 1 ## 135 4108015 1 1 1 ## 136 4108015 1 5 1 ## 137 4108015 1 7 0 ## 138 4108015 1 2 0 ## 139 4108015 1 9 0 ## 140 4108016 1 9 1 ## 141 4108016 1 7 1 ## 142 4108016 1 9 1 ## 143 4108016 1 4 1 ## 144 4108016 1 7 1 ## 145 4108016 1 5 1 ## 146 4108016 1 8 1 ## 147 4108017 1 5 1 ## 148 4108017 1 8 0 ## 149 4108018 1 5 0 ## 150 4108018 1 2 0 ## 151 4108019 1 4 1 ## 152 4108019 1 6 0 ## 153 4108020 1 1 0 ## 154 4108020 1 4 1 ## 155 4108020 1 2 0 ## 156 4108021 1 3 0 ## 157 4108021 1 7 0 ## 158 4108021 1 9 1 ## 159 4108021 1 2 0 ## 160 4108022 1 5 0 ## 161 4108022 1 8 0 ## 162 4108022 1 5 0 ## 163 4108023 1 6 1 ## 164 4108023 1 3 1 ## 165 4108024 1 2 1 ## 166 4108024 1 5 1 ## 167 4108024 1 2 0 ## 168 4108024 1 3 0 ## 169 4108024 1 8 0 ## 170 4108025 1 3 0 ## 171 4108025 1 8 0 ## 172 4108026 1 6 1 ## 173 4108026 1 8 0 ## 174 4108026 1 1 0 ## 175 4108026 1 7 0 ## 176 4108027 1 4 0 ## 177 4108027 1 7 0 ## 178 4108027 1 2 0 ## 179 4108027 1 2 0 ## 180 4108027 1 1 0 ## 181 4108028 1 4 0 ## 182 4108028 1 5 0 ## 183 4108028 1 7 0 ## 184 4108029 1 5 0 ## 185 4108029 1 3 0 ## 186 4108029 1 6 0 ## 187 4208001 1 6 1 ## 188 4208001 1 3 1 ## 189 4208002 1 8 1 ## 190 4208002 1 6 0 ## 191 4208003 1 8 0 ## 192 4208003 1 1 0 ## 193 4208004 1 7 0 ## 194 4208004 1 4 0 ## 195 4008004 1 2 0 ## 196 4208004 1 2 0 ## 197 4208004 1 4 0 ## 198 4208005 1 1 0 ## 199 4208006 1 7 0 ## 200 4208006 1 5 0 ## 201 4208007 1 5 0 ## 202 4208007 1 9 0 ## 203 4208007 1 9 0 ## 204 4208008 1 9 1 ## 205 4208008 1 8 0 ## 206 4208008 1 1 0 ## 207 4208009 1 4 1 ## 208 4208009 1 2 0 ## 209 4208010 1 7 1 ## 210 4208010 1 7 0 ## 211 4208010 1 9 0 ## 212 4208010 1 2 0 ## 213 4208011 1 5 0 ## 214 4208011 1 8 0 ## 215 4208011 1 5 0 ## 216 4208012 1 9 1 ## 217 4208013 1 3 1 ## 218 4208013 1 7 1 ## 219 4208013 1 5 1 ## 220 4208013 1 2 0 ## 221 4208013 1 3 0 ## 222 4208013 1 7 0 ## 223 4208014 1 9 1 ## 224 4208014 1 1 1 ## 225 4208014 1 7 1 ## 226 4208015 1 5 1 ## 227 4208015 1 6 1 ## 228 4208015 1 1 1 ## 229 4208015 1 5 1 ## 230 4208015 1 7 0 ## 231 4208015 1 2 0 ## 232 4208015 1 9 0 ## 233 4208016 1 9 1 ## 234 4208016 1 7 1 ## 235 4208016 1 9 1 ## 236 4208016 1 4 1 ## 237 4208016 1 7 1 ## 238 4208016 1 5 1 ## 239 4208016 1 8 1 ## 240 4208017 1 5 1 ## 241 4208017 1 8 0 ## 242 4208018 1 5 0 ## 243 4208018 1 2 0 ## 244 4208019 1 4 1 ## 245 4208019 1 6 0 ## 246 4208020 1 1 0 ## 247 4208020 1 4 1 ## 248 4208020 1 2 0 ## 249 4208021 1 3 0 ## 250 4208021 1 7 0 ## 251 4208021 1 9 1 ## 252 4208021 1 2 0 ## 253 4208022 1 5 0 ## 254 4208022 1 8 0 ## 255 4208022 1 5 0 ## 256 4208023 1 6 1 ## 257 4208023 1 3 1 ## 258 4208024 1 2 1 ## 259 4208024 1 5 1 ## 260 4208024 1 2 0 ## 261 4208024 1 3 0 ## 262 4208024 1 8 0 ## 263 4208025 1 3 0 ## 264 4208025 1 8 0 ## 265 4208026 1 6 1 ## 266 4208026 1 8 0 ## 267 4208026 1 1 0 ## 268 4208026 1 7 0 ## 269 4208027 1 4 0 ## 270 4208027 1 7 0 ## 271 4208027 1 2 0 ## 272 4208027 1 2 0 ## 273 4208027 1 1 0 ## 274 4208028 1 4 0 ## 275 4208028 1 5 0 ## 276 4208028 1 7 0 ## 277 4208029 1 5 0 ## 278 4208029 1 3 0 ## 279 4208029 1 6 0 ## id n.kids n.cases ## 1 4008001 2 2 ## 2 4008002 2 1 ## 3 4008003 2 0 ## 4 4008004 6 0 ## 5 4008005 1 0 ## 6 4008006 2 0 ## 7 4008007 3 0 ## 8 4008008 3 1 ## 9 4008009 2 1 ## 10 4008010 4 1 ## 11 4008011 3 0 ## 12 4008012 1 1 ## 13 4008013 6 3 ## 14 4008014 3 3 ## 15 4008015 7 4 ## 16 4008016 7 7 ## 17 4008017 2 1 ## 18 4008018 2 0 ## 19 4008019 2 1 ## 20 4008020 3 1 ## 21 4008021 4 1 ## 22 4008022 3 0 ## 23 4008023 2 2 ## 24 4008024 5 2 ## 25 4008025 2 0 ## 26 4008026 4 1 ## 27 4008027 5 0 ## 28 4008028 3 0 ## 29 4008029 3 0 ## 30 4108001 2 2 ## 31 4108002 2 1 ## 32 4108003 2 0 ## 33 4108004 5 0 ## 34 4108005 1 0 ## 35 4108006 2 0 ## 36 4108007 3 0 ## 37 4108008 3 1 ## 38 4108009 2 1 ## 39 4108010 4 1 ## 40 4108011 3 0 ## 41 4108012 1 1 ## 42 4108013 6 3 ## 43 4108014 3 3 ## 44 4108015 7 4 ## 45 4108016 7 7 ## 46 4108017 2 1 ## 47 4108018 2 0 ## 48 4108019 2 1 ## 49 4108020 3 1 ## 50 4108021 4 1 ## 51 4108022 3 0 ## 52 4108023 2 2 ## 53 4108024 5 2 ## 54 4108025 2 0 ## 55 4108026 4 1 ## 56 4108027 5 0 ## 57 4108028 3 0 ## 58 4108029 3 0 ## 59 4208001 2 2 ## 60 4208002 2 1 ## 61 4208003 2 0 ## 62 4208004 4 0 ## 63 4208005 1 0 ## 64 4208006 2 0 ## 65 4208007 3 0 ## 66 4208008 3 1 ## 67 4208009 2 1 ## 68 4208010 4 1 ## 69 4208011 3 0 ## 70 4208012 1 1 ## 71 4208013 6 3 ## 72 4208014 3 3 ## 73 4208015 7 4 ## 74 4208016 7 7 ## 75 4208017 2 1 ## 76 4208018 2 0 ## 77 4208019 2 1 ## 78 4208020 3 1 ## 79 4208021 4 1 ## 80 4208022 3 0 ## 81 4208023 2 2 ## 82 4208024 5 2 ## 83 4208025 2 0 ## 84 4208026 4 1 ## 85 4208027 5 0 ## 86 4208028 3 0 ## 87 4208029 3 0 We will now write a function that will simulate a single LQAS survey. Create a new function called lqas.run(): lqas.run &lt;- function() {} This creates an empty function called lqas.run(). Use the fix() function to edit the lqas.run() function: fix(lqas.run) Edit the function to read: Once you have made the changes shown above, check your work, save the file, and quit the editor. We should try this function on a low, a moderate, and a high prevalence dataset. To select suitable test datasets we need to know the prevalence in each dataset: for(i in dir(pattern = &quot;\\\\.sim$&quot;)) { data &lt;- read.table(i, header = TRUE) cat(i, &quot;:&quot;, mean(data$tfti), &quot;\\n&quot;) } ## egypt01.sim : 0.5913978 ## egypt02.sim : 0.09243697 ## egypt03.sim : 0.343949 ## egypt04.sim : 0.2546584 ## egypt05.sim : 0.1354839 ## egypt06.sim : 0.6993464 ## egypt07.sim : 0.09815951 ## egypt08.sim : 0.5668449 ## egypt09.sim : 0.5251799 ## egypt10.sim : 0.4343434 ## gambia01.sim : 0.08510638 ## gambia02.sim : 0.2517483 ## gambia03.sim : 0.1730769 ## gambia04.sim : 0.1866667 ## gambia05.sim : 0.1264368 ## gambia06.sim : 0.5180952 ## gambia07.sim : 0.3876652 ## gambia08.sim : 0.310559 ## gambia09.sim : 0.3548387 ## gambia10.sim : 0.3358491 ## ghana01.sim : 0.0990099 ## ghana02.sim : 0.2162162 ## ghana03.sim : 0.2781955 ## tanzania01.sim : 0.4672897 ## tanzania02.sim : 0.4563107 ## tanzania03.sim : 0.1025641 ## tanzania04.sim : 0.1724138 ## tanzania05.sim : 0.6962963 ## tanzania06.sim : 0.3092784 ## tanzania07.sim : 0.3727273 ## tanzania08.sim : 0.4454545 ## tanzania09.sim : 0.2146893 ## tanzania10.sim : 0.5925926 ## tanzania11.sim : 0.2544379 ## tanzania12.sim : 0.5619835 ## tanzania13.sim : 0.5086207 ## tanzania14.sim : 0.4105263 ## togo01.sim : 0.254902 ## togo02.sim : 0.1692308 ## togo03.sim : 0.2147651 ## togo04.sim : 0.1265823 ## togo05.sim : 0.1098266 ## togo06.sim : 0.2677165 ## togo07.sim : 0.3244681 ## togo08.sim : 0.3125 ## togo09.sim : 0.125 ## togo10.sim : 0.04385965 ## togo11.sim : 0.08609272 The coding scheme used for the tfti variable (0=no, 1=yes) allows us to use the mean() function to calculate prevalence in these datasets. The pattern \"\\\\.sim$\" is a regular expression for files ending in .sim. If you want to use the dir() function to list files stored outside of the current working directory you will need to specify an appropriate value for the path parameter. On some systems you may also need to set the value of the full.names parameter to TRUE. For example: for(i in dir(path = &quot;~/prfe&quot;, pattern = &quot;\\\\.sim$&quot;, full.names = TRUE)) { data &lt;- read.table(i, header = TRUE) cat(i, &quot;:&quot;, mean(data$tfti), &quot;\\n&quot;) } cycles through all files ending in .sim (specified by giving the value \"\\\\.sim$\" to the pattern parameter) that are stored the prfe directory under the users home directory (specified by giving the value \"~/prfe\" to the path parameter) on UNIX systems. You cannot usefully specify a URL for the path parameter of the dir() function. We will use tanzania04.sim as an example of a low prevalence dataset: x &lt;- read.table(&quot;tanzania04.sim&quot;, header = TRUE) x.hh &lt;- ind2hh(x) lqas.run(x = x.hh, n = 50, d = 14) ## $kids ## [1] 50 ## ## $cases ## [1] 9 ## ## $outcome ## [1] 0 Repeat the last function call several times. Remember that previous commands can be recalled and edited using the up and down arrow keys – they do not need to be typed out in full each time. The function should, for most calls, return: $outcome [1] 0 We will use tanzania08.sim as an example of a high prevalence dataset: x &lt;- read.table(&quot;tanzania08.sim&quot;, header = TRUE) x.hh &lt;- ind2hh(x) lqas.run(x = x.hh, n = 50, d = 14) ## $kids ## [1] 39 ## ## $cases ## [1] 16 ## ## $outcome ## [1] 1 Repeat the last function call several times. The function should, for most calls, return: $outcome [1] 1 We will use tanzania06.sim as an example of a moderate prevalence dataset: x &lt;- read.table(&quot;tanzania06.sim&quot;, header = TRUE) x.hh &lt;- ind2hh(x) lqas.run(x = x.hh, n = 50, d = 14) ## $kids ## [1] 50 ## ## $cases ## [1] 14 ## ## $outcome ## [1] 0 Repeat the last function call several times. The function should return: $outcome [1] 0 And: $outcome [1] 1 In roughly equal proportion. The simulation will require repeated sampling from the same dataset. We need to write a function to do this. Create a new function called lqas.simul(): lqas.simul &lt;- function() {} This creates an empty function called lqas.simul(). Use the fix() function to edit the lqas.simul() function: fix(lqas.simul) Edit the function to read: function(x, n, d, runs) { all &lt;- data.frame() for(i in 1:runs) { run &lt;- data.frame(lqas.run(x, n ,d)) all &lt;- rbind(all, run) } p &lt;- sum(x$n.cases) / sum(x$n.kids) asn &lt;- mean(all$kids) p.high &lt;- mean(all$outcome) result &lt;- list(p = p, asn = asn, p.high = p.high) return(result) } Once you have made the changes shown above, check your work, save the file, and quit the editor. We can test this function with the same three test datasets: x &lt;- read.table(&quot;tanzania04.sim&quot;, header = TRUE) x.hh &lt;- ind2hh(x) lqas.simul(x = x.hh, n = 50, d = 14, runs = 250) x &lt;- read.table(&quot;tanzania08.sim&quot;, header = TRUE) x.hh &lt;- ind2hh(x) lqas.simul(x = x.hh, n = 50, d = 14, runs = 250) x &lt;- read.table(&quot;tanzania06.sim&quot;, header = TRUE) x.hh &lt;- ind2hh(x) lqas.simul( x = x.hh, n = 50, d = 14, runs = 250) ## $p ## [1] 0.1724138 ## ## $asn ## [1] 50.56 ## ## $p.high ## [1] 0.024 ## $p ## [1] 0.4454545 ## ## $asn ## [1] 34.028 ## ## $p.high ## [1] 0.98 ## $p ## [1] 0.3092784 ## ## $asn ## [1] 46.152 ## ## $p.high ## [1] 0.596 The simulation consists of applying this function to each of the datasets in turn and collating the results. We will create a function to do this. Create a new function called main.simul(): main.simul &lt;- function() {} This creates an empty function called main.simul(). Use the fix() function to edit the main.simul() function: fix(main.simul) Edit the function to read: function(n, d, runs) { result &lt;- data.frame() for(i in dir(pattern = &quot;\\\\.sim$&quot;)) { cat(&quot;.&quot;, sep = &quot;&quot;) x &lt;- read.table(i, header = TRUE) y &lt;- ind2hh(x) z &lt;- lqas.simul(y, n, d, runs) z$file.name &lt;- i result &lt;- rbind(result, as.data.frame(z)) } return(result) } Once you have made the changes shown above, check your work, save the file, and quit the editor. We are now ready to run the simulation: z1 &lt;- main.simul(n = 50, d = 14, runs = 250) ## ................................................ Progress of the simulation is shown by a lengthening line of dots. Each dot represents one community screening file processed. The &gt; prompt will be displayed when the simulation has finished running. The returned data.frame object (z1) contains the results of the simulation: z1 ## p asn p.high file.name ## 1 0.59139785 26.292 1.000 egypt01.sim ## 2 0.09243697 50.820 0.000 egypt02.sim ## 3 0.34394904 43.140 0.812 egypt03.sim ## 4 0.25465839 48.928 0.304 egypt04.sim ## 5 0.13548387 50.652 0.004 egypt05.sim ## 6 0.69934641 22.676 1.000 egypt06.sim ## 7 0.09815951 50.744 0.000 egypt07.sim ## 8 0.56684492 27.076 1.000 egypt08.sim ## 9 0.52517986 29.004 1.000 egypt09.sim ## 10 0.43434343 35.828 0.984 egypt10.sim ## 11 0.08510638 51.552 0.004 gambia01.sim ## 12 0.25174825 49.688 0.380 gambia02.sim ## 13 0.17307692 52.060 0.056 gambia03.sim ## 14 0.18666667 51.496 0.088 gambia04.sim ## 15 0.12643678 52.436 0.024 gambia05.sim ## 16 0.51809524 32.092 1.000 gambia06.sim ## 17 0.38766520 41.628 0.916 gambia07.sim ## 18 0.31055901 46.848 0.588 gambia08.sim ## 19 0.35483871 42.036 0.776 gambia09.sim ## 20 0.33584906 44.888 0.644 gambia10.sim ## 21 0.09900990 52.272 0.000 ghana01.sim ## 22 0.21621622 51.296 0.180 ghana02.sim ## 23 0.27819549 47.968 0.508 ghana03.sim ## 24 0.46728972 33.204 0.988 tanzania01.sim ## 25 0.45631068 33.736 1.000 tanzania02.sim ## 26 0.10256410 50.672 0.000 tanzania03.sim ## 27 0.17241379 50.752 0.044 tanzania04.sim ## 28 0.69629630 22.484 1.000 tanzania05.sim ## 29 0.30927835 45.748 0.568 tanzania06.sim ## 30 0.37272727 40.628 0.920 tanzania07.sim ## 31 0.44545455 33.744 0.988 tanzania08.sim ## 32 0.21468927 50.220 0.156 tanzania09.sim ## 33 0.59259259 25.940 1.000 tanzania10.sim ## 34 0.25443787 48.132 0.360 tanzania11.sim ## 35 0.56198347 29.108 1.000 tanzania12.sim ## 36 0.50862069 30.744 0.996 tanzania13.sim ## 37 0.41052632 37.100 0.968 tanzania14.sim ## 38 0.25490196 49.388 0.360 togo01.sim ## 39 0.16923077 55.264 0.044 togo02.sim ## 40 0.21476510 51.148 0.176 togo03.sim ## 41 0.12658228 51.992 0.012 togo04.sim ## 42 0.10982659 53.100 0.000 togo05.sim ## 43 0.26771654 47.532 0.460 togo06.sim ## 44 0.32446809 45.224 0.708 togo07.sim ## 45 0.31250000 45.244 0.676 togo08.sim ## 46 0.12500000 51.532 0.004 togo09.sim ## 47 0.04385965 52.512 0.000 togo10.sim ## 48 0.08609272 52.188 0.000 togo11.sim We can examine the prevalences in the test datasets: x11() hist(z1$p, main = &quot;Prevalence in test datasets&quot;, xlab = &quot;Proportion TF/TI&quot;) If you are using a Macintosh computer then you can use quartz() instead of x11(). This will give better results. We examine the sample size required to make classifications at different levels of prevalence as anaverage sample number (ASN) curve: x11() plot(z1$p, z1$asn, main = &quot;ASN Curve&quot;, xlab = &quot;Proportion TF/TI&quot;, ylab = &quot;Sample size required&quot;) We can examine the performance of the sampling plan by plotting its operating characteristic (OC) curve: x11() plot(z1$p, z1$p.high, main = &quot;OC Curve&quot;, xlab = &quot;Proportion TF/TI&quot;, ylab = &quot;Probability&quot;) Before closing the OC curve plot, we can compare the simulation results with the expected operating characteristic (OC) curve under ideal sampling conditions: lines(seq(0, max(z1$p), 0.01), pbinom(14, 50,seq(0, max(z1$p),0.01), lower.tail = FALSE), lty = 3) The LQAS method appears to be robust to the loss of sampling variation introduced by the proposed sampling constraints. There is, however, some deviation from the expected operating characteristic (OC) curve at lower prevalences: x11() plot(z1$p, z1$p.high, xlim = c(0.10, 0.35), main = &quot;OC Curve&quot;, xlab = &quot;Proportion TF/TI&quot;, ylab = &quot;Probability&quot;) lines(seq(0.10, 0.35, 0.01), pbinom(14, 50, seq(0.10, 0.35, 0.01), lower.tail = FALSE), lty = 3) This deviation from the expected operating characteristic (OC) curve is likely to be due to a few very large households in which many of the children have active trachoma. You can check this by examining the p.high (i.e. the probability of a classification as high prevalence) column in z1, and household size and trachoma status in the individual data files that return higher than expected values for p.high. The ind2hh() function is likely to prove useful in this context. The observed deviation from the expected operating characteristic (OC) curve is small but it is important, in the resource-constrained context of trachoma-endemic countries, to minimise the false positive rate in order to ensure that resources are devoted to communities that need them most. We might be able to improve the performance of the survey method in this regard by restricting the sample so that only younger children are examined. This will have the effect of reducing the number of children examined in each household. It also has the benefit of making the surveys simpler and quicker since younger children will tend to be closer to home than older children. The range of ages in the datasets can be found using: for(i in dir(pattern = &quot;\\\\.sim$&quot;)) { x &lt;- read.table(i, header = TRUE) cat(i, &quot;:&quot;, range(x$age), &quot;\\n&quot;) } ## egypt01.sim : 1 10 ## egypt02.sim : 2 8 ## egypt03.sim : 2 10 ## egypt04.sim : 1 10 ## egypt05.sim : 2 6 ## egypt06.sim : 2 10 ## egypt07.sim : 2 6 ## egypt08.sim : 2 10 ## egypt09.sim : 2 10 ## egypt10.sim : 2 10 ## gambia01.sim : 1 9 ## gambia02.sim : 1 9 ## gambia03.sim : 1 9 ## gambia04.sim : 1 9 ## gambia05.sim : 1 9 ## gambia06.sim : 1 9 ## gambia07.sim : 1 9 ## gambia08.sim : 1 9 ## gambia09.sim : 1 9 ## gambia10.sim : 1 9 ## ghana01.sim : 1 10 ## ghana02.sim : 1 10 ## ghana03.sim : 1 10 ## tanzania01.sim : 1 10 ## tanzania02.sim : 1 10 ## tanzania03.sim : 1 10 ## tanzania04.sim : 1 10 ## tanzania05.sim : 1 10 ## tanzania06.sim : 1 10 ## tanzania07.sim : 1 10 ## tanzania08.sim : 1 10 ## tanzania09.sim : 1 10 ## tanzania10.sim : 1 10 ## tanzania11.sim : 1 10 ## tanzania12.sim : 1 10 ## tanzania13.sim : 1 10 ## tanzania14.sim : 1 10 ## togo01.sim : 1 10 ## togo02.sim : 1 10 ## togo03.sim : 1 10 ## togo04.sim : 1 10 ## togo05.sim : 1 10 ## togo06.sim : 1 10 ## togo07.sim : 1 10 ## togo08.sim : 1 10 ## togo09.sim : 1 10 ## togo10.sim : 1 10 ## togo11.sim : 1 10 We will investigate the effect of restricting the sample to children aged between two and five years. Use the fix() function to edit the main.simul() function: fix(main.simul) Edit the function to read: function(n, d, runs) { result &lt;- data.frame() for(i in dir(pattern = &quot;\\\\.sim$&quot;)) { cat(&quot;.&quot;, sep = &quot;&quot;) x &lt;- read.table(i, header = TRUE) x &lt;- subset(x, age &gt;= 2 &amp; age &lt;= 5) y &lt;- ind2hh(x) z &lt;- lqas.simul(y, n, d, runs) z$file.name &lt;- i result &lt;- rbind(result, as.data.frame(z)) } return(result) } Once you have made the changes shown above, check your work, save the file, and quit the editor. We are now ready to run the simulation again: z2 &lt;- main.simul(n = 50, d = 14, runs = 250) ## ................................................ The data.frame object z2 contains the results of the simulation: z2 ## p asn p.high file.name ## 1 0.59139785 25.764 1.000 egypt01.sim ## 2 0.09243697 50.792 0.000 egypt02.sim ## 3 0.34394904 43.004 0.780 egypt03.sim ## 4 0.25465839 49.304 0.340 egypt04.sim ## 5 0.13548387 50.632 0.008 egypt05.sim ## 6 0.69934641 22.468 1.000 egypt06.sim ## 7 0.09815951 50.716 0.000 egypt07.sim ## 8 0.56684492 27.704 1.000 egypt08.sim ## 9 0.52517986 29.008 1.000 egypt09.sim ## 10 0.43434343 36.136 0.984 egypt10.sim ## 11 0.08510638 51.568 0.000 gambia01.sim ## 12 0.25174825 49.724 0.292 gambia02.sim ## 13 0.17307692 52.284 0.040 gambia03.sim ## 14 0.18666667 51.404 0.076 gambia04.sim ## 15 0.12643678 52.180 0.012 gambia05.sim ## 16 0.51809524 32.124 1.000 gambia06.sim ## 17 0.38766520 40.892 0.912 gambia07.sim ## 18 0.31055901 46.236 0.672 gambia08.sim ## 19 0.35483871 43.208 0.768 gambia09.sim ## 20 0.33584906 45.212 0.728 gambia10.sim ## 21 0.09900990 52.240 0.000 ghana01.sim ## 22 0.21621622 51.604 0.188 ghana02.sim ## 23 0.27819549 49.212 0.516 ghana03.sim ## 24 0.46728972 33.060 0.988 tanzania01.sim ## 25 0.45631068 34.204 0.992 tanzania02.sim ## 26 0.10256410 50.808 0.000 tanzania03.sim ## 27 0.17241379 50.680 0.040 tanzania04.sim ## 28 0.69629630 22.832 1.000 tanzania05.sim ## 29 0.30927835 45.348 0.624 tanzania06.sim ## 30 0.37272727 40.548 0.888 tanzania07.sim ## 31 0.44545455 34.628 0.980 tanzania08.sim ## 32 0.21468927 50.352 0.156 tanzania09.sim ## 33 0.59259259 25.788 1.000 tanzania10.sim ## 34 0.25443787 49.104 0.364 tanzania11.sim ## 35 0.56198347 29.272 1.000 tanzania12.sim ## 36 0.50862069 29.980 1.000 tanzania13.sim ## 37 0.41052632 37.576 0.944 tanzania14.sim ## 38 0.25490196 49.096 0.400 togo01.sim ## 39 0.16923077 55.532 0.096 togo02.sim ## 40 0.21476510 50.824 0.188 togo03.sim ## 41 0.12658228 52.192 0.012 togo04.sim ## 42 0.10982659 52.584 0.000 togo05.sim ## 43 0.26771654 47.504 0.452 togo06.sim ## 44 0.32446809 45.092 0.692 togo07.sim ## 45 0.31250000 45.548 0.612 togo08.sim ## 46 0.12500000 51.304 0.000 togo09.sim ## 47 0.04385965 51.968 0.000 togo10.sim ## 48 0.08609272 52.208 0.000 togo11.sim We can examine the performance of the sampling plan on the age-restricted datasets by plotting its operating characteristic (OC) curve and comparing the simulation results with the expected operating characteristic (OC) curve under ideal sampling conditions: x11() plot(z2$p, z2$p.high, main = &quot;OC Curve&quot;, xlab = &quot;Proportion TF/TI&quot;, ylab = &quot;Probability &quot;) lines(seq(0, max(z2$p), 0.01), pbinom(14, 50, seq(0, max(z2$p), 0.01), lower.tail = FALSE), lty = 3) Remember that if you are using a Macintosh computer then you can use quartz() instead of x11(). This will give better results. We should also take a closer look at the range of prevalences where the deviation from the expected operating characteristic (OC) curve was largest and most problematic in the previous simulation: x11() plot(z2$p, z2$p.high, xlim = c(0.10, 0.35), main = &quot;OC Curve&quot;, xlab = &quot;Proportion TF/TI&quot;, ylab = &quot;Probability&quot;) lines(seq(0.10, 0.35, 0.01), pbinom(14, 50, seq(0.10, 0.35, 0.01), lower.tail = FALSE), lty = 3) We can compare the behaviour of the sampling plan in the unrestricted and age-restricted datasets: x11() plot(z1$p, z1$p.high, main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, axes = FALSE, xlim = c(0, 0.8)) par(new = TRUE) plot(z2$p, z2$p.high, main = &quot;OC Curve&quot;, xlab = &quot;Proportion TF/TI&quot;, ylab = &quot;Probability&quot;, xlim = c(0, 0.8), pch = 3) lines(seq(0, 0.8, 0.01), pbinom(14, 50, seq(0, 0.8, 0.01), lower.tail = FALSE), lty = 3) Restricting the sample to children aged between two and five years (inclusive) appears to have improved the behaviour of the survey method by lowering the false positive rate to close to expected behaviour under ideal sampling condition. Process simulation has allowed us to improve the performance of the survey method without expensive and lengthy field-work. In practice, the method would now be validated in the field probably by repeated sampling of communities in which prevalence is known from house-to-house screening. 9.4 Cellular automata machines Cellular automata machines are simple computing devices that are commonly used to simulate social, biological, and physical processes. Despite their simplicity, cellular automata machines are general purpose computing devices. This means that they may be used for any computable problem. The way that problems are specified to cellular automata machines make them simple to program for some types of problem and difficult to program for other types of problem. In this exercise we will explore the use of cellular automata machines to create a simple model of epidemic spread. Cellular automata machines model a universe in which space is represented by a uniform grid, time advances in steps, and the laws of the universe are represented by a set of rules which compute the future state of each cell of the grid from its current state and from the current state of its neighbouring cells. Typically, a cellular automata machine has the following features: It consists of a large number of identical cells arranged in a regular grid. The grid is a two-dimensional projection of a torus (a ring-doughnut shaped surface) and has no edges. Each cell can be in one of a limited number of states. Time advances through the simulation in steps. At each time-step, the state of a cell may change. The state of a cell after each time-step is determined by a set of rules that define how the future state of a cell depends on the current state of the cell and the current state of its immediate neighbours. This set of rules is used to update the state of every cell in the grid at each time-step. Since the rules refer only to the state of an individual cell and its immediate neighbours, cellular automata machines are best suited to modelling situations where local interactions give rise to global phenomena. In this exercise we will simulate a cellular automata machine using R and then use the simulated machine to simulate epidemic spread. We will use three functions to simulate the cellular automata machine (CAM): cam.run(): This function will display the initial state of the CAM, examine each cell in the CAM grid, apply the CAM rule-set to each cell, update the CAM grid, and display the state of the CAM at each time-step. cam.state.display(): This function will display the state of the CAM at each time-step. It will be implemented using the image() function to plot the contents of matrices held in the cam.state list object (see below). This function will be developed as we refine the epidemic model. cam.rule(): This function will contain the rule-set. It will be implemented using ifelse() functions to codify rules. This function will also be developed as we refine the epidemic model. The current and future states of the CAM will be held in two global list objects: cam.state: This object will contain the current state of the CAM and must contain a matrix object called grid. cam.state.new: This object will contain the the state of the CAM at the next time-step as defined by the the rule-set. Other global objects will be defined as required. Create a new function called cam.run(): cam.run &lt;- function() {} This creates an empty function called cam.run(). Use the fix() function to edit the cam.run() function: fix(cam.run) Edit the function to read: function(steps) { cam.state.new &lt;&lt;- cam.state cam.state.display(t = 0) mx &lt;- nrow(cam.state$grid) my &lt;- ncol(cam.state$grid) for(t in 1:steps) { for(y in 1:my) { for(x in 1:mx) { V &lt;- cam.state$grid[x, y] N &lt;- cam.state$grid[x, ifelse(y == 1, my, y - 1)] S &lt;- cam.state$grid[x, ifelse(y == my, 1, y + 1)] E &lt;- cam.state$grid[ifelse(x == mx, 1, x + 1), y] W &lt;- cam.state$grid[ifelse(x == 1, mx, x - 1), y] cam.rule(V, N, S, E, W, x, y, t) } } cam.state &lt;&lt;- cam.state.new cam.state.display(t) } } Once you have made the changes shown above, check your work, save the file, and quit the Note that when we assign anything to the state of the CAM (held in cam.state and cam.state.new) we use the &lt;&lt;- (instead of the usual &lt;-) assignment operator. This operator allows assignment to objects outside of the function in the global environment. Objects that are stored in the global environment are available to all functions. Create a new function called cam.state.display(): cam.state.display &lt;- function() {} This creates an empty function called cam.state.display(). Use the fix() function to edit the cam.state.display() function: fix(cam.state.display) Edit the function to read: function(t) { if(t == 0) { x11(); par(pty = &quot;s&quot;) } image(cam.state$grid, main = paste(&quot;Infected at :&quot;, t), col = c(&quot;wheat&quot;, &quot;navy&quot;), axes = FALSE) } Remember that if you are using a Macintosh computer then you can use quartz() instead of x11(). This will give better results. Once you have made the changes shown above, check your work, save the file, and quit the editor. The cam.rule() function contains the rules of the CAM universe. We will start with a very simple infection rule: Each cell can be either infected or not-infected. If a cell is already infected it will remain infected. If a cell is not-infected then it will change its state to infected based on the state of its neighbours: If a neighbouring cell is infected it will infect the cell with a fixed probability or transmission pressure. Create a new function called cam.rule(): cam.rule &lt;- function() {} This creates an empty function called cam.rule(). Use the fix() function to edit the cam.rule() function: fix(cam.rule) Edit the function to read: function(V, N, S, E, W, x, y, t) { tp &lt;- c(N, S, E, W) * rbinom(4, 1, TP) cam.state.new$grid[x, y] &lt;&lt;- ifelse(V == 1 | sum(tp) &gt; 0, 1, 0) } Once you have made the changes shown above, check your work, save the file, and quit the editor. The basic CAM machine is now complete. We need to specify a value for the transmission pressure (TP): TP &lt;- 0.2 And define the initial state of the CAM: cases &lt;- matrix(0, nrow = 19, ncol = 19) cases[10, 10] &lt;- 1 cam.state &lt;- list(grid = cases) We can now run the simulation: cam.run(steps = 20) The number of infected cells is: sum(cam.state$grid) ## [1] 1 We can use this model to investigate the effect of different transmission pressures by systematically altering the transmission pressure specified in TP: cases &lt;- matrix(0, nrow = 19, ncol = 19) cases[10, 10] &lt;- 1 cam.state.initial &lt;- list(grid = cases) pressure &lt;- vector(mode = &quot;numeric&quot;) infected &lt;- vector(mode = &quot;numeric&quot;) for(TP in seq(0.05, 0.25, 0.05)) { cam.state &lt;- cam.state.initial cam.run(steps = 20) graphics.off() pressure &lt;- c(pressure, TP) infected &lt;- c(infected, sum(cam.state$grid)) } plot(pressure, infected) In practice we would run the simulation many times for each transmission pressure and plot (e.g.) the median number of infected cells found at the end of each run of the model. We can extend the model to include host immunity by specifying a new layer of cells (i.e. for host immunity) and modifying the CAM rule-set appropriately. Use the fix() function to edit the cam.rule() function: fix(cam.rule) Edit the function to read: function(V, N, S, E, W, x, y, t) { tp &lt;- c(N, S, E, W) * rbinom(4, 1, TP) cam.state.new$grid[x, y] &lt;&lt;- ifelse(V == 1 | (sum(tp) &gt; 0 &amp; cam.state$immune[x, y] != 1), 1, 0) } Once you have made the changes shown above, check your work, save the file, and quit the editor. We need to specify values for the transmission pressure (TP) and the proportion of the population that is immune (IM): TP &lt;- 0.2 IM &lt;- 0.4 And define the initial state of the CAM: cases &lt;- matrix(0, nrow = 19, ncol = 19) cases[10, 10] &lt;- 1 immune &lt;- matrix(rbinom(361, 1, IM), nrow = 19, ncol = 19) immune[10,10] &lt;- 0 cam.state &lt;- list(grid = cases, immune = immune) We can display the distribution of immune cells using the image() function: image(cam.state$immune, main = &quot;Immune&quot;, col = c(&quot;wheat&quot;, &quot;navy&quot;), axes = FALSE) We can now run the simulation: cam.run(steps = 30) The number of infected cells is: sum(cam.state$grid) ## [1] 1 A better summary is the proportion of susceptible (i.e. non-immune) cells that become infected during a run: sum(cam.state$grid) / (361 - sum(cam.state$immune)) ## [1] 0.004608295 We can use this model to investigate the effect of different proportions of the population that are immune by systematically altering the value assigned to IM: immune.p &lt;- vector(mode = &quot;numeric&quot;) infected.p &lt;- vector(mode = &quot;numeric&quot;) for(IM in seq(0, 0.8, 0.05)) { cases &lt;- matrix(0, nrow = 19, ncol = 19) cases[10, 10] &lt;- 1 immune &lt;- matrix(rbinom(361, 1, IM), nrow = 19, ncol = 19) immune[10,10] &lt;- 0 cam.state &lt;- list(grid = cases, immune = immune) cam.run(steps = 30) graphics.off() immune.p &lt;- c(immune.p, sum(cam.state$immune) / 361) infected.p &lt;- c(infected.p, sum(cam.state$grid) / (361 - sum(cam.state$immune))) } plot(immune.p, infected.p) In practice we would run the simulation many times for each value of IM and plot (e.g.) the median proportion of susceptible (i.e. non-immune) cells that become infected. A simple modification to this model would be to record the time-step at which individual cells become infected. We can do this by adding a new layer of cells (i.e. to record the time-step at which a cell becomes infected) and modifying the CAM rule-set appropriately. Use the fix() function to edit the cam.rule() function: fix(cam.rule) Edit the function to read: function(V, N, S, E, W, x, y, t) { tp &lt;- c(N, S, E, W) * rbinom(4, 1, TP) cam.state.new$grid[x, y] &lt;&lt;- ifelse(V == 1 | (sum(tp) &gt; 0 &amp; cam.state$immune[x, y] != 1), 1, 0) if(V != 1 &amp; cam.state.new$grid[x, y] == 1) { cam.state.new$ti[x, y] &lt;&lt;- t } } Once you have made the changes shown above, check your work, save the file, and quit the editor. We need to specify values for the transmission pressure (TP) and the proportion of the population that is immune (IM): TP &lt;- 0.2 IM &lt;- 0.4 And define the initial state of the CAM: cases &lt;- matrix(0, nrow = 19, ncol = 19) cases[10, 10] &lt;- 1 immune &lt;- matrix(rbinom(361, 1, IM), nrow = 19, ncol = 19) immune[10,10] &lt;- 0 ti &lt;- matrix(NA, nrow = 19, ncol = 19) ti[10,10] &lt;- 1 cam.state &lt;- list(grid = cases, immune = immune, ti = ti) We can now run the simulation: cam.run(steps = 120) Recording the time-step at which individual cells become infected allows us to plot an epidemic curve from the model: x11() hist(cam.state$ti) Remember, if you are using a Macintosh computer then you can use quartz() instead of x11(). This will give better results. The image() function can provide an alternative view of the same data: image(cam.state$ti, axes = FALSE) The colour of each cell reflects the time-step of infection (i.e. the darker cells were infected before the lighter cells). The CAM models that we have developed are general models of an infectious phenomenon. They could, for example, be models of the spread of an item of gossip, a forest fire, or an ink-spot. They are, however, poorly specified models for the epidemic spread of an infectious disease. In particular, they assume that a cell is infectious to other cells immediately after infection, that an infected cell never loses its ability to infect other cells, recovery never takes place, and immunity is never acquired. These deficits in the models may be addressed by appropriate modification of the CAM rule-set. A simple and useful model of epidemic spread is the SIR model. The letters in SIR refer to the three states that influence epidemic spread that an individual can exist in. The three states are Susceptible, Infectious, and Recovered. We will now modify our CAM model to follow the SIR model using the following parameters: Susceptible: A cell may be immune or non-immune. A cell may be immune prior to the epidemic or acquire immunity fourteen time-steps after infection. Once a cell is immune it remains immune. Infectious: An infected cell is infectious from eight to fourteen time-steps after being infected. Recovered: A cell is clinically sick from ten to twenty time-steps after being infected. If one time-step is taken to equal one day, these parameters provide a coarse simulation of the course of a measles infection. Use the fix() function to edit the cam.rule() function: fix(cam.rule) Edit the function to read: function(V, N, S, E, W, x, y, t) { tsi &lt;- t - cam.state$ti[x, y] cam.state.new$grid[x, y] &lt;&lt;- ifelse(tsi %in% INFECTIOUS, 1, 0) cam.state.new$cf[x, y] &lt;&lt;- ifelse(tsi %in% CLINICAL, 1, 0) cam.state.new$immune[x, y] &lt;&lt;- ifelse(!is.na(tsi) &amp; tsi &gt; IMMUNITY, 1, cam.state$immune[x,y]) if(cam.state$infected[x, y] == 1) { cam.state.new$infected[x, y] &lt;&lt;- 1 cam.state.new$ti[x, y] &lt;&lt;- cam.state$ti[x, y] } else { tp &lt;- c(N, S, E, W) * rbinom(4, 1, TP) if(sum(tp) &gt; 0 &amp; cam.state$immune[x, y] != 1) { cam.state.new$infected[x, y] &lt;&lt;- 1 cam.state.new$ti[x, y] &lt;&lt;- t } } } Once you have made the changes shown above, check your work, save the file, and quit the editor. It will also be useful to have a more detailed report of the state of the CAM at each time-step. Use the fix() function to edit the cam.state.display() function: fix(cam.state.display) Edit the function to read: function(t) { if(t == 0) { x11(width = 9, height = 9) par(mfrow = c(2, 2)) par(pty = &quot;s&quot;) } image(cam.state$grid, main = paste(&quot;Infectious at :&quot;, t), col = c(&quot;wheat&quot;, &quot;navy&quot;), axes = FALSE) image(cam.state$cf, main = paste(&quot;Clinical at :&quot;, t), col = c(&quot;wheat&quot;, &quot;navy&quot;), axes = FALSE) image(cam.state$ti, main = paste(&quot;Infected at :&quot;, t), axes = FALSE) image(cam.state$immune, main = paste(&quot;Immune at :&quot;, t), col = c(&quot;wheat&quot;, &quot;navy&quot;), axes = FALSE) } Remember, if you are using a Macintosh computer then you can use quartz() instead of x11(). This will give better results. Once you have made the changes shown above, check your work, save the file, and quit the editor. We need to specify values for the transmission pressure (TP) and the proportion of the population that is immune (IM): TP &lt;- 0.2 IM &lt;- 0.2 And the SIR parameters: CLINICAL &lt;- 10:20 INFECTIOUS &lt;- 8:14 IMMUNITY &lt;- 14 And define the initial state of the CAM: infected &lt;- matrix(0, nrow = 19, ncol = 19) infected[10, 10] &lt;- 1 ti &lt;- matrix(NA, nrow = 19, ncol = 19) ti[10,10] &lt;- 0 infectious &lt;- matrix(0, nrow = 19, ncol = 19) immune &lt;- matrix(rbinom(361, 1, IM), nrow = 19, ncol = 19) immune[10,10] &lt;- 0 cf &lt;- matrix(0, nrow = 19, ncol = 19) cam.state &lt;- list(grid = infectious, infected = infected, ti = ti, immune = immune, cf = cf) We should also record the number of susceptible cells for later use: susceptibles &lt;- 361 - sum(cam.state$immune) We can now run the simulation: cam.run(steps = 200) We can now calculate the proportion of susceptible (i.e. non-immune) cells that become infected: sum(cam.state$infected) / susceptibles ## [1] 0.003355705 We can use this model to test the effect of an intervention such as isolating an infected cell for a short period after clinical features first appear. We can do this, imperfectly because it does not allow us to specify compliance, by shortening the infectious period to include only the non-symptomatic time-steps and one time- step after clinical features have appeared: INFECTIOUS &lt;- 8:11 resetting the initial state of the CAM: infected &lt;- matrix(0, nrow = 19, ncol = 19) infected[10, 10] &lt;- 1 ti &lt;- matrix(NA, nrow = 19, ncol = 19) ti[10,10] &lt;- 0 infectious &lt;- matrix(0, nrow = 19, ncol = 19) immune &lt;- matrix(rbinom(361, 1, IM), nrow = 19, ncol = 19) immune[10,10] &lt;- 0 cf &lt;- matrix(0, nrow = 19, ncol = 19) cam.state &lt;- list(grid = infectious, infected = infected, ti = ti, immune = immune, cf = cf) Recording the number of susceptible cells: susceptibles &lt;- 361 - sum(cam.state$immune) Running the simulation: cam.run(steps = 200) And recalculating the proportion of susceptible (i.e. non-immune) cells that become infected: sum(cam.state$infected) / susceptibles ## [1] 0.003424658 We have developed a simple but realistic model of epidemic spread using a cellular automata machine. Such a model could be used to investigate the relative effect of model parameters (e.g. initial proportion immune, initial number of infective cells, transmission pressure, etc.) on epidemic spread by changing a single parameter at a time and running the simulation. Since the model is stochastic, the effect of each parameter change would be simulated many times and suitable summaries calculated. The model could be improved by, for example: Allowing for an open population with births (or immigration) and deaths (or emigration). This could be implemented by allowing immune cells to become susceptible after a specified number of time-steps. The ratio of births to deaths could then be modelled as the ratio of the length of the infectious period to the length of the immune period. Specifying a non-uniform distribution of transmission pressure during the infectious period. Allowing for individual variation in susceptibility. Allowing for individual variation in the duration of the infectious period. Simulating a non-uniform population density by allowing cells to be empty. Note that an immune cell is the same as an empty cell in the current model. Simulating a clustered distribution of immunity in the initial state of the CAM. Allowing a small proportion of infections to be infectious without exhibiting clinical features. Specifying rules that are applied at different time-points such as introducing isolation only after a certain number of clinical cases have appeared (i.e. after an epidemic has been detected). Allowing the coverage of interventions (e.g. the coverage of a vaccination campaign or compliance with isolation instructions) that are introduced after an epidemic has been detected to be specified. Simulating a more complex social structure. This could be implemented by allowing cells to belong to one of a finite set of castes with different initial conditions (e.g. immunity) and having rules that specify the level of interaction (i.e. the transmission pressure) between members of separate castes and the levels of intervention coverage achievable in the separate castes. Extending the neighbourhood definition by using the corner (i.e. north-east, south-east, south-west, and north-west) cells as neighbours for consideration in the CAM rule-set. We can now quit R: q() For this exercise there is no need to save the workspace image so click the No button (GUI) or enter n when prompted to save the workspace image (terminal). 9.5 Summary Computer intensive methods provide an alternative to classical statistical techniques for both estimation and statistical hypothesis testing. They have the advantage of being simple to implement and they remain simple even with complex estimators. Computer based simulation can simulate both data and processes. Process simulations maybe arbitrarily complex. Process simulation is a useful development tool that can save considerable time and expense when developing systems and methods. Process simulation can also be used to model complex social phenomena such as epidemic spread. Such simulations allow (e.g.) the the relative efficacy of interventions to be evaluated. R provides functions that allow you to implement computer intensive methods such as the bootstrap and computer based simulation. "],["what-now.html", "What now?", " What now? Now that you have had a taste of using R you will be able to decide whether it meets your requirements for a data-analysis system. The file R-intro.pdf which is installed with the R system contains the document ‘An Introduction to R’. This document provides a solid introduction to R. The file refman.pdf which is also installed with the R system contains the document ‘The R reference index’. This document provides a complete function-by-function reference to the R base system and several standard function libraries (packages). Other documents are available from the R Website: http://www.r-project.org/ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
